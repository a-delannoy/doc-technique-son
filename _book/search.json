[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guide pratique des techniques du son",
    "section": "",
    "text": "Avant-propos\nCe livre est né de la nécessité d’un support de cours pour la formation professionnelle “Technique de Prise de Son”, dispensée par l’auteur. Il intègre donc l’ensemble des notions abordées, expliquées en détail, ainsi que des exemples sonores.\nCe livre est écrit dans la philosophie de l’Open Source. L’intégralité de son contenu est donc disponible gratuitement. Son code source est accessible dans un dépôt GitHub. Ainsi, il est possible à tout à chacun de reporter les éventuelles erreurs ou de proposer des modifications. La grande majorité des outils utilisés pour sa rédaction et la création du contenu sont open source : R & Rmarkdown, Python, FAUST et draw.io."
  },
  {
    "objectID": "index.html#a-qui-sadresse-cet-ouvrage",
    "href": "index.html#a-qui-sadresse-cet-ouvrage",
    "title": "Guide pratique des techniques du son",
    "section": "A qui s’adresse cet ouvrage",
    "text": "A qui s’adresse cet ouvrage\nCe livre s’adresse à toutes personnes désireuses d’en apprendre plus sur le son ainsi que sur les métiers de preneur de son et de mixeur. Ainsi, il fait état des principes physiques nécessaires à la bonne appréhension des techniques de travail des métiers susnommés, avec le souci de les rendre accessibles à toutes et tous.\nIl pourra donc servir aux musiciens, aux étudiants, et pourquoi pas, à certains professionnels des métiers du son et du divertissement en général."
  },
  {
    "objectID": "index.html#mise-à-jour",
    "href": "index.html#mise-à-jour",
    "title": "Guide pratique des techniques du son",
    "section": "Mise à jour",
    "text": "Mise à jour\nLa distribution numérique de ce livre permet une mise à jour régulière de son contenu. Cela implique deux choses :\n\nCertaines sections peuvent être incomplètes, et seront complétées plus tard\nC’est une bonne idée de revenir consulter ce site régulièrement\n\n\nPour l’instant, ce livre n’inclut pas encore d’exemples sonores, cela est en cours de création."
  },
  {
    "objectID": "index.html#structures",
    "href": "index.html#structures",
    "title": "Guide pratique des techniques du son",
    "section": "Structures",
    "text": "Structures\nDans un premier temps, le livre aborde des principes généraux, aussi bien sur la physique que sur l’environnement de production de la musique enregistrée. Est ensuite abordé l’ensemble de la chaîne audio, en y explicitant le rôle et le fonctionnement de chacun de ses composants. L’objectif est de fournir une base technique objective au preneur de son.\nDans un second temps, le livre détaille un ensemble de techniques de prise de son et de mixage, insistant particulièrement sur les mécanismes généraux de la prise et sur l’écoute critique.\n\nLa partie dédiée à la pratique du mixage son n’est pas encore disponible en ligne."
  },
  {
    "objectID": "index.html#à-propos-de-lauteur",
    "href": "index.html#à-propos-de-lauteur",
    "title": "Guide pratique des techniques du son",
    "section": "À propos de l’auteur",
    "text": "À propos de l’auteur\nJean-Loup Pecquais est formateur et consultant dans le monde de l’audio professionnel (FLUX:: Immersive, Whiti Audio, Arkalya). Il est plus particulièrement spécialisé dans les techniques de mixage sonore immersives. Il est diplômé de l’ENS Louis-Lumière en 2019."
  },
  {
    "objectID": "generalites/qualifier_le_son.html#phénomène-physique",
    "href": "generalites/qualifier_le_son.html#phénomène-physique",
    "title": "1  Quantifier et qualifier le son",
    "section": "1.1 Phénomène physique",
    "text": "1.1 Phénomène physique\n\n1.1.1 Quelques définitions\nLe son est une vibration mécanique d’un fluide. Dans le cadre de ce cours, nous ne considérerons que l’air comme médium de propagation. Cette onde cause une variation de la pression dans l’espace. Nous, les êtres humains, le percevons grâce à notre ouïe. Il s’agit donc, par définition, d’un phénomène ondulatoire et peut être caractérisé par un nombre d’oscillations par seconde, aussi appelé fréquence. On estime que notre espèce est sensible aux fréquences allant de 20 Hz (très grave) jusqu’à 20 000 Hz (très aigu).\nOn parlera d’évènement sonore pour parler généralement de phénomènes physiques produisant une onde sonore.\nLes sons composés d’une seule fréquence se nomment sons purs. Cependant, de tels signaux n’existent pas dans la nature, et sont souvent utilisés afin de réaliser des mesures ou des tests psychoacoustiques.\n\n\nText(0, 0.5, 'Amplitude (dB)')\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nDans notre environnement, les sons sont donc composés de plusieurs fréquences. La fréquence la plus grave d’un son est sa fréquence fondamentale. Les autres sont alors appelées partiels. Si ces partielles ont pour fréquence un multiple de la fréquence fondamentale, alors on les nomme harmoniques.\n\nPlus généralement, on admettra que la composition fréquentielle, ou spectrale, de tout son peut être décomposée par une somme de sinusoïde. L’outil permettant de passer de la représentation temporelle d’un signal à sa représentation fréquentiel s’appelle la transformée de Fourrier.\n\n\n\nText(0, 0.5, 'Amplitude (dB)')\n\n\n<Figure size 672x480 with 0 Axes>\n\n\n\n\n\nLa fréquence fondamentale donne la hauteur du son (sa note en musique par exemple). Les partiels enrichissent cette fréquence fondamentale et créés le timbre d’un son. C’est en partie grâce au timbre que l’on peut reconnaître différents instruments de musiques jouant la même note.\nUn son se caractérise également par l’évolution de son amplitude au cours du temps. On parle alors de son enveloppe. Un modèle courant d’enveloppe est l’ADSR : Attack, Decay, Sustain, Release, soit Attaque, Décroissance, Maintient et Relâchement.\n\n\n\nExemple d’enveloppe ADSR\n\n\nLorsque son temps et très bref, l’ensemble attaque et décroissance forme les transitoires. Cette partie du signal est responsable de la sensation percussive du son.\n\n\n1.1.2 Relation entre temps, distance et fréquence\nIl est important de garder à l’esprit que les notions de temps, de fréquence et de distance sont étroitement liées. Nous avons vu ci-dessus que tous les sons peuvent être décrits par une somme de sinusoïde. Leur fréquence la plus grave, dite fondamentale, permet de définir la période. La période est le temps que met un signal à répéter son motif oscillatoire (voir schémas 3.1 et 3.2). Le lien mathématique entre fréquence et période est très simple, car l’un est l’inverse de l’autre :\n\\[ f = \\frac 1 T \\]\nSi nous étudions les fréquences extrêmes, audibles par notre ouïe, nous trouvons que pour \\(f_{min} = 20 \\,Hz\\), sa période \\(T_{f_{min}} = 50 \\,ms\\). Pour \\(f_{max} = 20\\,000 \\,Hz\\), \\(T_{f_{max}} = 0.5 \\,ms\\).\nUne onde sonore est également caractérisée par sa célérité. Celle-ci est constante dans un milieu donné. Dans l’air, à une température de \\(15 \\,°C\\) et au niveau de la mer, sa célérité \\(c\\) est de \\(340\\,m.s^{-1}\\). On admettra cette valeur pour réaliser l’ensemble de nos différents calculs.\nComme son unité l’indique, la célérité du son est homogène à une distance divisée par un temps, soit :\n\\[ c =\\frac d t \\]\nSuivant cette formule, nous pouvons alors calculer la longueur d’onde correspondant à une fréquence. La longueur d’onde se note \\(\\lambda\\).\n\\[ \\lambda = cT \\; \\iff \\; \\lambda = \\frac c f\\]\nSi nous étudions à nouveau les bornes minimale et maximale de notre audition, nous trouvons que \\(\\lambda_{f_{min}} = 17 \\,m\\) et \\(\\lambda_{f_{max}} = 17 \\,mm\\).\nNous pouvons également calculer le temps de propagation du son. En pratique, nous serons souvent intéressés par le temps de propagation séparant deux points dans l’espace (par exemple, le temps séparant deux microphones par rapport à un instrument).\n\n\n\nDistance entre deux microphones.\n\n\n\\[ t = \\frac {d_2-d_1}{c}\\]"
  },
  {
    "objectID": "generalites/qualifier_le_son.html#perception-du-son",
    "href": "generalites/qualifier_le_son.html#perception-du-son",
    "title": "1  Quantifier et qualifier le son",
    "section": "1.2 Perception du son",
    "text": "1.2 Perception du son\nNous avons abordé quelques notions de physique permettant de mieux caractériser le phénomène sonore. Comme indiqué au début de ce chapitre, le son peut également être discuté sous l’angle de notre ouïe, et donc, de notre perception. Cette branche de la science se nomme la psychoacoustique et cherche à étudier la façon dont nous percevons le son.\nNotre corps, et a fortiori notre cerveau, sont des machines extrêmement complexes. Nous sommes équipés d’une multitude de capteurs permettant de sentir le contact d’une matière, des odeurs, d’entendre, de goûter, de voir, de positionner nos membres dans l’espace, de ressentir la douleur, etc. Pris indépendamment, chacun de ces sens est déjà un phénomène complexe à décrire, mais il existe en plus une grande interdépendance entre ceux-ci. Par exemple, l’interdépendance entre la vision et l’audition est à l’origine d’un certain nombre de mécanismes biaisant notre écoute.\nNous nous bornerons au fil de ce cours à quelques notions liées à l’ouïe et à son interdépendance à d’autre sens quand cela sera pertinent.\n\n1.2.1 Spectre, timbre et vocabulaire\nD’un point de vue perceptif, le spectre d’un évènement sonore est facilement remarquable. Il est, par contre, beaucoup plus difficile à qualifier. Il n’est pas rare de rencontrer les adjectifs “chaud”, “brillant”, “rond”, “aéré”, “ouvert”, “sombre”, voir d’autres encore plus ésotérique, pour tenter de communiquer la sensation ressentie à l’écoute de tel ou tel son.\nCette difficulté liée à l’absence de vocabulaire commun quant à la qualification le son emmène systématiquement la redéfinition de ce vocable en fonction de son interlocuteur. En effet, le mot “rond” ne signifiera pas forcément la même chose selon à qui on s’adresse. Une stratégie possible consiste à questionner son interlocuteur sur l’utilisation de ses adjectifs tout en cherchant à y associer des exemples sonores.\nNous pouvons tout de même nous essayer à cet exercice pour nous permettre d’avoir un vocabulaire commun au fil de ce cours. Vous aurez sans doute compris qu’il n’y aura, dans les termes employés, aucun critère absolu.\n\n\n\nExemple de spectre\n\n\nProposition d’association entre bandes de fréquences et sensation.\n\n20 Hz — 80 Hz : Subharmonique, sensation tripale\n80 Hz — 160 Hz : Grave, sensation d’assise\n160 Hz — 380 Hz : bas-médium, sensation de « chaleur », voir « boueux »\n380 Hz — 1400 Hz : Medium, sensation de « boîte » quand trop présent, sonne « creux » quand trop absent\n1400 Hz — 3200 Hz : Haut-medium : zone de sensibilité maximale de l’oreille.\n3200 Hz — 8000 Hz : Aigu, apporte de la précision voir de l’agressivité\n8000 Hz — 20 000 Hz : Air, apporte une sensation d’ouverture voir de finesse\n\nIl est intéressant de former son oreille à reconnaître une plage de fréquence, ainsi que d’y associer son propre vocabulaire et une sensation. Les appellations proposées ci-dessus ne sont à prendre que comme guides et n’ont pas valeur de référence. Cela favorise une écoute critique et analytique.\n\nAussi, les fréquences graves ont un effet masquant sur les fréquences plus aiguës. Ce phénomène est dû au fonctionnement de notre oreille, et plus particulièrement de la cochlée.\n\n\n\n1.2.2 Pression acoustique & niveau sonore\nNous l’avons abordé plus haut, lorsqu’une onde sonore se déplace dans l’air, on constate la variation de la pression atmosphérique en ce point. Dès lors, il est facile de corréler l’amplitude de la variation de la pression avec le niveau sonore entendu (ou mesuré).\nL’unité du système international de la pression est le pascal (Pa). Or, il est très rare de parler de la pression acoustique en pascal, car la variation de cette pression exprimée en pascal ne correspond pas à ce que nous percevons. En d’autres termes, si la pression acoustique exprimée en pascal double, nous ne percevons pas un son deux fois plus fort.\nNotre oreille fonctionne de façon logarithmique, et non linéairement, face à une variation de pression acoustique. C’est pour cela que l’on parle généralement de niveau de pression acoustique, où SPL (pour Sound Pressure Level en anglais), qui s’exprimera en décibel. La relation entre la variation de pression et le niveau de pression acoustique se fait grâce à la relation :\n\\[L_p = 20\\,\\log_{10}\\Big(\\frac{p_{eff}}{p_{ref}}\\Big) \\qquad p_{ref} = 20\\mu Pa\\]\n\nSi la pression acoustique double, on observe une augmentation du niveau sonore de 6 dB SPL. Lorsqu’on ressent un doublement du niveau sonore, on observe une augmentation de 20 dB.\nLa pression acoustique est divisée par deux à chaque doublement de distance.\n\nLa question se complexifie lorsque l’on rajoute la dimension fréquentielle à la question de la perception du niveau sonore. En effet, nous percevons des niveaux sonores différents pour différentes fréquences pourtant émises au même niveau de pression acoustique. Pour inclure cette dépendance fréquentielle, nous avons mis en place une unité de mesure : la sonie ou bruyance (loudness en anglais). Il est donc possible ensuite de définir des courbes d’isosonie, c’est-à-dire des courbes indiquant un niveau sonore de perception égale en fonction de la fréquence et du niveau de pression acoustique.\n\n\n\nCourbes d’isosonie, aussi dites de Fletcher-Munson\n\n\nQue conclure de cet abaque ?\n\nNotre oreille ne perçoit pas les fréquences de manière égale.\nNotre zone de sensibilité maximale se situe dans l’aigu (3k-4k Hz).\nNotre perception d’un matériau sonore en fonction du niveau auquel nous l’écoutons !\n\n\n\n1.2.3 Positionnement dans l’espace\nNotre système auditif nous permet de situer l’émission d’un son dans l’espace. Cette capacité de localisation repose sur un ensemble de facteurs étroitement liés entre eux.\nOn qualifie notre écoute de binaurale, littéralement, écouter avec deux oreilles. La présence de deux “capteurs de pression” (oserait-on parler de microphones ?) sur les faces latérales de notre crâne et un premier élément expliquant notre capacité de localisation du son.\nEn effet, l’espacement de nos oreilles (en moyenne 15 cm), créer un décalage temporel entre nos deux canaux d’écoutes. Ce léger retard entendu d’un côté ou de l’autre nous permettra de placer un son plutôt à gauche ou plutôt à notre droite. On appelle cet écart de temps différence de temps interaural, ou ITD (interaural time difference en anglais) et se note \\(\\Delta t\\).\nOn pourrait d’ailleurs, grâce aux formules de ce début de chapitre, calculer le retard maximal moyen entre nos deux oreilles.\n\\[\\Delta t_{max} = \\frac d c = \\frac {0,15}{340} = 0.4 \\> ms\\]\n\n\n\nIllustration de l’ITD\n\n\nSi nos oreilles sont espacées de quelques centimètres, notre tête les séparant représente un obstacle acoustique non négligeable. De plus, les pavillons des oreilles imposent également une certaine directivité à notre écoute. En première approximation, on pourra donc considérer que l’ensemble formé par la tête et les pavillons implique une atténuation linéaire des ondes sonores, elle-même fonction de l’angle d’incidence. On appelle cette différence de niveau différence d’intensité interaural, ou ILD (interaural level difference) et se note \\(\\Delta i\\). On considère que si la différence de niveau de pression acoustique entre les deux oreilles est supérieure à 20 dB, on entendra l’évènement sonore complètement latéralisé.\n\nL’ombre acoustique que représentent la tête et le pavillon n’est en réalité pas du tout linéaire en fréquence. La modification du timbre induite par ce système n’est pas perçue par notre cerveau comme une information de couleur, mais bien comme une information de spatialisation. Ainsi, selon l’angle d’incidence de l’évènement sonore, son spectre sera filtré d’une certaine manière qui permettra à notre cerveau de le positionner dans l’espace. La réponse en fréquence d’une tête se nomme HRTF (Head Related Transfer Function).\n\nEnfin, nous sommes également capables de déterminer la distance d’un évènement sonore. La plupart des paramètres permettant d’évaluer cette distance sont relatifs. Cela signifie que l’évènement doit être comparé à un autre pour pouvoir le repositionner dans l’espace. On pourra alors comparer :\n\nLeurs niveaux sonores : un évènement sonore plus fort paraît plus proche\nLeurs timbres : l’absorption de l’air aura pour effet de diminuer les fréquences aiguës\nLa sensation de réverbération associée : plus le signal de l’évènement sonore semblera solliciter la réponse acoustique du lieu, plus celui-ci semblera fort.\nLe temps d’arrivée des premières réflexions : le son direct d’un évènement sonore lointain arrivera quasi simultanément avec ses premières réflexions. Le son direct d’un évènement sonore proche arrivera avant ses premières réflexions.\n\nLe chapitre suivant traitera des notions d’acoustique élémentaire ainsi que de la réverbération."
  },
  {
    "objectID": "generalites/l_acoustique_des_salles.html#généralités",
    "href": "generalites/l_acoustique_des_salles.html#généralités",
    "title": "2  Acoustique des salles",
    "section": "2.1 Généralités",
    "text": "2.1 Généralités\n\n2.1.1 La réverbération\nAfin d’étudier l’acoustique d’une salle, on procède à la mesure de sa réponse impulsionnelle. Pour se faire, on émet dans le lieu à mesurer un signal audio impulsionnel (clappement de main, explosion d’un ballon, émission d’une impulsion de Dirac grâce à un haut-parleur), et l’on enregistre le résultat à l’aide d’un microphone de mesure.\nLa réponse impulsionnelle d’une salle est généralement décrite en deux temps : le temps des premières réflexions et le temps du champ diffus.\n\n\n\nSchéma d’une réponse impulsionnelle de réverbération.\n\n\nLes premières réflexions sont les premiers rebonds d’une onde sonore sur les parois d’une salle et sont caractéristiques de la signature acoustique du lieu. Ces rebonds reviennent à l’auditeur avec un certain temps. Ce retard se nomme souvent « pré-délai » dans les moteurs de réverbération artificiels. Ce prédélai est fonction de deux paramètres :\n\nla taille de la pièce ; plus la pièce est petite, plus les premières réflexions reviendront à l’auditeur rapidement.\nles positions de la source sonore et de l’auditeur ; plus l’auditeur est proche de la source, plus les premières réflexions arriveront après le son direct, plus l’auditeur est loin de la source, plus les premières réflexions arriveront en même temps que le son direct.\n\nLorsque les premières réflexions elles-mêmes auront rebondi plusieurs fois sur les parois du lieu, le phénomène d’écho des premières réflexions va se muer en champs diffus, par nature plus dense. La longueur du champ diffus se mesure grâce au RT60. Cette méthode de mesure propose de regarder le temps que met la réverbération à perdre 60 dB. Ce temps permettra ensuite de donner une longueur de réverbération.\n\n\n2.1.2 Calcul du temps de réverbération\nL’équation de Sabine permet de calculer le temps de réverbération d’une salle à partir de son volume et du coefficient d’absorption de ses matériaux.\n\\[RT_{60} = 0.1611 \\times \\frac{V}{\\sum_{i=0}^{k} S_i.\\alpha_i}\\]\n\\(V\\) s’exprime en \\(m^3\\) et \\(S\\) en \\(m^2\\). \\(\\alpha\\) est le coefficient d’absorption du matériau, en sabins. Ce coefficient est compris entre 0 et 1, plus il est important plus le matériau est absorbant.\nEn guise d’exemple sur l’utilisation de la formule ci-dessus, prenons le cas d’une pièce de \\(25\\,m^2\\) (\\(5\\,m\\) par \\(5\\,m\\)) et de \\(2.40\\,m\\) de hauteur. Nous considérons que le sol est en parquet et les murs en plâtre. Nous avons donc \\(25\\,m^2\\) de parquet et \\(4\\times(5\\times2.4)=48\\,m^2\\). On trouve sur les sites de fabricant de matériaux que le plâtre peint a un coefficient d’absorption de 0.05 sabins et le bois un coefficient de 0.15 sabins. Notre calcul final.\n\\[RT_{60} = 0.1611 \\times \\frac{25 \\times 2.4}{25\\times0.15+48\\times0.05} \\approx 1.57\\,s\\]\nOn peut dès lors calculer la distance critique, distance à partir de laquelle on entendra autant un évènement sonore que la réponse acoustique de la salle à son stimulus.\n\\[d_c \\approx 0.057 \\times \\sqrt{\\frac{V}{RT60}}\\]\nDans notre exemple \\(d_c \\approx 0.35\\,m\\).\nIl est souvent considéré que la taille de la pièce joue un rôle déterminant sur la longueur de réverbération. L’équation de Sabine indique bien que le coefficient d’absorption des matériaux y joue un rôle beaucoup plus important. Le modèle de réverbération de l’IRCAM va jusqu’à complètement décorréler la taille de la pièce simulée du temps de réverbération. Au final, la taille de l’espace joue davantage sur la structure temporelle des échos, et donc, principalement sur les premières réflexions.\n\n\n2.1.3 Limite de l’équation de Sabine\nIl convient d’observer plusieurs réserves quant à l’utilisation de l’équation de Sabine. Premièrement, elle ne tient pas compte de l’aspect fréquentiel lié à l’absorption des matériaux. En effet, le temps de réverbération des graves est presque toujours plus long que celui des aigus. Afin de contourner ce problème, on pourra chercher des coefficients d’absorption tenant compte de la fréquence et ainsi résoudre l’équation de Sabine pour certaines plages fréquentielles.\nL’équation de Sabine pose également problème pour de petits espaces (régie d’écoute par exemple) en prédisant un temps de réverbération trop long. Dans ce cas, l’équation d’Eyring est plus adaptée.\n\\[RT_{60} = -0.1611 \\times \\frac{V}{\\sum_{i=0}^{k} S_i.\\ln(1-\\alpha_i)}\\]\n\n:warning: L’équation d’Eyring n’améliore pas non plus la problématique fréquentielle.\n\n\n\n2.1.4 L’indice de “Speech Clarity” C50\nL’indice d’intelligibilité (noté \\(C_{50}\\)), ou “Speech Clarity” en anglais, indique la faculté d’une pièce à permettre une bonne compréhension d’une voix parlée. Son principe repose sur la mesure de l’énergie de la réponse impulsionnelle de la pièce avant 50 ms et après 50 ms. On en fait ensuite un rapport logarithmique pour obtenir une valeur en décibel.\n\\[ C_{50} = 10 \\times \\log \\frac{Energie(<50ms)}{Energie(>50ms)} \\, dB \\]\nPlus la valeur du \\(C_{50}\\) est grande, plus la salle concentre la majorité de son énergie avant les 50 ms de propagation de la réverbération. À l’inverse, plus le \\(C_{50}\\) est faible, plus la salle a une énergie prédominante après 50 ms de temps de propagation. Dans ce cas une voix parlée paraîtra moins intelligible, car la réponse acoustique de la pièce engendrera un effet de fusion et de masquage.\n\n\n2.1.5 Le phénomène d’onde stationnaire\nLa plupart des pièces de vie sont des salles rectangulaires. Dans ce cas, les surfaces sont toutes parallèles. Ce type de salle est particulièrement propice à l’apparition d’ondes stationnaires. Une onde stationnaire est un phénomène acoustique provoquant l’augmentation de volume de certaines fréquences (ventre) et la disparition d’autres (nœuds).\nNous aborderons ici ce phénomène sous l’angle de l’acoustique des salles, mais il est applicable dans d’autres situations, comme la vibration d’une corde par exemple.\n\n\n\n\n\nLes points rouges représente les noeuds, les amplitudes maximales sont les ventres. Infographie par Lucas Vieira\n\n\n\n\nIl est possible de calculer les fréquences d’un mode grâce aux formules vues au chapitre précédent :\n\\[f(n) = \\frac{c}{2L}.n\\] où \\(c=340\\,m.s^{-1}\\), \\(L\\) est la longueur considérée de la pièce. Pour \\(n=1\\) on trouve le mode propre. Pour \\(n>1\\) on trouvera tous les modes harmoniques.\nÉtudions la fréquence du mode propre pour deux cas théoriques : une salle de 16 m² (4x4) et une autre de 49 m² (7x7). On trouvera donc :\n\\[f(1)_{L=4m} = 42.5 \\,Hz \\>\\>\\>\\> f(1)_{L=7m} = 24 \\,Hz\\]\nOn en déduit donc que, plus la pièce est grande, plus la fréquence des modes propres sera grave. Il convient également de considérer la distance de chaque surface parallèle, car les pièces sont rarement cubiques. Cela implique donc la présence de trois modes propres, plus leurs modes harmoniques, pour une seule et même salle."
  },
  {
    "objectID": "generalites/l_acoustique_des_salles.html#premières-réflexions-et-filtre-en-peigne",
    "href": "generalites/l_acoustique_des_salles.html#premières-réflexions-et-filtre-en-peigne",
    "title": "2  Acoustique des salles",
    "section": "2.2 Premières réflexions et filtre en peigne",
    "text": "2.2 Premières réflexions et filtre en peigne\nNous avons vu que la réponse acoustique, ou réverbération, d’une salle se décompose généralement en deux parties, la première étant les premières réflexions. Ces premières réflexions sont donc, comme leur nom l’indique, les premiers rebonds que nous entendons suite à un évènement sonore.\nDans de petites pièces, les premières réflexions peuvent être entendues si proche du son direct que cela génère un type de filtrage bien particulier appelé filtre en peigne.\n\n\n\n\n\n\n\n\n\nToujours en utilisant les formules définies au premier chapitre, on établit la relation suivante :\n\\[ fc = \\frac 1{2t} = \\frac c{2d} \\]\nOù \\(fc\\) correspond à la fréquence d’annulation la plus grave du filtre en peigne. Les autres fréquences se calculent grâce à la relation \\(f(n) = fc*n\\). Le phénomène de filtre en peigne est donc également harmonique.\nAinsi, on peut calculer les filtres en peignes présents au point d’écoute d’une régie de mixage ou de prise de son grâce à la mesure du chemin des premières réflexions.\n\n\n\nEnsemble des premières reflexions entendues par une oreille pour une enceinte (hors plafond et plancher/bureau).\n\n\n\nLa réflexion du son sur une paroi est tout à fait comparable à de l’optique géométrique. Une onde sonore arrivant avec un angle d’incidence \\(\\alpha\\) sur une surface sera réfléchie avec le même angle. Ainsi, il est souvent conseillé d’utiliser un miroir lorsque l’on positionne des traitements acoustiques. Lorsque la personne assise au point d’écoute voit une enceinte dans un miroir placé sur un mur, on sait alors qu’il faudra placer le panneau à la place du miroir.\n\nOn se rend donc compte que l’influence des filtres en peigne générés par les premières réflexions est très importante. Ce phénomène à lui seul explique l’intérêt d’une grande régie d’écoute. En effet, plus une pièce est grande, plus l’écart de temps entre le son direct et les premières réflexions est important. Cela implique deux choses :\n\nNotre cerveau favorisera le son direct plus facilement (effet de précédence)\nÀ partir d’une certaine taille, l’effet du filtre en peigne se mue en information d’acoustique pour notre cerveau. Au-delà de 40 ms (trajet d’une première réflexion d’environ 14 m), l’écart entre le son direct et les premières réflexions est tel que nous entendons un écho (effet Haas).\n\nAfin de réduire au maximum les effets des filtres en peignes, il est recommandé de placer des traitements aux points de réflexion critique par rapport à la position d’écoute (voir schéma ci-dessus)."
  },
  {
    "objectID": "generalites/l_acoustique_des_salles.html#traitement-acoustique",
    "href": "generalites/l_acoustique_des_salles.html#traitement-acoustique",
    "title": "2  Acoustique des salles",
    "section": "2.3 Traitement acoustique",
    "text": "2.3 Traitement acoustique\nGrâce aux différents points abordés ci-dessus, nous avons maintenant bien l’idée que l’acoustique d’un lieu est un des facteurs les plus déterminants sur le rendu sonore. Mais c’est aussi celui sur lequel il est plus difficile et technique d’intervenir.\nOn favorisera au maximum une architecture optimisée pour l’acoustique. Dans ce but, il convient de n’avoir aucune surface parallèle, cela permettant de grandement limiter l’apparition d’ondes stationnaires. On choisira également des matériaux avec des propriétés acoustiques intéressantes (plâtre et carrelage sont à proscrire, au profit du bois par exemple).\nOn se posera ensuite la question des endroits de la pièce les plus propices pour y positionner un évènement sonore (enceinte, musicien, etc.). On cherchera donc un point où la contribution des différents modes semble équilibrée. Pour cela, il suffit de se munir d’une enceinte et d’y diffuser une musique ou un signal test qui nous est familier. En déplaçant l’enceinte, on pourra évaluer la contribution acoustique de la pièce en différents points.\nUne fois ces considérations prises en compte, on pourra alors aborder le traitement de l’acoustique.\n\nIl ne faut pas confondre isolation acoustique et traitement acoustique. Dans le premier cas, on chercher a limiter la contribution sonore d’un lieu sur son environnement, dans l’autre on cherche à améliorer la propagation du son dans un espace donné. Une isolation acoustique satisfaisante nécessite de lourds travaux, voire l’aménagement d’une “boîte dans une boîte”. Ces notions d’acoustiques dépassent le cadre de ce cours.\n\n\n2.3.1 Les types de traitements\nOn trouve, en général, deux types de traitements :\n\nLes absorbeurs, qui réduisent l’énergie d’une onde sonore à son impact.\nLes diffuseurs, qui répartissent l’énergie d’une onde sonore dans l’espace.\n\nDans un lieu où la quantité de réverbération est jugée trop importante, on utilisera des absorbeurs. À l’inverse, dans un lieu où l’on souhaite préserver la quantité de réverbération, mais en évitant les phénomènes de modes ou de filtre en peignes, on utilisera des diffuseurs.\nDans de petits lieux, l’usage de diffuseur semble contre-productif, la priorité étant d’absorber au maximum les premières réflexions, celle-ci arrivant très rapidement après l’émission du son direct.\n\n\n2.3.2 Considération d’acoustique pour le travail de son\nIl est vivement recommandé d’installer un studio, de prise de son ou de monitoring, dans un lieu plutôt grand. En effet, plus le lieu est grand, plus il sera facile de positionner un point de prise de son ou d’écoute suffisamment éloigné des parois afin de minimiser l’influence des premières réflexions. Aussi, plus le lieu est grand, plus l’espace y sera suffisant pour installer des traitements acoustiques. Certains types de traitements, comme les basstraps, peuvent prendre une place bien trop importante pour être installée dans des pièces de dimension habituelle (chambres, bureau, etc.). On se rappellera aussi de choisir une pièce de travail avec le minimum de surface parallèle, afin de limiter les ondes stationnaires.\nEn ce qui concerne les traitements en eux-mêmes, il est vivement recommandé de traiter en priorité le bas du spectre. L’ajout de basstrap est donc prioritaire sur le reste des traitements. Plus la longueur d’onde à traiter est grande (donc la fréquence grave), plus la taille des matériaux devra être importante. On retrouve donc le point abordé précédemment : traiter une pièce correctement, demande un certain espace. Par ailleurs, il est important que les traitements appliqués à un lieu soient linéaires en fréquence, c’est-à-dire qu’il ne se concentre pas sur une seule zone du spectre. Cela arrive souvent avec les kits de mousses peu onéreux, mais n’ayant une réelle efficacité que dans les médiums et hautes fréquences.\nPour une régie d’écoute, on sera tenté de privilégier des traitements d’absorption. En effet, une réverbération trop longue dans une régie de monitoring risque fort de fausser certaines prises de décisions (distance des microphones à la source, quantité de réverbération, etc.). À l’inverse, une pièce avec un temps de réverbération trop court pourra créer un sentiment d’inconfort, voire de malaise.\nPour une salle de prise de son, l’idéal est de disposer d’un grand espace avec un traitement acoustique principalement basé sur de la diffusion, pour ensuite disposer de traitements absorbants amovibles permettant de sculpter le rendu acoustique en fonction de la prise de son à réaliser. Pour des petits lieux (- de 25 m²), on cherchera à absorber au maximum afin de limiter les effets de filtre en peigne."
  },
  {
    "objectID": "generalites/notions_electronique.html#les-grandeurs-physiques",
    "href": "generalites/notions_electronique.html#les-grandeurs-physiques",
    "title": "3  Notions élémentaires d’électronique",
    "section": "3.1 Les grandeurs physiques",
    "text": "3.1 Les grandeurs physiques\nCommençons par aborder les grandeurs physiques liées à l’électricité.\n\n3.1.1 L’intensité\nL’intensité électrique, notée I et exprimée en Ampère (A), est une grandeur permettant de mesurer le débit du courant électrique. Ceci est parfaitement analogue à un débit d’eau. Si un robinet est faiblement ouvert, l’écoulement de l’eau sera faible, s’il est complètement ouvert, le débit sera fort.\n\n\n3.1.2 La tension\nLa tension, généralement notée U et exprimée en Volt (V), désigne une différence de potentiel entre deux points d’un circuit. Imaginons deux réservoirs d’eau, remplis d’un volume différent et connectés par une valve. Dans ce cas, la différence de potentiel serait la différence du volume d’eau entre les deux réservoirs. En d’autres termes, s’il n’y a pas de tension, il n’y a pas de débit.\n\nOn choisit, en général, la masse, valant zéro volt, comme point de référence.\n\nDans le cas de l’audio, la tension électrique du signal sonore est homologue à la variation de pression.\nTout comme la pression acoustique, il est possible de rendre compte d’une variation de tension électrique à un niveau sonore en décibel. Le relation liant la tension et le niveau est :\n\\[ L_{dB} = 20 \\, log (\\frac{U}{U_{ref}}) \\]\nIl existe plusieurs valeurs pour U_{ref}, donnant lieu à différentes unités de mesures :\n\ndBm, définie à l’apparition du téléphone, propose \\(U_{ref} = 0.775 V\\) pour une impédance de \\(600 \\omega\\). Cette impédance correspond à celle des lignes téléphoniques.\ndBu / dBv, qui ne tient plus compte de la charge d’impédance, \\(U_{ref} = 0.775 V\\).\ndBV, où \\(U_{ref} = 1 V\\)\n\n\nLorsque la tension double, le niveau augmente de six décibels. Lorsque la tension est multiplié par dix, le niveau augmente de vingt décibels.\n\nOn peut également définir l’augmentation du niveau sonore par rapport à la puissance du signal. On admet que :\n\\[ P = \\frac{U^2}{Z} \\, U = \\sqrt{P \\times Z} \\]\nOù \\(P\\) est la puissance. En remplaçant dans l’équation précédente, on trouve :\n\\[ L_{dB} = 20 \\, log (\\frac{\\sqrt{P}}{\\sqrt{P_{ref}}}) \\> = 10 \\, log (\\frac{P}{P_{ref}}) \\]\n\nLorsque la puissance double, le niveau augmente de trois décibels. Lorsque la puissance est multiplié par dix, le niveau augmente de dix décibels.\n\nOn utilisant la loi d’ohm (voir ci-dessous) et la relation entre la puissance, la tension et l’impédance, on trouve également que :\n\\[ P = U \\times I \\]\n\n\n3.1.3 L’impédance\nNous connaissons, en général, la loi d’Ohm. Celle-ci permet de donner une relation entre l’intensité du courant et sa tension, aux bornes d’un composant d’un circuit (aussi appelé dipôle).\n\\[ U = R \\times I \\]\nOù \\(R\\) est la résistance du dipôle. Elle traduit la facilité d’un courant à se déplacer dans le dipôle. Pour reprendre les analogies ci-dessus, la résistance correspondrait à une valve. À tension constante, si la résistance tend vers zéro, le débit est très important. Si la résistance tend vers l’infini, le débit sera très faible. Si elle est nulle, alors nous sommes dans le cas d’un court-circuit (interrupteur fermé). Si elle est infinie, cela traduit une absence de connexion entre deux points d’un circuit (interrupteur ouvert). L’unité de cette résistance est l’ohm.\nL’impédance traduit elle aussi l’opposition d’un circuit au passage d’un courant électrique, mais dans le cas d’une tension oscillante. Dès lors, l’impédance englobe les effets de résistance, de capacitance et d’inductance (voir ci-dessus)."
  },
  {
    "objectID": "generalites/notions_electronique.html#les-composants-électroniques",
    "href": "generalites/notions_electronique.html#les-composants-électroniques",
    "title": "3  Notions élémentaires d’électronique",
    "section": "3.2 Les composants électroniques",
    "text": "3.2 Les composants électroniques\n\n3.2.1 Les composants passifs\n\n\n\n \n\n\nReprésentation d’une résistance et de son symbole\n\n\n\nÉtudions maintenant les composants électroniques les plus communs. Nous avons en premier les résistances. Ce sont des dipôles purement résistifs. Leur valeur s’exprime en ohm. Une résistance s’oppose donc au passage du courant. Pour rappel, la tension a ses bornes est \\(U = R \\times i\\).\n\n\n\n \n\n\nReprésentation d’un condensateur et de son symbole\n\n\n\nViennent ensuite les condensateurs. Ils sont constitués de matériaux conducteurs séparés par une couche isolante. La relation entre tension et intensité à ses bornes en régime oscillant est :\n\\[ U = Z_c \\times I \\]\nOù \\(Z_c\\) est l’impédance d’un condensateur idéal. Nous pouvons ici la même analyse que plus haut, quand \\(Z_c\\) tend vers l’infini le courant ne passe plus, quand \\(Z_c\\) tend vers 0, le débit est important. L’impédance d’un condensateur est fonction de sa capacité (noté C, et s’exprime en farads).\n\\[ Z_c = \\frac{1}{jC\\omega} \\> = 2 \\pi \\, f\\]\nSi la fréquence \\(f\\) tend vers l’infini, \\(Z_c\\) tend vers zéro, si la fréquence tend vers zéro, \\(Z_c\\) tend vers l’infini. On constate donc que l’impédance d’un condensateur varie en fonction de sa fréquence. On peut assimiler un condensateur à un interrupteur ouvert en basse fréquence et à un interrupteur fermé en haute fréquence.\n{fig.align=“center”, out.width=“25%”}\nTerminons sur les bobines. Ces composants sont constitués d’un enroulement de câble en cuivre et possède une inductance notée L et s’exprimant en henrys. Étudions à nouveau la relation entre tension et intensité, aux bornes d’une bobine :\n\\[ U = Z_L \\times I \\]\nOù \\(Z_L\\) est l’impédance d’une bobine idéale. Cette impédance se calcule grâce à la relation suivante :\n\\[ Z_L = j\\omega L = 2 \\pi \\, f \\]\nSi la fréquence \\(f\\) tend vers l’infini, \\(Z_L\\) tend vers l’infini. Si \\(f\\) tend vers zéro, alors \\(Z_L\\) tend vers zéro. On observe donc le comportement inverse du condensateur. Une bobine se comporte comme un court-circuit en basse fréquence et comme un interrupteur ouvert en haut fréquence.\n\nOn admet j comme un outil mathématique permettant de simplifier certaines écritures et certains calculs. On l’appelle le nombre complexe, tel que \\(j^2 = -1\\). Dans nos applications, sa présence dans les relations des impédances de condensateur et de bobine implique un déphasage de \\(-\\frac{\\pi}{2}\\) pour un condensateur, et, de \\(\\frac{\\pi}{2}\\) pour une bobine.\n\nL’association de résistances, de condensateurs et de bobines donne des circuits RL, RC où RLC, permettant de réaliser des opérations de filtrage sur le signal.\n\n\n3.2.2 Tubes & semi-conducteurs\n\n\n\n  \n\n\nTubes, transistor et circuits intégrés\n\n\n\nLes tubes, tubes à vide, ou parfois, lampes, sont historiquement les premiers composants permettant d’amplifier le signal, contre une certaine tension d’alimentation. On les retrouve donc dans les préamplificateurs, égaliseurs, compresseurs, et autres amplificateurs jusque dans les années soixante. Ils sont alors progressivement remplacés par les transistors, composants appelés semi-conducteurs. Ces transistors permettent de réaliser la même amplification du signal qu’une lampe, mais sont beaucoup plus petits et demandent aussi moins de puissance électrique pour réaliser le même facteur d’amplification (aussi appelé gain). Peu de temps après la mise au point des transistors, les circuits intégrés sont inventés. Ces petites boîtes renferment plusieurs transistors, et peuvent également servir à l’amplification de signaux.\nIl est très important de savoir que l’invention du transistor et des circuits intégrés est sans doute l’avancée technologique la plus importante du siècle dernier. Elle a permis le développement exponentiel de l’industrie informatique grâce à la miniaturisation des composants.\nL’utilisation de tubes, de transistors ou de circuits intégrés au sein des machines audio, est souvent associée à une certaine « couleur ». Il y aurait donc un son des tubes, un son des transistors et un son des circuits intégrés. Les différences entre ces dipôles apparaissent principalement dans les zones de non-linéarité des composants, typiquement dans leur zone de saturation. La saturation apparaît lorsque la tension du signal amplifiée dépasse la tension d’alimentation du composant responsable de cette amplification. On observe alors l’apparition de certaines harmoniques. La distribution des harmoniques générés est différente en fonction du dipôle utilisé.\nIl est compliqué d’attribuer une couleur sonore particulière à un composant. En effet, le comportement d’un composant est fondamentalement conditionné par la topologie du circuit dans lequel il est utilisé ainsi que par les autres composants qui l’entourent. Il convient donc, à l’humble avis de l’auteur, d’être relativement prudent sur des expressions telles que « son des tubes » ou « son des transistors », particulièrement quand il s’agit de dire que l’une des technologies « sonnerait mieux » que l’autre. L’histoire de l’électronique musicale regorge d’exemples et de contre-exemples pour chacune de ces affirmations."
  },
  {
    "objectID": "generalites/notions_electronique.html#linfluence-de-limpédance-entre-différents-appareils.",
    "href": "generalites/notions_electronique.html#linfluence-de-limpédance-entre-différents-appareils.",
    "title": "3  Notions élémentaires d’électronique",
    "section": "3.3 L’influence de l’impédance entre différents appareils.",
    "text": "3.3 L’influence de l’impédance entre différents appareils.\nSur la fiche technique des appareils, on trouve des valeurs pour son impédance d’entrée et son impédance de sortie. Imaginons que nous connections un appareil A dans un appareil B. En pratique, nous faisons en sorte que l’impédance de sortie de l’appareil A soit dix fois inférieure à l’impédance d’entrée de l’appareil B. À partir du moment où ces impédances sont proches, voire que l’impédance de sortie de A soit plus grande que celle d’entrer de B, nous allons atténuer le signal transitant entre les deux appareils. Étudions de plus près ce phénomène.\nSoit le schéma électronique ci-dessous. On appelle \\(U_{out}\\) la tension de sortie de l’appareil A et \\(Z_{out}\\) son impédance de sortie. De façon similaire, on appelle \\(U_{in}\\) la tension d’entré de l’appareil B et \\(Z_{in}\\) son impédance d’entré.\n\nDans ce circuit, \\(Z_{eq} = Z_{in} + Z_{out}\\) et \\(U_{out} = Z_{eq} \\times i\\). Alors, \\(i = \\frac{U_{out}}{Z_{eq}} = \\frac{U_{out}}{Z_{in} + Z_{out}}\\). Toujours grâce au circuit ci-dessus, on peut dire que \\(U_{in} = Z_{in} \\times I\\). En remplaçant dans l’expression précédente on trouve : \\(\\frac{U_{in}}{Z_{in}} = \\frac{U_{out}}{Z_{in} + Z_{out}}\\)\n\\[ \\frac{U_{in}}{U_{out}} = \\frac{Z_{in}}{Z_{in} + Z_{out}} = \\frac{1}{1+\\frac{Z_{out}}{Z_{in}}} \\]\nDès lors, si \\(Z_{in}\\) est très grand devant \\(Z_{out}\\), alors \\(U_{in}\\) tend vers \\(U_{out}\\). Si \\(Z_{out}\\) est très grand devant \\(Z_{in}\\), alors \\(U_{in}\\) tend vers \\(0\\).\nCela nous amène à démontrer l’affirmation ci-dessus. Maintenant, nous savons que dans un circuit, l’impédance varie en fonction de la fréquence. Dès lors, une mauvaise adaptation d’impédance ne fera pas que diminuer l’amplitude du signal, mais filtrera aussi une partie de spectre, généralement les hautes fréquences.\n\nOn considère aussi l’adaptation d’impédance en tension. Lorsque que nous considérons la puissance les relations sont différentes (cf section sur les hautparleurs)."
  },
  {
    "objectID": "generalites/production_musicale.html#les-acteurs-de-la-réalisation-dune-œuvre-enregistrée",
    "href": "generalites/production_musicale.html#les-acteurs-de-la-réalisation-dune-œuvre-enregistrée",
    "title": "4  Description d’une production musicale type",
    "section": "4.1 les acteurs de la réalisation d’une œuvre enregistrée",
    "text": "4.1 les acteurs de la réalisation d’une œuvre enregistrée\nNous allons ici rapidement discuter des différents rôles apparaissant dans la production d’une œuvre musicale enregistrée. Ceux-ci sont volontairement très séparés, bien que dans les cas pratiques, une personne puisse en incarner plusieurs.\nLe compositeur est la personne qui a composé la mélodie et l’harmonie de l’œuvre.\nL’arrangeur est chargé de l’orchestration (choix des instruments) et l’écriture des différentes partitions.\nL’interprète a la responsabilité de retranscrire une partition le plus justement possible, à la fois dans sa dimension technique et sensible.\nLe directeur artistique (ou DA) supervise l’ensemble de l’enregistrement. Il aura, par exemple, à choisir le preneur de son, le mixeur ou dans quel studio enregistrer. Lors de la session d’enregistrement, il aura à diriger les musiciens (comme un réalisateur dirige ses acteurs au cinéma) afin de leur faire jouer la meilleure interprétation possible pour l’œuvre. Lors du mixage, il sera le principal interlocuteur du mixeur. Pour faire court : il est le garant de l’orientation esthétique du projet.\nLe producteur finance l’ensemble de projets. C’est donc un investisseur qui attend un retour sur investissement.\n\nL’appellation abusive de « producteur » pour parler du directeur artistique vient d’un anglicisme du mot « producer ». Le producteur est donc bien l’équivalent du directeur artistique dans les pays anglo-saxons. Si le DA a besoin d’un certain talent, le producteur a surtout besoin d’argent.\n\nLe preneur de son est chargé d’enregistrer les musiciens et musiciennes. Il a donc un rôle premier très technique : il doit inscrire sur un support les ondes sonores produites par ces musiciens. Il a également un rôle esthétique très important, d’un point de vue sonore, car le choix du dispositif de prise de son aura un fort effet sur la suite de la vie de l’œuvre.\nLe mixeur intervient après la prise de son et doit réaliser une sommation de l’ensemble des points de captations (microphone) vers un format écoutable par le grand public (mono, stéréo, 5.1, Ambisonique, Dolby Atmos, etc.). Son rôle esthétique est fortement contraint par le travail de prise de son. Si celle-ci est réussie, il pourra amplifier et bonifier les choix de production. Dans le cas contraire, il devra lutter pour essayer de faire sortir le meilleur d’une matière imparfaite.\nLe technicien de mastering est le dernier maillon de la chaîne. Son rôle premier sera de préparer le travail de mixage à aux supports de diffusion. Il se devra également d’offrir une oreille nouvelle sur le travail réalisé au mixage."
  },
  {
    "objectID": "generalites/production_musicale.html#la-préproduction",
    "href": "generalites/production_musicale.html#la-préproduction",
    "title": "4  Description d’une production musicale type",
    "section": "4.2 La préproduction",
    "text": "4.2 La préproduction\nLa préproduction concerne toutes les étapes d’une œuvre enregistrée qui ont lieu avant ledit enregistrement. On parlera donc en premier lieu de la composition et particulièrement de l’arrangement.\nLa qualité d’un arrangement aura une influence énorme sur la facilité à mixer une œuvre. Si les instruments sont astucieusement répartis sur l’ensemble du spectre sonore, cela sera une difficulté de moins à gérer au mixage par exemple.\nIl est aussi courant pour des artistes de réaliser des « démos ». Celles-ci sont souvent des enregistrements réalisés en home studio afin de définir un cap esthétique pour la suite de la production sonore. C’est un atout extrêmement précieux pour un preneur de son, cela permet de rapidement identifier quel est le projet esthétique de l’œuvre."
  },
  {
    "objectID": "generalites/production_musicale.html#la-production",
    "href": "generalites/production_musicale.html#la-production",
    "title": "4  Description d’une production musicale type",
    "section": "4.3 La production",
    "text": "4.3 La production\nC’est ici que le travail du preneur de son commence. L’étape de production consiste à fixer les interprétations définitives. Le premier objectif est donc de s’assurer du bon enregistrement de tous les canaux prévus. Bien sûr, l’enjeu n’est pas seulement technique, mais aussi esthétique. Et il n’est pas moindre, les choix pris lors de la prise de son seront des carcans impossibles à outrepasser lors de la phase de mixage. Enfin, l’élément le plus déterminant de cette étape est d’obtenir des musiciens leurs meilleures interprétations. La présence d’un directeur artistique est d’une aide précieuse afin de diriger et d’orienter les musiciens. Il permet aussi de faire le lien entre les artistes et l’équipe technique, en exprimant les besoins des uns aux autres.\nSur les projets les plus modestes, le poste de directeur artistique est souvent sacrifié. Il en va donc à l’ingénieur du son de, parfois, remplir ce rôle."
  },
  {
    "objectID": "generalites/production_musicale.html#la-postproduction",
    "href": "generalites/production_musicale.html#la-postproduction",
    "title": "4  Description d’une production musicale type",
    "section": "4.4 La postproduction",
    "text": "4.4 La postproduction\nArrivé à ce stade, la majorité du travail est déjà accompli, il ne reste que le mixage et le mastering. Classiquement, chacune de ces tâches incombe à un technicien différent. Le travail du mixeur consistera à réaliser la sommation, généralement en stéréo, de l’ensemble des canaux enregistrés lors de la prise de son. Afin de faire cohabiter tous ces signaux, il est commun d’utiliser des traitements pour les répartir sur l’ensemble du spectre et de gérer leur dynamique. Parfois, ces traitements remplissent un rôle esthétique, en déformant le signal d’origine pour aboutir à une nouvelle matière.\nUne fois le travail du mixeur terminé, le mastering commence. Le but et d’homogénéiser l’ensemble des titres d’un disque, en volume, en dynamique et en couleur. Ensuite, il convient aussi de définir le niveau de sortie général du disque. La dernière étape consistera à monter l’ordre des morceaux pour le disque, d’y inscrire les métadonnées (nom de l’artiste, des titres, genre musical, etc.) et de générer le fichier final, dédier à l’exploitation."
  },
  {
    "objectID": "equipement-usages/le_chemin_du_signal.html",
    "href": "equipement-usages/le_chemin_du_signal.html",
    "title": "5  Le chemin du signal",
    "section": "",
    "text": "La première mission d’un preneur de son est d’assurer l’arrivée à bon port des signaux dans l’enregistreur. En effet, toute notion de mise en scène sonore et d’esthétique devient très secondaire si le contenu n’a pas été enregistré.\nLe diagramme ci-dessous reprend les principaux étages rencontrés par un signal audio dans un contexte de production numérique. Il est essentiel d’être le plus familier possible avec ces différents composants.\n{out.width=“100%”, fig.align=“center”}\nNous pourrions catégoriser à partir de ce schéma différents « milieux ». Tout d’abord, nous avons le milieu acoustique, où nous trouverons toutes sortes d’instruments de musique, les différents lieux dans lesquels ils pourront s’y trouver. C’est donc le domaine de l’onde sonore mécanique.\nOn trouve ensuite le milieu analogique, où l’onde sonore est représentée, de façon analogue, par des grandeurs électriques. Celles d’un signal sonore dans un circuit analogique sont fonction, par exemple, de la variation de la pression atmosphérique provoquée en un point par une onde sonore. Les éléments clefs du milieu analogique sont les préamplificateurs et les amplificateurs, mais on trouve aussi certains traitements, comme les égaliseurs et les compresseurs. On définira « analogique » comme une représentation dans laquelle les grandeurs (tension, courant, etc.) qui entrent dans les calculs sont représentées par des grandeurs analogues et qui varient de manière identique (définition du CNRTL).\nPour passer du milieu acoustique au milieu analogique, et vice-versa, on utilise des microphones et des haut-parleurs. Tous deux sont des transducteurs, permettant de transformer une énergie en une autre. Le microphone transforme une énergie mécanique en énergie électrique. Le haut-parleur réalise l’opération inverse.\nOn en vient ensuite au milieu numérique. Fondamentalement, le signal est toujours de nature électrique, mais il a subi une opération très importante nommée échantillonnage. On a donc mesuré à intervalle régulier la tension électrique générée par l’onde sonore. Le passage par le numérique permet une myriade de traitements sur le signal, beaucoup plus complexes que ceux permis par l’électronique analogique. L’audio numérique permet aussi un stockage de l’information à moindre coût et l’acheminement d’un grand nombre de voies (canaux) grâce à un faible nombre de modulations (câble).\nL’appareil permettant de passer du milieu analogique au milieu numérique est le convertisseur analogique/numérique. Il ne s’agit pas d’un transducteur, car les signaux d’entrées et de sorties sont de même nature électrique. Pour opérer l’opération inverse, on utilise un convertisseur numérique/analogique.\nLe milieu informatique nous permet d’utiliser des applications relatives aux traitements du son par le biais d’ordinateurs. Il s’agit aujourd’hui indubitablement de notre outil de travail principal. Nous y réalisons la grande majorité des traitements audio, ainsi que l’enregistrement et le routage des sources.\nLe lien entre un ordinateur et un convertisseur A/N/A se fait grâce à un bus de sérialisation associé à un pilote (ou driver). L’ensemble des deux permet de mettre en forme la donnée numérique et de la rendre compréhensible à l’ordinateur.\nChaque élément évoqué ci-dessus sera abordé dans des sections dédiées dans la suite de ce livre."
  },
  {
    "objectID": "equipement-usages/les_microphones.html#petit-historique-des-microphones",
    "href": "equipement-usages/les_microphones.html#petit-historique-des-microphones",
    "title": "6  Les microphones",
    "section": "6.1 Petit historique des microphones",
    "text": "6.1 Petit historique des microphones\nSans vouloir rentrer dans un récit exhaustif sur l’invention et l’évolution des microphones, relater les moments clefs de cette technologie permet d’avoir une vision globale du marché d’aujourd’hui.\nLa nécessité de capter un évènement sonore grâce à un microphone provient de trois besoins :\n\nle transmettre (télécommunication)\nl’amplifier (concert, spectacle vivant)\nl’enregistrer (industrie du disque)\n\nEn 1876, Alexandre Graham Bell propose un système à base liquide, permettant de transformer une onde sonore en tension électrique. Le système ne fut jamais réellement exploité, car le rendu sonore était jugé trop peu satisfaisant.\nLe premier type de microphone utilisé industriellement est le microphone à charbon (au UK, par David Edward Hugues, aux US par Emile Berliner et Thomas Edison. Le brevet sera d’ailleurs disputé, avec un gain de cause pour Edison malgré des démonstrations publiques de Hugues antérieur aux publications d’Edison). En raison de sa faible bande passante et de son niveau de bruit élevé, il se révèle de piètres qualités pour l’enregistrement et la transmission de la musique. Il aura, par contre, une place de choix dans les téléphones durant de longues décennies.\nViennent ensuite les microphones à condensateur, dont les premiers modèles remontent à 1916, par le chercheur Edward Wente. Ces microphones sont tout d’abord réputés assez capricieux, leurs réponses en fréquences pouvant varier significativement en fonction de l’humidité de l’air et de la température.\nÀ cause de ces variations sonores présentes dans les premiers microphones à condensateur, on leur préférera un temps les microphones à ruban. Ils sont inventés en 1923 par Harry Olson. Ils sont par contre d’une grande fragilité mécanique.\nGeorge Neumann est un des noms à connaître dans cette histoire des microphones. On lui doit, entre autres, la stabilisation des microphones statiques. Il sera aussi le premier à produire un microphone (U87) utilisant un transistor en lieu et place des traditionnels tubes.\nÀ partir des années 1970, les microphones dynamiques arrivent sur le marché, notamment porté par la marque Shure. Ces microphones ont la grande qualité d’être très robustes, et remplaceront leurs homologues à ruban dans bien des cas.\nDepuis, les principales améliorations ont concerné la robustesse d’une part, et la miniaturisation des dispositifs d’autre part, menant ainsi au développement des capsules MEMS."
  },
  {
    "objectID": "equipement-usages/les_microphones.html#les-types-et-technologies-de-microphones",
    "href": "equipement-usages/les_microphones.html#les-types-et-technologies-de-microphones",
    "title": "6  Les microphones",
    "section": "6.2 Les types et technologies de microphones",
    "text": "6.2 Les types et technologies de microphones\nAvant d’aborder en détail certaines constructions de microphones, il convient de faire attention à certains raccourcis associant des méthodes de fabrications à un niveau présumé de qualité. Par exemple, il est commun d’associer les microphones à électret à une construction « bas de gamme ». Or, c’est oublier que la série 4000 de chez DPA, considérée par beaucoup comme une référence indétrônable de la prise de son, ne contient que des microphones à électret. Les MEMS souffrent du même biais, ceux-ci se retrouvent pourtant de plus en plus souvent sur des microphones ambisoniques, comme le Zyla ou le SPC mic.\nNous allons maintenant aborder les types de microphones suivants :\n\nLes microphones électrostatiques/à condensateur\nLes microphones à ruban\nLes microphones dynamiques\n\n\n6.2.1 Les microphones électrostatiques/à condensateur\n\n\n\n  \n\n\nNeumann U87, Schoeps CMC64, Line Audio CM4\n\n\n\nCe sont, historiquement, les premiers microphones à permettre une captation du spectre audible satisfaisante. Ils sont cependant très sensibles aux conditions de température et d’humidité et il fallut attendre les années trente pour que ce problème cesse. Ils nécessitent une alimentation externe, appelée alimentation fantôme, normalisée à +48V. Il existe deux familles de microphones électrostatiques, les condensateurs à hautes fréquences et condensateur polarisés en courant continu.\nLes microphones à condensateur polarisés en courant continu ont le fonctionnement le plus commun. Un courant continu vient polariser la capsule/condensateur. Lorsqu’une onde sonore rencontre la capsule, une de ses armatures se déforme et génère une variation de tension analogue à la variation de pression.\nLes microphones à condensateur à haute fréquence proposent une approche différente. Un oscillateur est intégré dans le microphone et la variation de pression enregistrée par le condensateur vient moduler la fréquence de cet oscillateur. Le signal est ensuite démodulé dans la plage audible. Cette méthode de construction offre une impédance de sortie plus faible et une plus grande résistance aux variations de conditions climatiques.\nConcernant leurs caractéristiques, ces microphones possèdent des réponses en fréquence souvent très linéaire et une excellente réponse en transitoire. Leur niveau de sortie (sensibilité) est élevé. Leur impédance de sortie est basse.\nExemples : Neumann U87/AKG C414/Shoeps CMC4/Série 4000 DPA/Série MKH Sennheiser\n\n\n6.2.2 Les microphones à ruban\n\n\n\n  \n\n\nRoyer R121, Cascade Vinjet, Coles 4038\n\n\n\nLes microphones à ruban souvent préférés à leurs homologues statiques dans les débuts de la musique enregistrée. Leur fonctionnement repose sur l’utilisation d’une feuille métallique placée entre deux aimants. Lorsqu’une onde sonore rencontre cette feuille (le ruban), celle-ci vibre et perturbe le champ électromagnétique créé par les aimants et génère une tension analogue à la variation de pression.\nD’un point de vue sonore, les microphones à ruban ont souvent un bas du spectre assez généreux et une réponse plutôt douce pour les hautes fréquences. Ils sont aussi connus pour avoir une impédance de sortie assez élevée et un niveau de sortie faible. Attention à l’alimentation fantôme (+48V), elle peut endommager le microphone.\nExemples : Royer R121/Cohles/Beyerdynamic M160\n\n\n6.2.3 Les microphones dynamiques\n\n\n\n  \n\n\nShure SM57, Electro-Voice RE20, Sennheiser MD441\n\n\n\nLes microphones dynamiques sont conçus pour des conditions d’utilisation rudes, où les niveaux sonores sont élevés et où le risque de chute est important. Ils sont donc monnaie courante en sonorisation. Leur membrane est attachée à une bobine entourant un aimant. Lorsqu’une onde sonore la met en vibration, la bobine se déplace autour de l’aimant, et, par perturbation du champ électromagnétique, génère une tension de sortie analogue à la variation de pression.\nLeur réponse en fréquence est souvent accidentée, particulièrement dans le haut du spectre. Cela peut être vu comme un inconvénient ou comme un outil de « coloration » du son. Comme leurs homologues à ruban, ils possèdent un niveau de sortie faible et une impédance de sortie élevée.\nExemples : Shure SM57/Electrovoice RE20/Sennheiser MD441\n\n\n6.2.4 La taille des membranes\nLa taille des membranes influe sur la captation du son. Plus la capsule est grande, plus les fréquences aiguës seront diffractées et donc atténuées dans la prise de son. Un microphone à petite membrane est donc techniquement un microphone plus « juste ». Cependant, l’emploi de large membrane permet aussi d’adoucir un surplus d’énergie dans le haut du spectre.\n\n\n6.2.5 Microphones à tubes ou transistors?\nHistoriquement, les tubes ont été les premiers composants électroniques à permettre l’amplification du signal. Le transistor est apparu à la fin des années 40 et a permis de remplir les mêmes fonctions qu’un tube, par une consommation moindre et avec un encombrement beaucoup plus faible.\nCertains microphones continuent à être fabriqués avec des tubes, préférant leur comportement vis-à-vis du son. Une écrasante majorité est cependant fabriquée avec des transistors.\nLe choix entre un microphone à tube et un microphone à transistor semble cependant anecdotique par rapport à son type, à son placement et à sa directivité."
  },
  {
    "objectID": "equipement-usages/les_microphones.html#timbre-et-directivité",
    "href": "equipement-usages/les_microphones.html#timbre-et-directivité",
    "title": "6  Les microphones",
    "section": "6.3 Timbre et directivité",
    "text": "6.3 Timbre et directivité\nLa directivité d’un microphone permet de décrire sa capacité à réaliser une « écoute » sélective de son environnement. On rencontre les directivités suivantes :\n\nOmnidirectionnel : capte l’ensemble du champ sonore de façon indifférenciée.\nHypercardioïde : compromis entre Omnidirectionnel et cardioïde.\nCardioïde : capte à l’avant, mais rejette à l’arrière du microphone.\nSupercardioïde : ressers la zone d’écoute avant au prix de l’apparition d’une résurgence arrière.\nHypercardioïde : ressers davantage la zone d’écoute et augmente d’autant plus la résurgence arrière.\nBidirectionnel : capte à l’avant et à l’arrière, mais selon un lobe plus resserré qu’en cardioïde.\n\n\n\n\n\n\n\nPlus la directivité d’un microphone est large, plus la contribution de l’acoustique est apparente. Le timbre est également aussi linéaire que possible. À l’inverse, plus la directivité tant à être étroite, plus le microphone aura une capacité à échantillonner seulement une zone de l’espace. Le timbre est, par contre, amoindri dans le bas du spectre. Les microphones omnidirectionnels sont donc les plus larges et les plus « neutres », tandis que les microphones bidirectionnels sont les plus focalisés et ont la plus importante perte dans le bas du spectre.\nLe preneur de son choisit donc une directivité en fonction de la tâche à accomplir. Les microphones directifs ont l’avantage de limiter la contribution d’évènements sonores que l’on ne souhaite pas capter. Les microphones omnidirectionnels ont la faculté d’être un dispositif de prise de son plus transparent, mais seront beaucoup plus sensibles à une acoustique moins optimale, ainsi qu’au bruit environnant.\nNous allons par la suite nous intéresser au cœur du microphone : sa capsule. Il existe deux familles de capsules, celles dites « à pression », et celles dites à « gradient de pression ».\n\n6.3.1 Capsules à pression\nUne capsule sensible à la pression est omnidirectionnelle : elle capte les fluctuations de pressions en un point. Mathématiquement, cette relation s’exprime, en coordonnées polaires, par :\n\\[ \\theta = 1 \\]\nL’angle d’incidence de l’onde sonore par rapport au microphone importe donc peu.\nPour réaliser une capsule à pression, on enferme une partie de la membrane dans un milieu acoustique à pression constante.\n\n\n6.3.2 Capsules à gradient de pression\nUne capsule à gradient de pression est sensible à la variation du champ de pression. Ces capsules ne sont plus omnidirectionnelles, mais bidirectionnelles : elles captent devant et derrière elles. Mathématiquement, une telle directivité s’exprime par la relation (en coordonnées polaires) :\n\\[ \\theta = cos(\\alpha) \\]\nOù \\(\\alpha\\) est l’angle d’incidence d’un son par rapport à la capsule.\nPour réaliser une capsule à gradient de pression, il suffit de laisser exposer les deux faces de la membrane aux variations de pressions.\n\n\n6.3.3 Et les autres directivités ?\nIl est possible, à partir des deux équations ci-dessus, de retrouver toutes les autres directivités. Par exemple, un microphone cardioïde a une équation de directivité polaire tel que :\n\\[ \\theta(\\alpha) = \\frac{1}{2}(1 + cos[\\alpha]) \\]\nElles découlent donc des deux directivités primaires : omnidirectionnelle et bidirectionnelle. Pour obtenir une directivité particulière, il suffit de « doser » l’influence de ces deux directivités. Par exemple, un microphone cardioïde possède une contribution égale de chacune d’elles. Plus on augmente la proportion de la directive bidirectionnelle, plus on tend vers un microphone supercardioïde, voire hypercardioïde. À l’inverse, augmenter la proportion de la directivité omnidirectionnelle fait tendre le microphone vers une directivité hypocardioïde.\nIl existe deux solutions pour agir sur la contribution des directivités primaires. La première consiste à utiliser un labyrinthe acoustique pour changer le milieu acoustique d’une des faces de la membrane. L’autre consiste à avoir une capsule omnidirectionnelle et une seconde bidirectionnelle et de sommer leur tension de sortie.\nLes microphones à multidirectivité permettent à l’utilisateur d’influer, soit sur le labyrinthe acoustique, soit sur la sommation des deux capsules. Il n’est pas rare que ces microphones soient moins performants qu’un microphone spécifiquement dédié à une seule directivité.\nOn retiendra donc :\n\\[ \\theta(\\alpha) = A + B \\cos (\\alpha) \\> où \\> A + B = 1\\]\n\n\n\n6.3.4 Directivités réelles & détimbrage\nNous avons jusque là considéré que la directivité d’un microphone était un phénomène indépendant de la fréquence. Or, cela n’est pas vrai. En d’autres termes, la directivité d’un microphone n’est pas la même en fonction de la fréquence de l’onde sonore lui arrivant. Typiquement, un microphone tendra vers une directivité plus resserrée dans le haut du spectre, et vers une directivité plus large dans le bas du spectre.\n\n\n\nRéponse en fréquence du microphone Neumann U87 (omnidirectionnel)\n\n\n\n\n\nRéponse en fréquence du microphone Neumann U87 (cardioïde)\n\n\n\n\n\nRéponse en fréquence du microphone Neumann U87 (figure en huit)\n\n\nCela signifie donc que positionner un microphone hors axe face à un évènement sonore n’aura pas seulement un effet sur le niveau du signal en sortie du microphone, mais également sur le timbre. On appelle alors « timbré », un évènement sonore capté plein axe par un microphone, et détimbré un évènement sonore capté hors axe.\nCe phénomène est un outil précieux pour les preneurs de son. Par exemple, lorsqu’on enregistre une voix, certains sons sont exagérés par le microphone, particulièrement les « s ». En tournant légèrement le microphone pour placer la voix hors axe, on peut déjà grandement améliorer les problèmes de sifflante avant même de penser à un éventuel traitement ultérieur."
  },
  {
    "objectID": "equipement-usages/transport_signaux_analogiques.html#anatomie-dun-câble",
    "href": "equipement-usages/transport_signaux_analogiques.html#anatomie-dun-câble",
    "title": "7  Transport de signaux analogiques",
    "section": "7.1 Anatomie d’un câble",
    "text": "7.1 Anatomie d’un câble\nLes câbles véhiculant un seul signal sont généralement constitués de quatre à cinq composants :\n\nd’un cœur composé d’un filament (souvent multibrin) en un métal conducteur, véhiculant le signal électrique.\nd’une gaine isolante (plastique) protégeant le cœur\nd’une tresse en cuivre connectée à la masse\nParfois, d’une feuille de cuivre, enrobant la tresse, permettant de réaliser une cage de faraday et de protéger le cœur des ondes électromagnétiques.\nEnfin, d’une dernière gaine isolante, permettant de protéger l’ensemble du câble.\n\n\nCeux permettant de transporter plus de signaux rajouteront des cœurs multibrins entourés de leur gaine isolante. La plupart du temps, les câbles véhiculent un ou deux signaux à la fois, mais certains permettent d’en acheminer beaucoup plus (multipaire, Sub-D). On appelle un câble en fonction de ses connecteurs (ou fiches).\n\n7.1.1 Longueur du câble, son et impédance\nÉtudions la section d’un câble à modulation unique. Nous pouvons faire plusieurs observations. Le fil conducteur du signal, généralement en cuivre, est d’une longueur non négligeable. Plus cette longueur est importante, plus ce fil aura une résistance importante. De plus, ce fil est séparé de la tresse de masse (élément également conducteur) par un isolant. Il existe donc un effet de capacitance entre le point chaud et la masse. Enfin, la tresse de masse peut être comparée à une bobine, et possède donc une inductance. Nous pouvons donc assimiler un câble à un circuit RLC filtrant le haut du spectre audio.\nPhysiquement, notre description précédente est valide, mais elle est à corréler à l’impédance de sortie de la source. Typiquement, lors de l’utilisation d’un microphone statique, son impédance est suffisamment faible pour que la longueur du câble soit totalement transparente. Certains microphones dynamiques ou à ruban, possédant une impédance plus élevée, peuvent très légèrement souffrir de la longueur du câble.\nCe phénomène d’altération du timbre à cause de la longueur d’un câble a surtout lieu avec les instruments électriques passifs (guitares et basses). L’impédance de sorte de ces instruments est tellement grande que l’on peut aisément entendre la différence de son entre deux câbles.\n\nIl est amusant de constater que certains musiciens utilisent ce phénomène, et jouent avec des câbles volontairement trop longs, pour atténuer le haut du spectre. Brian May et Eric Johnson en sont deux exemples.\n\n\n\n7.1.2 Les connexions asymétriques\nLes connexions asymétriques permettent de transporter un signal entre une source et un récepteur. Il s’agit de la façon la plus simple de connecter deux appareils devant échanger des signaux. Cependant, sur de longue distance, le câble peut se comporter comme une antenne et induire sur le signal certaines ondes électromagnétiques (comme la radio). Ces connexions ne nécessitent qu’un fil conducteur par modulation.\n\n\n7.1.3 Les connexions symétriques\nLe but de ces connexions est de palier au problème des connexions asymétrique. Dans l’appareil émetteur, le signal à transporter est dupliqué, et ce duplicata est inversé en phase. Cette étape s’appelle la symétrisation.\n\nC’est deux signaux qui sont appelés point chaud (signal d’origine) et point froid (signal opposé en phase). Sur le trajet du câble, lorsqu’une perturbation électromagnétique est induite sur le signal, celle-ci s’inscrit en phase sur les deux conducteurs (point chaud et point froid). À l’arrivée, l’appareil récepteur inverse la phase du point froid et le somme avec le point chaud. Cette étape se nomme la dé-symétrisation. Ainsi le signal d’origine est sommé en phase, tandis que les interférences sont sommées hors phase et s’annulent.\nLes connexions symétriques nécessitent deux fils conducteurs pour chaque modulation."
  },
  {
    "objectID": "equipement-usages/transport_signaux_analogiques.html#les-fiches-connecteurs",
    "href": "equipement-usages/transport_signaux_analogiques.html#les-fiches-connecteurs",
    "title": "7  Transport de signaux analogiques",
    "section": "7.2 Les fiches & connecteurs",
    "text": "7.2 Les fiches & connecteurs\n\n\n\n\n\n\nJack TS, Jack TRS, Jack Bantam, XLR, Sub-D 25\n\nOn appelle « fiche » les éléments mécaniques situés aux extrémités d’un câble et permettant sa connexion à un équipement. Celle-ci nous permet de facilement identifier le type de câble que nous avons entre les mains.\n\nContrairement aux dires de certains mythes, majoritairement reliée à l’audiophilie, la différence de matériau utilisé pour le contact de la fiche n’a pas d’incidence sur le son.\n\nOn rencontre principalement les connectiques jack TS, jack TRS, jack bantam, XLR et Sub-D. Les fiches jack TS ont deux points de connexion. Les fiches jack TRS, XLR et bantam en ont deux. Les Sub-D 25 en possèdent vingt-cinq.\n\nLes fiches jack TS sont souvent utilisées sur les instruments électriques et électroniques (guitare, basse, synthétiseurs, etc.). Ces connexions sont nécessairement symétriques. Les jack TRS sont un peu plus rare et se trouvent généralement sur des instruments avec des sorties stéréophoniques, ou sur des équipements audio possédant des entrés/sorties symétriques. La connectique XLR remplit fondamentalement le même usage qu’un jack TRS, mais offre un verrouillage mécanique, permettant de sécuriser la connexion. On la trouve principalement sur les microphones et sur préamplificateurs. L’avantage du jack TRS est son plus faible encombrement mécanique. On le préfère donc sur les appareils possédant un grand nombre d’entrées/sorties. Le jack bantam se trouve sur les boîtiers de patch. Leur petite taille permet une grande densité de point de connexion. Les patchbay prennent ainsi moins de place. On trouve aussi les connectiques Sub-D 25, principalement pour remplacer des connexions XLR. En effet, une seule connectique Sub-D 25 permet de remplacer huit câbles XLR."
  },
  {
    "objectID": "equipement-usages/transport_signaux_analogiques.html#exemples-pratiques",
    "href": "equipement-usages/transport_signaux_analogiques.html#exemples-pratiques",
    "title": "7  Transport de signaux analogiques",
    "section": "7.3 Exemples pratiques",
    "text": "7.3 Exemples pratiques\n\n7.3.1 Câble jack « mono »\nCe type de câble est souvent utilisé sur les instruments électriques. On l’appelle « mono » car il ne véhicule qu’un seul signal.\n\n\n7.3.2 Câble jack « stéréo »\nL’appellation de ce câble est ambiguë. Le terme stéréo fait référence à ses deux voies de connexion, cependant, le nom « stéréo » impliquerait qu’une des voies est destinée à alimenter l’enceinte gauche, et l’autre, le canal droit. Or, ce type de câble peut également convenir pour des connexions symétriques.\n\nOn notera qu’il est possible de brancher un câble jack TS dans une fiche TRS. Un seul des canaux sera alors acheminé.\n\n\n\n7.3.3 Câble « Y »\nCes câbles possèdent trois connecteurs, et sont le plus souvent équipés de jack TS. Ils permettent de récupérer un signal pour le transmettre sur deux appareils. Attention, la duplication du signal étant passive, on risque un problème d’impédance si l’impédance d’entrée des deux appareils est trop différente. Le cas d’école est souvent rencontré lorsqu’on branche deux casques sur le même amplificateur. Si l’impédance des deux casques est trop différente, un des deux aura presque l’intégralité du niveau du signal alors que l’autre sous-modulera.\n\n\n7.3.4 Les connexions d’insert\nCes câbles ont la particularité d’avoir une fiche jack TRS et deux fiches jack TS. En studio, on les rencontre très souvent pour insérer un périphérique de traitement dans la chaîne audio. Le « tip » du jack TRS est connecté au « tip » d’un des jack TS qui est connectés sur l’entré du périphérique. Le deuxième jack TS est raccordé à la sortie de l’appareil et son « tip » est connecté au « ring » du jack TRS.\nCes câbles sont aussi utilisés pour séparer une sortie dite « stéréo » en deux voies « mono »."
  },
  {
    "objectID": "equipement-usages/transport_signaux_analogiques.html#routage-des-signaux",
    "href": "equipement-usages/transport_signaux_analogiques.html#routage-des-signaux",
    "title": "7  Transport de signaux analogiques",
    "section": "7.4 Routage des signaux",
    "text": "7.4 Routage des signaux\nEn studio d’enregistrement, il n’est pas rare de rencontrer plusieurs cabines de prise de son, chacune équipée de boîtier de patch. Ces boîtiers sont constitués d’un certain nombre d’entrées XLR. On achemine ensuite les sorties de ces boîtiers via des multipaires jusqu’à une « patchbay ». Le rôle de la « patchbay » est de permettre de connecter n’importe quelle entrée (signal provenant d’un microphone) vers n’importe quel préamplificateur.\nOn trouve évidemment beaucoup d’usage à ces « patchbay ». Elles sont à considérer comme la matrice de routage d’un studio d’enregistrement."
  },
  {
    "objectID": "equipement-usages/les_preamplificateurs.html#critères-de-choix-dun-préamplificateur",
    "href": "equipement-usages/les_preamplificateurs.html#critères-de-choix-dun-préamplificateur",
    "title": "8  Les préamplificateurs",
    "section": "8.1 Critères de choix d’un préamplificateur",
    "text": "8.1 Critères de choix d’un préamplificateur\nLe critère de première importance dans le choix d’un préamplificateur est son gain maximal. Plus l’amplification disponible est grande, plus le préampli sera capable de répondre à des situations exigeantes, telles que l’enregistrement d’un évènement sonore à faible niveau, ou l’emploi d’un microphone à faible sensibilité.\nLe second critère important dans le choix d’un préampli est sa réponse en fréquence. Théoriquement, celle-ci doit être la plus neutre possible. Une certaine coloration peut être acceptée (voire souhaitée), mais celle-ci doit rester raisonnable pour répondre à des critères d’utilisations professionnelles.\nLa réponse en transitoire est un autre élément important. Certains préamplis auront tendance à adoucir la sensation d’attaque des sources. Cet effet n’est pas souhaitable.\nEnfin le rapport signal sur bruit doit être le plus grand possible. Nous cherchons toujours à rajouter le moins de bruit possible sur le chemin de notre signal."
  },
  {
    "objectID": "equipement-usages/les_preamplificateurs.html#les-technologies-de-préampli",
    "href": "equipement-usages/les_preamplificateurs.html#les-technologies-de-préampli",
    "title": "8  Les préamplificateurs",
    "section": "8.2 Les technologies de préampli",
    "text": "8.2 Les technologies de préampli\nNous avons vu dans le chapitre trois qu’il existe trois familles de composants électroniques permettant d’amplifier le signal : les tubes, les transistors et les circuits intégrés. On retrouve donc des topologies de circuit de préamplificateurs utilisant chacun de ces composants.\nChacune de ces topologies offre de très légère variation de son lorsque les préamplis sont poussés dans leur retranchement (seuil de saturation). Comme pour les microphones, il est délicat de parler de son « à tube » ou « à transistor ». De plus, l’influence sur le son d’un préamplificateur apparaît en pratique comme très marginale par rapport au positionnement du microphone."
  },
  {
    "objectID": "equipement-usages/les_preamplificateurs.html#les-réglages-dun-préampli",
    "href": "equipement-usages/les_preamplificateurs.html#les-réglages-dun-préampli",
    "title": "8  Les préamplificateurs",
    "section": "8.3 Les réglages d’un préampli",
    "text": "8.3 Les réglages d’un préampli\nUn préampli propose souvent les réglages suivants :\n\nUn potentiomètre de gain (qui est souvent remplacé par un sélecteur cranté, plus précis, pour les modèles haut de gamme).\nUn bouton activant l’alimentation fantôme. En effet, c’est bien le préampli qui génère cette tension d’alimentation pour les microphones statiques.\nUn bouton d’inversion de phase.\nUn coupe-bas."
  },
  {
    "objectID": "equipement-usages/les_convertisseurs.html#la-nécessité-de-la-conversion-analogique-numérique",
    "href": "equipement-usages/les_convertisseurs.html#la-nécessité-de-la-conversion-analogique-numérique",
    "title": "9  La conversion analogique numérique",
    "section": "9.1 La nécessité de la conversion analogique numérique",
    "text": "9.1 La nécessité de la conversion analogique numérique\nDurant toute la période de l’audio analogique, le support de stockage de prédilection fut la bande magnétique. Cependant, celle-ci n’offre pas un rapport signal sur bruit très satisfaisant, limitant alors la dynamique musicale enregistrable. De plus, la bande a également un coût non négligeable. On a donc cherché à remplacer ce support afin de résoudre ces deux problèmes. Le stockage numérique offre, sous certaines conditions, une dynamique bien supérieure à celle des supports analogiques.\nUne représentation numérique de l’audio permet aussi la réalisation de traitement délicat, voire impossible, en analogique. On pense, par exemple, aux algorithmes de réverbération, d’écho et de « pitch shifting » (modification de la hauteur d’un son).\nEnfin, nos principaux outils de manipulation du son sont aujourd’hui informatiques. Dès lors, une représentation numérique des signaux est toute indiquée pour les manipuler grâce à nos ordinateurs. Il en découle donc une nécessité de bien maîtriser les principes entourant la numérisation des signaux.\n\nEn français, le « digital » est un anglicisme. Le mot correct est donc bien « numérique », et non « digital », qui qualifie ce qui a rapport au doigt."
  },
  {
    "objectID": "equipement-usages/les_convertisseurs.html#théorie-de-léchantillonnage",
    "href": "equipement-usages/les_convertisseurs.html#théorie-de-léchantillonnage",
    "title": "9  La conversion analogique numérique",
    "section": "9.2 Théorie de l’échantillonnage",
    "text": "9.2 Théorie de l’échantillonnage\n\n9.2.1 D’un signal continu vers un signal échantillonné\nUne des caractéristiques principales d’un signal analogique est qu’il est continu. Une fonction, en mathématique, est dite continue si elle est définie en n’importe quel instant. Afin d’être numérisé, un signal doit donc être dénombré. En effet, la notion d’infini imposé par la continuité du signal n’a pas d’existence en numérique.\nLa numérisation du signal est comparable à l’utilisation d’un multimètre pour mesurer une tension. Un convertisseur va prélever la valeur du signal, de façon régulière, au cours du temps.\nAfin de correctement numériser un signal, il convient de définir deux paramètres :\n\nla vitesse de prélèvement, ou fréquence d’échantillonnage\nla plage de valeur permise pour le signal, ou résolution de quantification\n\n\n\n9.2.2 La fréquence d’échantillonnage\nCette fréquence définit le nombre de prélèvements par seconde. Par exemple, un morceau édité sur un CD audio a une fréquence d’échantillonnage de 44 100 Hz (44,1 kHz), cela signifie que le signal est mesuré 44 100 fois par seconde.\nLa fréquence de travail la plus courante est 48 kHz, mais l’on rencontre parfois des valeurs supérieures, multiple de celle-ci : 96 kHz, 192 kHz, etc. Cette augmentation proportionnelle de la fréquence d’échantillonnage s’appelle suréchantillonnage. Certains techniciens espèrent ainsi améliorer la qualité de l’enregistrement. Ce suréchantillonnage à un coût en ressource CPU et en espace de stockage. Un flux audio échantillonné à 96 kHz demande deux fois plus de ressource et d’espace qu’un flux échantillonné à 48 kHz. Cette valeur initiale de 44 100 Hz (ou 48 kHz) n’a pas été choisie au hasard. Pour la comprendre, il faut revenir au phénomène physique que nous cherchons à numériser.\nRappelons que le son est une onde mécanique, et nous l’entendons lorsqu’elle oscille dans une plage de fréquence comprise entre 20 Hz (très grave) et 20 000 Hz (très aigu). Il faut donc que notre système de numérisation soit capable de reproduire une fréquence maximale allant jusqu’à 20 000 Hz. Pour cela, nous utilisons les résultats des travaux des chercheurs Harry Nyquist et Claude Shannon (tous deux ayant travaillé aux laboratoires Bell).\nLe théorème de Shannon-Nyquist stipule que, pour être capable d’échantillonner un signal de fréquence \\(f\\), la fréquence d’échantillonnage doit au moins être de \\(2f\\). Ainsi, un ensemble de points généré par une fréquence inférieure à \\(f\\) ne peut correspondre qu’à cette seule et unique fréquence. Notre plage d’écoute étant limitée à 20 kHz, la fréquence d’échantillonnage minimale dont nous avons besoin est de 40 kHz.\nQue se passe-t-il si la fréquence du signal dépasse la moitié de la fréquence d’échantillonnage ? Dans ce cas, la vitesse de prélèvement n’est plus suffisante et nous observons l’apparition de nouvelles fréquences ne provenant pas du signal original. Ce phénomène se nomme repliement spectral.\n\n\n9.2.3 La résolution de quantification\nLa résolution de quantification permet de définir la plage de valeur dynamique permise dans le système numérique. Celle-ci s’exprime en bit. Par exemple, si nous prenons un convertisseur travaillant en 8 bit. Le nombre de valeurs que peut prendre un signal numérisé par un tel convertisseur est de \\(2^8-1 = 255\\) en base 10. Admettons que ce convertisseur accepte des signaux ayant une tension en entrée variant entre +15V et -15 V, celles-ci seront échelonnées sur 255 valeurs. Si maintenant, ce convertisseur travaille en 16 bit, il y aura 65 535 échelons. La précision de mesure de la dynamique du signal n’est donc pas du tout la même.\nEn pratique, augmenter la résolution de quantification permet principalement de définir le niveau de bruit du convertisseur. Plus la résolution est élevée, plus le bruit se retrouvera faible. En 8 bit, l’écart entre le niveau maximal d’un signal et le bruit est de 48 dB, en 16 bit cet écart est de 96 dB, en 24 bit, 144 dB. On peut approximativement calculer cette dynamique par la relation suivante :\n\\[ \\Delta_L \\approx 6 \\times N_{bits} \\]\nLa résolution de quantification standard en enregistrement est 24 bit. La plage dynamique est telle qu’elle rend le travail d’enregistrement beaucoup plus souple sur les niveaux d’acquisition des différentes sources."
  },
  {
    "objectID": "equipement-usages/les_convertisseurs.html#quelle-influence-sur-le-signal",
    "href": "equipement-usages/les_convertisseurs.html#quelle-influence-sur-le-signal",
    "title": "9  La conversion analogique numérique",
    "section": "9.3 Quelle influence sur le signal ?",
    "text": "9.3 Quelle influence sur le signal ?\nLe son numérique a longtemps eu la réputation d’être « dur », particulièrement dans le haut du spectre. Cela s’explique assez facilement par le fonctionnement des premiers convertisseurs.\nEn effet, toute la difficulté de fabrication d’un convertisseur réside dans la réalisation d’un filtre anti-repliement, pour prévenir le repliement spectral. Ce filtre doit enlever toutes les fréquences au-dessus de la moitié de la fréquence d’échantillonnage, sans pour autant affecter le spectre audible. Ce type de filtre est extrêmement délicat à réaliser en analogique. Cependant, ce problème est résolu grâce à une méthode d’échantillonnage appelée « sigma-delta » (voir ci-dessous).\nLe repliement spectral n’apparaît pas seulement lors de la conversion. Il peut également survenir lors de l’utilisation de certains traitements (saturation, simulation analogique, compresseurs). Lorsqu’il devient audible, le repliement spectral se caractérise par l’apparition de fréquences non harmoniques souvent qualifiées de « dures » et désagréables. Il est cependant bon de rappeler que ce phénomène, certes bien réel, apparaît dans des conditions de saturation du signal importante et sur des sources sonores riches en hautes fréquences.\nMalgré la dure vie que mène parfois la réputation du son numérique, il est important de rappeler qu’il a apporté un grand nombre d’avantages sur le son analogique, y compris sur des questions de rendus sonores. Par exemple, la dynamique est bien plus importante, la distorsion involontaire du signal infime et l’ajout de bruit inexistant."
  },
  {
    "objectID": "equipement-usages/les_convertisseurs.html#la-conversion-sigma-delta",
    "href": "equipement-usages/les_convertisseurs.html#la-conversion-sigma-delta",
    "title": "9  La conversion analogique numérique",
    "section": "9.4 La conversion sigma-delta",
    "text": "9.4 La conversion sigma-delta\nAujourd’hui, les convertisseurs ne travaillent pas directement à 44,1 kHz/16 bit ou 48 kHz/24 bit. Ils utilisent à la place un procédé appelé échantillonnage sigma-delta. Le principe est d’utiliser une fréquence d’échantillonnage très rapide (384 kHz) et de coder la dynamique du signal, en relatif, sur un seul bit (ce bit prend une valeur de 1 si le nouvel échantillon est plus fort que l’ancien, 0 pour le cas inverse). Les formats de travail que nous utilisons sont générés après cette première étape.\nL’intérêt de cette méthode est double :\n\nLe signal est suréchantillonné dès l’enregistrement\nLes filtres permettant d’éviter le repliement spectral sont donc très simples à réaliser"
  },
  {
    "objectID": "equipement-usages/transport_signaux_numériques.html#les-liaisons-point-à-point",
    "href": "equipement-usages/transport_signaux_numériques.html#les-liaisons-point-à-point",
    "title": "10  Transports de signaux numériques",
    "section": "10.1 Les liaisons point à point",
    "text": "10.1 Les liaisons point à point\nLes protocoles ci-dessous permettent de transmettre un ou plusieurs signaux numériques entre deux appareils.\n\n10.1.1 AES3 ; AES/EBU\nL’AES3 est un protocole défini par l’Audio Engineering Society et par l’European Union Broadcast. Il est principalement destiné aux appareils audio dits « professionnels ». Il permet de véhiculer deux canaux audio, à une fréquence d’échantillonnage maximal de 48 kHz, via une fiche XLR ou BNC (coaxial).\nLa S/PDIF est relativement proche de l’AES3, plutôt utilisé dans les équipements grand public, utilisant des câbles coaxiaux (sur fiches RCA) ou optiques (fiche toslink).\n\n\n10.1.2 ADAT\nL’ADAT lightpipe, souvent abrégé ADAT, est un autre protocole de transmission de signaux numériques. Il a été développé par Alesis pour fonctionner avec les magnétophones à bandes numériques de la même marque. ADAT signifie enfaîte « Alesis Digital Audio Tape ». On retrouve ce protocole sur un grand nombre d’appareils, notamment les interfaces audio, afin d’augmenter le nombre d’entrées/sorties accessibles.\nL’ADAT peut transporter jusqu’à huit canaux à 44.1/48 kHz, quatre canaux à 88.2/96 kHz et deux canaux à 176.4/192 kHz. Le débit d’information est donc constant, doubler la fréquence d’échantillonnage divise par deux le nombre de canaux.\nLa connectique la plus courante pour l’ADAT est la fibre optique avec fiches toslink.\n\n\n\nCâble Toslink\n\n\n\n\n10.1.3 MADI\nLe MADI, ou AES10, est un protocole permettant d’acheminer un grand nombre de canaux. On peut donc récupérer soixante-quatre canaux audio à une fréquence de 44.1/48 kHz. Comme pour l’ADAT, le nombre de canaux est divisé par deux à chaque doublement de la fréquence d’échantillonnage.\nCe protocole se retrouve fréquemment dans le monde de l’audio professionnel. Les connexions entre appareils supportant le MADI peuvent se faire soit avec des fibres optiques, soit sur câble coaxial (fiches BNC). Certains constructeurs, comme DIGICO, ont choisi les câbles RJ45 comme support d’acheminement.\n\n\n\n\n\n\n\n\n\n\n\nFibre optique et câble BNC"
  },
  {
    "objectID": "equipement-usages/transport_signaux_numériques.html#les-réseaux-audio-numériques",
    "href": "equipement-usages/transport_signaux_numériques.html#les-réseaux-audio-numériques",
    "title": "10  Transports de signaux numériques",
    "section": "10.2 Les réseaux audio numériques",
    "text": "10.2 Les réseaux audio numériques\nAujourd’hui, dans le monde du spectacle vivant, la plupart des salles de spectacle sont équipées avec des solutions de transmission des signaux audio sur réseau. Ces solutions se retrouvent aussi de plus en plus dans les studios d’enregistrement et de production audiovisuelle.\nIl existe plusieurs protocoles permettant le déploiement de tels dispositifs, mais leur logique fondamentale reste identique. Chaque appareil capable de se connecter au réseau audio peut recevoir et envoyer un flux audio a n’importe quels autres appareils appartenant au même réseau.\nLes réseaux audio sont régis par les mêmes règles que les réseaux informatiques. Chaque appareil pouvant être connecté à un réseau est identifiable par une adresse matérielle unique, appelée adresse MAC. Lorsqu’un appareil est connecté sur un réseau, il faut lui attribuer une adresse logique appelée adresse IP. Il y a ici deux façons de faire. Soit l’utilisateur attribue manuellement une adresse différente à chaque machine (solution préférée en audio, mais fastidieuse lorsque le réseau comprend un grand nombre d’appareils), soit le réseau possède un serveur DHCP qui se chargera d’attribuer une adresse IP unique à chacun des appareils connectés. Cet outil est généralement intégré dans un appareil nommé routeur, permettant d’interconnecter plusieurs appareils ainsi que de gérer le routage des flux d’information. Une fois les appareils interconnectés, chaque constructeur de solutions audio sur IP fournit un logiciel de routage de l’audio entre les appareils.\nLes principaux acteurs industriels des réseaux audionumériques sont Audinet avec DANTE, ALC NetworX (appartenant à Lawo) avec Ravenna, et les protocoles open source AES67 et AVB."
  },
  {
    "objectID": "equipement-usages/introduction_informatique_musicale.html#fonctionnement-dun-ordinateur",
    "href": "equipement-usages/introduction_informatique_musicale.html#fonctionnement-dun-ordinateur",
    "title": "11  Introduction à l’informatique musicale",
    "section": "11.1 Fonctionnement d’un ordinateur",
    "text": "11.1 Fonctionnement d’un ordinateur"
  },
  {
    "objectID": "equipement-usages/introduction_informatique_musicale.html#les-systèmes-dexploitation",
    "href": "equipement-usages/introduction_informatique_musicale.html#les-systèmes-dexploitation",
    "title": "11  Introduction à l’informatique musicale",
    "section": "11.2 Les systèmes d’exploitation",
    "text": "11.2 Les systèmes d’exploitation\n\n11.2.1 Linux\n\n\n11.2.2 Microsoft Windows\n\n\n11.2.3 Apple MacOS"
  },
  {
    "objectID": "equipement-usages/introduction_informatique_musicale.html#les-pilotes-audio",
    "href": "equipement-usages/introduction_informatique_musicale.html#les-pilotes-audio",
    "title": "11  Introduction à l’informatique musicale",
    "section": "11.3 Les pilotes audio",
    "text": "11.3 Les pilotes audio\n\n11.3.1 ASIO\n\n\n11.3.2 CoreAudio\n\n\n11.3.3 ALSA\n\n\n11.3.4 Jack Audio"
  },
  {
    "objectID": "equipement-usages/introduction_informatique_musicale.html#les-stations-de-travail-audio-numérique-daw",
    "href": "equipement-usages/introduction_informatique_musicale.html#les-stations-de-travail-audio-numérique-daw",
    "title": "11  Introduction à l’informatique musicale",
    "section": "11.4 Les Stations de Travail Audio-Numérique (DAW)",
    "text": "11.4 Les Stations de Travail Audio-Numérique (DAW)\n\n11.4.1 Le moteur audio\n\n\n11.4.2 Les fonctionnalités"
  },
  {
    "objectID": "equipement-usages/introduction_informatique_musicale.html#les-protocoles-de-transmission-dinformations",
    "href": "equipement-usages/introduction_informatique_musicale.html#les-protocoles-de-transmission-dinformations",
    "title": "11  Introduction à l’informatique musicale",
    "section": "11.5 Les protocoles de transmission d’informations",
    "text": "11.5 Les protocoles de transmission d’informations\n\n11.5.1 MIDI\n\n\n11.5.2 OSC"
  },
  {
    "objectID": "equipement-usages/les_enceintes.html#anatomie-dun-haut-parleur",
    "href": "equipement-usages/les_enceintes.html#anatomie-dun-haut-parleur",
    "title": "12  Enceintes et amplificateurs",
    "section": "12.1 Anatomie d’un haut-parleur",
    "text": "12.1 Anatomie d’un haut-parleur\n\n\n\n  \n\n\nCoupe de hautparleur (dans l’ordre, woofer, mid-range, tweeter). Infographie par Svjo, CC BY-SA 3.0\n\n\n\nSur les schémas ci-dessus, nous trouvons les éléments suivants :\n\nL’aimant\nLa bobine\nLa suspension\nLa membrane\n\nLa plupart des haut-parleurs sont qualifiés de dynamiques. La membrane du haut-parleur est reliée à une bobine, elle-même entourée par un système d’aimants. Lorsqu’un courant est appliqué aux bornes de cette bobine, sa position change dût à la modification du champ électromagnétique. Si le courant oscille, la bobine oscille de façon analogue, entraînant la membrane et permet donc la reproduction du son.\nOn appelle généralement « subwoofer » les haut-parleurs conçus pour retranscrire les fréquences très graves (20-200 Hz), « woofer » les haut-parleurs dédiés aux fréquences graves (50 Hz à 1000 kHz), « mid-range » les haut-parleurs du médium (1 kHz à 6 kHz), et « tweeter », ceux de l’aigu (au-delà de 5 kHz).\nLe haut-parleur est sans doute l’appareil audio le plus imparfait qui soit. Il est sujet à de nombreuses sources de distorsion du signal.\nNous ne savons pas fabriquer des haut-parleurs capables de reproduire uniformément toutes les fréquences. Ces derniers sont souvent spécialisés dans une certaine plage de fréquence. La plupart des enceintes de monitoring utilisent 3 voies : deux actives (utilisant des haut-parleurs) pour l’aigu et le médium, et une passive (évent avant ou arrière) pour le grave. L’utilisation du plusieurs voies imposent donc l’utilisation de filtres induisant un déphasage de certaines fréquences.\nÉgalement, un haut-parleur peut être approché par un modèle « masse-ressort ». Cela signifie qu’il y a une certaine inertie à sa mise en action et une certaine inertie à sa mise en arrêt. L’enceinte idéale devrait posséder une inertie nulle. Cette inertie est potentiellement responsable d’un adoucissement des transitoires et d’une sensation de flou."
  },
  {
    "objectID": "equipement-usages/les_enceintes.html#amplification-et-impédance",
    "href": "equipement-usages/les_enceintes.html#amplification-et-impédance",
    "title": "12  Enceintes et amplificateurs",
    "section": "12.2 Amplification et impédance",
    "text": "12.2 Amplification et impédance\nNous avons précédemment abordé le préamplificateur, qui permet d’amplifier la tension d’un signal audio analogique et d’en baisser son impédance. On appelle alors « amplificateur » un amplificateur de puissance. On rappel que la puissance d’un signal s’exprime par la relation ci-dessous. On cherche donc à augmenter la tension et l’intensité du signal.\n\\[ P = U \\times I \\]\nNous pouvons rapidement aborder la notion de classe d’amplification. En audio, nous n’utilisons que les classes A, AB et D. La classe A utilise un transistor (ou tube) pour amplifier l’ensemble du signal. Elle possède un très mauvais rendement. Cela signifie qu’il faut fournir beaucoup d’énergie au transistor pour un faible gain sur la puissance du signal. La classe AB utilise deux transistors, un pour les alternances négatives et un pour les alternances positives. Le rendement est meilleur que pour la classe A. Cependant, le point de raccordement entre les deux transistors est assez sensible et peu généré de la distorsion sur le signal (distorsion de croisement). La classe D, aussi appelée à tort « numérique », utilise un transistor afin d’indiquer l’état du signal. On retrouve donc la même idée que dans l’échantillonnage du signal. Ces amplificateurs offrent un excellent rendement.\nNous avons également abordé précédemment la notion d’adaptation d’impédance en tension, sans aborder l’adaptation d’impédance en puissance. Pour rappel, afin de préserver la tension entre deux appareils A et B, nous faisons en sorte d’avoir une faible impédance à la sortie de l’appareil A et une grande impédance à l’entré de l’appareil B. Pour préserver la puissance du signal, ce paradigme ne fonctionne plus. On cherche alors à avoir la même impédance entre la sortie d’un appareil et l’entrée d’un autre. En pratique, on raccordera, sur la sortie « 8 ohms » d’un amplificateur, un haut-parleur ayant une impédance de « 8 ohms ».\n\nOn remarque qu’ici, les impédances sont extrêmement faibles. Les impédances typiques des haut-parleurs (et donc des sorties d’amplificateurs en puissance) sont 4, 8 et 16 ohms.\n\nLa plupart des enceintes de monitoring ont aujourd’hui une amplification de classe D directement intégré."
  },
  {
    "objectID": "equipement-usages/les_enceintes.html#puissance-et-sensibilité",
    "href": "equipement-usages/les_enceintes.html#puissance-et-sensibilité",
    "title": "12  Enceintes et amplificateurs",
    "section": "12.3 Puissance et sensibilité",
    "text": "12.3 Puissance et sensibilité\nOn trouve généralement deux mesures de la puissance pour les enceintes. La puissance crête à crête (peak) et la puissance moyenne (RMS, ou Root Mean Square). La puissance se calcule grâce à la formule suivante :\n\\[ P =\\frac {U^2}{Z} \\]\n\\(P\\) est la puissance, \\(U\\) la tension et \\(Z\\) l’impédance. Pour réaliser la mesure de puissance d’un haut-parleur, on le soumet à un signal test, en général un bruit rose, pendant plusieurs heures. La puissance crête à crête se calcul grâce à la tension crête à crête (aussi dite maximale). Par exemple, pour une tension maximale de 100 V, sous une impédance de 8 ohms, on trouve une puissance crête à crête d’environ 1200 watts (abbrégé W). On considère généralement que la tension moyenne d’un signal est six décibels plus petite que sa tension maximale. En reprenant notre exemple, pour une tension maximale de 100 V, on a une tension moyenne de 50 V, sous une impédance de 8 ohms, on trouve une puissance moyenne d’environ 300 W.\nAlors, il faut donc faire très attention sur les spécifications données par les constructeurs, parfois volontairement floues. À défaut, si on lit que la puissance admissible d’un haut-parleur est de 1000 W, on supposera par défaut qu’il s’agit d’une puissance crête à crête. On ne dépassera donc pas une puissance moyenne d’amplification de 250 W.\nLa sensibilité est une mesure du niveau sonore à un mètre de l’enceinte pour une puissance RMS d’entrée d’un watt. Grâce à cette valeur, on peut calculer le niveau sonore produit par le haut-parleur à diverses distances et en fonction de différentes puissances d’entrées. On comprend aussi que la puissance électrique admissible dans l’enceinte ne donne que peu d’information sur son niveau sonore de sortie."
  },
  {
    "objectID": "equipement-usages/les_enceintes.html#conseils-pratiques",
    "href": "equipement-usages/les_enceintes.html#conseils-pratiques",
    "title": "12  Enceintes et amplificateurs",
    "section": "12.4 Conseils pratiques",
    "text": "12.4 Conseils pratiques\n\n12.4.1 Choisir une paire d’écoutes\nChoisir une paire d’enceintes peut sembler être un exercice difficile. Il existe énormément de modèles, coûtant de quelques dizaines d’euros à plusieurs dizaines de milliers.\nIl y a cependant plusieurs critères assez objectifs pour évaluer la qualité d’une enceinte :\n\nLa réponse en fréquence : l’enceinte flatte-t-elle particulièrement une zone du spectre ? En délaisse-t-elle une autre ?\nLa réponse en transitoires : les attaques sont-elles respectées ? Retrouve-t-on l’énergie initiale du signal ?\nLa linéarité en fonction du volume : a-t-on une sensation de compression du signal lorsque l’on augmente le niveau envoyé dans l’enceinte ?\nLe centre fantôme : le centre du système stéréophonique paraît-il stable ? Paraît-il précis ?\nLa couleur sonore de l’enceinte : a-t-on plaisir à écouter du son et de la musique sur ce système ?\n\n\n\n12.4.2 Placer correctement son écoute\nAfin de satisfaire les critères de la stéréophonie, il convient de respecter les règles suivantes :\n\nLes deux enceintes doivent être de même marque, de même modèle et appairée. Si une des membranes a dû être changée sur l’une d’elle, l’autre aurait dû recevoir la même opération.\nLes deux enceintes doivent être séparées par un angle de 60°\nL’auditeur doit être placé à équidistance des deux haut-parleurs, et regarder vers le milieu du segment formé par les deux enceintes.\n\nUne fois ces critères respectés, voici quelques conseils sur le placement des enceintes dans une pièce :\nOn préférera des pièces de grandes tailles, afin de repousser au maximum le temps d’arrivée des premières réflexions. Le système stéréophonique devrait être positionné dans un souci de symétrie : l’enceinte de gauche ne devrait pas être plus proche d’un mur que l’enceinte de droite, par exemple. Dans le cas de petit espace, on préféra coller les enceintes contre un mur. Cela permettra de supprimer l’influence d’une des premières réflexions au prix de l’augmentation du niveau de grave. Il est vivement recommandé de procédé au traitement, même minimal, acoustique de la pièce de travail, à commencer par les zones de réflexions premières et par les angles (ou le grave va s’accumuler). Si le traitement acoustique n’est pas envisageable, il convient de privilégier une écoute à faible niveau et une proximité maximale avec les enceintes."
  },
  {
    "objectID": "equipement-usages/les_enceintes.html#lécoute-au-casque",
    "href": "equipement-usages/les_enceintes.html#lécoute-au-casque",
    "title": "12  Enceintes et amplificateurs",
    "section": "12.5 L’écoute au casque",
    "text": "12.5 L’écoute au casque\nLe casque est un outil permettant d’écouter un signal tout en s’extrayant de son environnement (acoustique et/ou bruit). Cependant, de par son mode de fonctionnement, à savoir deux haut-parleurs placés dans une enceinte en contact direct avec les oreilles, il génère un certain nombre de déformations.\nPremièrement, la stéréophonie écoutée au casque est hypertrophiée. En effet, dans ces conditions d’écoutes, l’oreille gauche n’entend que le haut-parleur gauche et l’oreille droite n’entend que le haut-parleur droit.\nDeuxièmement, il est très difficile de trouver des casques avec une réponse en transitoire satisfaisante. Il convient donc d’être excessivement prudent lorsque l’on mix du contenu percussif sur un casque.\nTroisièmement, les casques sont encore moins linéaires en fréquence que les haut-parleurs, il convient là aussi d’être très prudent lors de la réalisation d’un mixage.\nCes défauts peuvent être compensés par l’habitude et la connaissance du système d’écoute, mais la transportabilité d’un mixage (à savoir, sa compatibilité avec d’autres systèmes d’écoute) réalisé au casque est souvent discutable.\n\n12.5.1 Casque fermé ou casque ouvert ?\nLe casque fermé, comme son nom l’indique, propose une fabrication enfermant le haut-parleur dans une enceinte close. Cette méthode de fabrication offre l’avantage d’isoler celui qui écoute de l’environnement, mais aussi d’isoler l’environnement de ce qui est diffusé dans le casque. Par contre, ces casques ont souvent une réponse en fréquence très accidentée, et ne sont pas recommandés pour le mixage. Il est par contre vivement recommandé pour les musiciens en session de prise de son.\nLe casque ouvert, à l’inverse de son homologue fermé, n’offre aucune isolation acoustique, au prix d’une meilleure réponse en fréquence du casque. Ces casques sont tout indiqués pour le mixage, mais beaucoup moins pour des situations de prise de son."
  },
  {
    "objectID": "methode-pds/mono_et_multimicrophonie.html",
    "href": "methode-pds/mono_et_multimicrophonie.html",
    "title": "13  Mono et multimicrophonie",
    "section": "",
    "text": "(En cours d’écriture)"
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#généralités-sur-les-mécanismes-de-la-localisation-du-son-par-loreille-humaine",
    "href": "methode-pds/le_couple_de_pds.html#généralités-sur-les-mécanismes-de-la-localisation-du-son-par-loreille-humaine",
    "title": "14  La prise de son au couple",
    "section": "14.1 Généralités sur les mécanismes de la localisation du son par l’oreille humaine",
    "text": "14.1 Généralités sur les mécanismes de la localisation du son par l’oreille humaine\nAfin de mieux comprendre comment fonctionne un couple de prise de son, il convient d’étudier rapidement les principes fondamentaux de notre écoute.\nNotre capacité à localiser les sons dans l’espace repose principalement sur deux mécanismes : + La différence de temps + La différence de niveau\n\n14.1.1 La localisation par différence de temps\nNos oreilles sont espacées, d’environ 15 à 25 cm. Cette distance implique qu’un son émis plus proche de l’oreille droite arrivera également plus tôt qu’à l’oreille gauche. Cet écart de temps, de quelques millisecondes, est suffisant pour donner à notre cerveau un indice sur la localisation du son.\nAfin de sentir l’ordre de grandeur en jeu, calculons la différence de temps (\\(\\Delta t\\)) maximale pour un individu possédant un écart d’oreille de 20 cm.\nOn sait que la célérité du son dans l’air vaut \\(c = 340 m.s^{-1}\\), et est invariant en fonction de la fréquence. On sait également que \\(c = \\frac{d}{t}\\).\nDès lors, si on pose \\(d = 20 cm\\) soit \\(d = 0.2 m\\), on peut en déduire que :\n\\(t = \\frac{d}{c} \\iff t= \\frac{0.2}{340} \\approx 0.0006 s \\approx 0.6 ms\\)\nAfin de mettre en relief ce résultat, il est communément admis que l’oreille humaine commence à faire la différence entre deux répétitions d’un même son à partir de \\(20 ms\\).\n\n\n14.1.2 La localisation par différence d’intensité\nA priori, l’espace entre nos deux oreilles n’est pas creux. La densité de notre crâne et de son contenu va réfléchir et absorber une partie des fréquences rencontrées.\nÉgalement, la partie externe de nos oreilles, appelées pavillon, permet, grâce à sa forme, de donner une directivité à notre écoute.\nEn d’autres termes, notre tête et le pavillon de nos oreilles se comportent comme un filtre, variant en fonction de l’angle d’incidence de la source. Cette altération du timbre n’est pas perçue comme une coloration, mais bien comme une information de localisation. La modélisation mathématique de ces filtres se retrouve dans la littérature scientifique sous le nom HRTF.\nCette atténuation séquentiellement dépendante est décisive dans notre capacité à localiser les sons. On la retrouve communément sous le nom \\(\\Delta i\\).\n\n\n14.1.3 Prévalence fréquentielle de ces deux phénomènes\nIl est communément admis que le \\(\\Delta t\\) aura une efficacité maximale dans les basses fréquences, et le \\(\\Delta i\\) dans les hautes fréquences."
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#principes-de-la-prise-de-son-au-couple",
    "href": "methode-pds/le_couple_de_pds.html#principes-de-la-prise-de-son-au-couple",
    "title": "14  La prise de son au couple",
    "section": "14.2 Principes de la prise de son au couple",
    "text": "14.2 Principes de la prise de son au couple\nPour créer son effet stéréophonique, les couples de prise de son utilisent les mêmes mécanismes que notre écoute naturelle :\n\nLa différence de temps\nLa différence d’intensité\n\nIl va de soi que, pour fonctionner de façon optimale, les microphones utilisés pour réaliser une prise de son stéréophonique doivent être de même marque, de même modèle et appairée.\n\nL’appairage garantit que les microphones aient des caractéristiques techniques suffisamment proches pour être considérés comme identiques.\n\nAfin de manipuler ces mécanismes, le preneur de son peut jouer sur les paramètres suivant :\n\nLa directivité des microphones\nL’angle entre les capsules\nLa distance entre les capsules\n\nModifier chacun de ses paramètres influe sur l’angle de prise de son. Plus l’ange de prise de son est faible, plus l’impression de stéréophonie sera grande. Plus l’angle de prise de son est grand, plus l’impression de stéréophonie sera faible, jusqu’à tendre vers la monophonie.\n\nAttention de ne pas confondre l’angle de prise de son avec l’angle entre les capsules.\n\n\n14.2.1 Comment choisir un angle de prise de son.\nL’angle de prise de son est étroitement lié à la distance du couple par rapport à l’évènement sonore à enregistrer. En règle générale, plus le couple est loin des objets sonores à enregistrer, plus son angle de prise de son sera faible. À l’inverse, plus le couple sera proche, plus son angle de prise de son sera grand.\nEnsuite, lors de la réalisation d’un couple de prise de son, il est commun d’enregistrer un ensemble d’éléments : plusieurs instruments (batterie), voire plusieurs musiciens (quatuor à corde, orchestre). L’objectif est bien souvent de retrouver une sensation de disposition des éléments dans l’espace proche de la situation réelle. On cherche donc un angle de prise de son suffisamment petit pour que les sources occupent l’intégralité de l’espace stéréophonique, mais également suffisamment grand pour ne pas créer une sensation de trou au centre.\n\n\n14.2.2 Comment réaliser un angle de prise de son.\nPlusieurs outils existent pour aider le preneur de son à configurer son angle de prise de son correctement.\nIl est important de commencer par évoquer les abaques de Michael Williams, ayant cherché à étudier l’angle de prise de son et ses qualités en fonction des paramètres vues précédemment. Les résultats de ses travaux se trouvent sur le site mmad.info.\nOn trouve également beaucoup d’application mobile, comme celle du constructeur du microphone Neumann, s’appuyant sur les travaux de Michael Williams pour aider leurs utilisateurs à correctement positionner leurs microphones. Évidemment, et heureusement, rien n’est spécifique à un fabricant de microphones en particulier, l’application d’un constructeur A peut servir pour placer des microphones d’un constructeur B.\nPlus récemment, des chercheurs britanniques ont développé une application, nommée MARRS, permettant de positionner son couple de prise de son par rapport aux sources via une interface graphique très simple à utiliser. Cette application est disponible sur mobile et sur navigateur internet.\n\n\n14.2.3 Privilégier le \\(\\Delta i\\) ou le \\(\\Delta t\\) ?\nLa différence de perception du champ stéréophonique est très différente entre celui produit par le \\(\\Delta i\\) ou par le \\(\\Delta t\\).\n\nUn couple reposant sur le \\(\\Delta i\\) aura une sensation de localisation des sources précise. De plus si un tel couple enregistre une source ce déplaçant a vitesse constante, la sensation de déplacement retranscrite par le couple sera, elle aussi, linéaire. Il est également possible de sommer les deux microphones ensemble afin d’obtenir un signal monophonique. Un tel couple est appelé compatible mono.\nUn couple reposant sur le \\(\\Delta t\\) aura une sensation de localisation plus floue, mais apportera un sens de l’espace plus grand et une dimension spacieuse. À l’inverse d’un couple \\(\\Delta i\\), la sensation d’un déplacement linéaire d’une source n’est pas linéaire. Il n’est pas possible de sommer les deux capsules pour en obtenir une réduction mono sans générer des altérations de timbre sévères.\n\nChaque couple possède ses avantages et ses inconvénients. Heureusement, nous ne sommes pas limités à l’un où l’autre et nous pouvons à loisir réaliser une combinaison des deux mécanismes."
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#les-topologies-classiques-de-prise-de-son-au-couple",
    "href": "methode-pds/le_couple_de_pds.html#les-topologies-classiques-de-prise-de-son-au-couple",
    "title": "14  La prise de son au couple",
    "section": "14.3 Les topologies classiques de prise de son au couple",
    "text": "14.3 Les topologies classiques de prise de son au couple\nLe premier ingénieur à se poser la question du son stéréophonique est l’anglais Alan Blumlein en 1929. Il imagine l’entièreté de la chaîne d’enregistrement et de diffusion nécessaire à la stéréophonie. Cependant, la BBC lui impose comme contrainte que toutes ses propositions soient compatibles avec des systèmes monophoniques. Il inventera donc le couple XY et MS.\nPlus tard, la plupart des radios européennes développeront des couples de prises de son mêlant \\(\\Delta i\\) et \\(\\Delta t\\), tel que l’ORTF.\n\n14.3.1 Le couple Blumlein / XY\nLes deux microphones sont ici directifs, placés au même point de l’espace et ongulé d’une certaine valeur entre eux.\nDe par les contraintes technologiques de son époque, Blumlein a décrit ce couple pour une utilisation de deux microphones bidirectionnels. Il est aujourd’hui plus commun de le rencontrer avec deux cardioïdes.\nDans sa version originale, le couple Blumlein comprend donc deux microphones bidirectionnels avec un angle de 90°.\nLa formulation du couple XY comprend deux microphones cardioïdes avec un angle compris entre 90° et 135°.\n\n\n14.3.2 Le couple MS\nLe couple MS, également inventé par Alan Blumlein, permet de doser la quantité de stéréophonie après l’enregistrement.\nPour se faire, ce couple utilise deux microphones :\n\nUn omnidirectionnel, historiquement, mais aujourd’hui fréquemment remplacé par un microphone cardioïde.\nUn bidirectionnel\n\nLe microphone omnidirectionnel, ou cardioïde, va rendre compte du centre de la stéréophonie, tandis que le microphone bidirectionnel rendra compte de la latéralité.\nUne fois enregistrés, ces deux canaux ont besoin d’être convertis, plus exactement dématricés, vers une paire de canaux stéréophonique. L’opération est très simple :\n\\[L = M+S\\] \\[R = M-S\\]\nCette opération peut être réalisée sur une console de mixage, telle que décrite ci-dessous.\n\n\n\nDématriçage MS\n\n\n\n\n14.3.3 Le couple ORTF\nLe couple ORTF, inventé par la radio française du même nom, combine l’effet du \\(\\Delta i\\) et du \\(\\Delta t\\) afin de s’approcher de l’écoute humaine.\nSa topologie est précisément définie. Elle propose l’utilisation d’une paire de microphones cardioïde, ongulé du 110° et avec un écart de 17 cm.\n\n\n14.3.4 Les couples AB\nLes couples AB peuvent avoir une définition ambiguë. Une partie de la littérature scientifique considère comme couple AB tout couple non coïncident. À cet égard l’ORTF est considéré comme un couple AB. Pour d’autre, les couples AB ne concernent que des couples constitués de microphones omnidirectionnels.\nCes derniers ont la particularité de n’utiliser que le \\(\\Delta t\\) afin de placer les sources dans l’espace. Le rendu est donc souvent spacieux, au prix d’une certaine instabilité et d’un certain manque de précision de l’image stéréophonique."
  },
  {
    "objectID": "methode-pds/le_couple_de_pds.html#compléter-une-prise-de-son-au-couple-par-des-appoints",
    "href": "methode-pds/le_couple_de_pds.html#compléter-une-prise-de-son-au-couple-par-des-appoints",
    "title": "14  La prise de son au couple",
    "section": "14.4 Compléter une prise de son au couple par des appoints",
    "text": "14.4 Compléter une prise de son au couple par des appoints\nIl est commun, lors d’une prise de son au couple, de chercher à obtenir une entière satisfaction sonore à la seule aide du couple. Cependant, cela n’est parfois pas possible, souvent pour des contraintes physiques et acoustiques (un instrument de l’ensemble jouant moins fort que les autres). Dans ces cas, l’utilisation d’appoint, donc de microphone supplémentaire, placé en proximité de la source, va permettre de venir récupérer une précision supplémentaire de l’instrument.\nLors de l’étape de mixage, le couple servira de base principale et l’on viendra ajouter la quantité nécessaire d’appoints pour préciser le propos. Il sera parfois nécessaire de remettre en phase l’appoint et le couple pour améliorer la sommation de l’ensemble."
  },
  {
    "objectID": "methode-pds/considérations_pratiques.html#le-confort-du-musicien",
    "href": "methode-pds/considérations_pratiques.html#le-confort-du-musicien",
    "title": "15  Considérations pratiques",
    "section": "15.1 Le confort du musicien",
    "text": "15.1 Le confort du musicien\nMême si elle peut sembler triviale, cette « étape » de la chaîne de prise de son est de loin la plus importante. La qualité de l’interprétation donnée par le musicien dépendra grandement de son état moral et psychologique :\n\nEst-il stressé\nEst-il confiant\nSe sent-il accueilli\netc.\n\nNous pourrions considérer qu’un ou une musicienne arrivant dans un studio d’enregistrement se présente avec un taux de confiance maximal envers l’équipe technique. Dès lors l’objectif des différents techniciens est de conserver cette jauge au maximum.\nLes premières minutes sont particulièrement importantes et va poser un ressenti fort sur la journée de travail. Il y a donc un équilibre à trouver entre un accueil chaleureux et décontracté et un rapport productiviste et sérieux.\nLe système permettant aux musiciens de communiquer entre eux et avec les techniciens est primordial. En pratique, il n’est pas rare de dédier certains microphones du plateau à cette tâche. Du côté régi, le « talkback » est l’outil de communication premier des techniciens présents sur la session. Il convient de l’utiliser avec soin et prudence. Un musicien peut rapidement se sentir isolé, s’il enregistre seul. Il convient de maintenir un contact régulier et précis afin de ne pas l’abandonner dans le seul dans sa cabine. Qui plus est, un quiproquo peut être vite arrivé avec les systèmes de talkback. Prudence quant à l’état d’ouverture ou de fermeture du microphone."
  },
  {
    "objectID": "methode-pds/considérations_pratiques.html#le-choix-de-linstrument",
    "href": "methode-pds/considérations_pratiques.html#le-choix-de-linstrument",
    "title": "15  Considérations pratiques",
    "section": "15.2 Le choix de l’instrument",
    "text": "15.2 Le choix de l’instrument\nLa plupart des musiciens se présenteront avec leurs instruments. La marge de manœuvre est donc ici quasi nulle.\nCependant il n’est pas rare que le studio possède du « backline », souvent composé de batteries, d’amplificateur guitare et basse, voire de guitares et de basses. Si l’instrument utilisé par le musicien pose problème pour la prise de son, proposer une alternative peut s’avérer être un bon pari. Il convient évidemment de sonder l’ouverture du musicien par rapport à cette proposition, afin de ne pas le braquer.\nIl peut également être intéressant de « préparer » les instruments. Cette technique est très courante sur les pianos et les batteries, afin de changer les propriétés acoustiques de l’instrument grâce a l’utilisation de draps, coussins, couvertures disposées dans ou sur l’instrument."
  },
  {
    "objectID": "methode-pds/considérations_pratiques.html#le-choix-de-lacoustique",
    "href": "methode-pds/considérations_pratiques.html#le-choix-de-lacoustique",
    "title": "15  Considérations pratiques",
    "section": "15.3 Le choix de l’acoustique",
    "text": "15.3 Le choix de l’acoustique\nL’acoustique de la salle d’enregistrement est-elle aussi plus souvent une contrainte qu’une variable d’ajustement.\nOn préférera souvent de grandes salles afin de limiter l’apparition prématurée de premières réflexions. Plus la salle sera petite, plus celle-ci apportera une forte coloration sur le contenu enregistré. Il convient donc d’être attentif aux petites cabines de studio, celles-ci sont souvent très mates, mais leur apport sur le timbre des instruments qui y sont enregistrés est souvent très important.\nLorsque l’on a la possibilité d’enregistrer dans de grandes salles, il est souvent intéressant de disposer des quelques panneaux acoustiques mobiles, afin de modeler la pièce à sa convenance.\nSi l’acoustique imposée est défavorable, on préférera dans ce cas des prises d’hyper proximité, afin de minimiser son effet au maximum."
  },
  {
    "objectID": "methode-pds/considérations_pratiques.html#placer-et-choisir-son-microphone",
    "href": "methode-pds/considérations_pratiques.html#placer-et-choisir-son-microphone",
    "title": "15  Considérations pratiques",
    "section": "15.4 Placer et choisir son microphone",
    "text": "15.4 Placer et choisir son microphone\nEn pratique, il est bien difficile de dissocier le choix du microphone de son placement, les deux étant très interdépendants. Cependant, il convient de garder à l’esprit que le positionnement du microphone est, parmi les deux, sans doute le plus déterminant.\nLa première étape, avant même de choisir un microphone, consiste à écouter l’instrument dans l’acoustique d’enregistrement. Il s’agit ici d’une écoute active. On se déplace autour de l’instrument, on s’en approche, on s’en éloigne, afin de sentir l’interaction entre la source et l’acoustique du lieu. Aussi, il est important de trouver deux zones d’émission particulière de l’instrument : la zone de projection maximale et la zone au timbre le plus favorable. La première peut nous servir à positionner l’instrumentiste par rapport aux autres instruments afin de minimiser les reprises entre microphones. La deuxième zone nous indique l’axe de prise de son.\nCette zone au timbre le plus favorable est relative. Elle dépend de l’instrument, bien sûr, mais aussi du modèle. Elle dépend également du mode de jeu, de l’articulation du joueur et évidemment, de l’esthétique de la musique.\n\n15.4.1 Le rapport a la distance du microphone\nLa distance de positionnement du microphone est un élément excessivement important sur le rendu esthétique de la prise de son.\nEn règle générale, plus on prend de distance, plus on approche une prise de son naturaliste, cherchant à reproduire un évènement sonore dans son environnement, tel qu’il aurait été entendu dans la pièce. Plus on se rapproche, plus on fragmente l’événement sonore et plus on l’arrache aussi a son contexte de diffusion.\nAfin de déterminer efficacement le placement d’un microphone, il convient d’abord d’en connaître sa distance critique. Celle-ci correspond au point, dans une pièce, où le son provenant directement d’une source est perçu au même niveau sonore que la réponse acoustique à cette source. Cela signifie que si nous plaçons notre microphone au-delà de ce point, nous obtiendrons plus d’acoustique que de son direct de l’instrument.\nIl est important aussi de considérer que la directivité du microphone influe sur la distance critique. En effet, plus la directivité du microphone est large (tends vers l’omnidirectionnalité), plus le microphone paraîtra éloigné de la source. À l’inverse, plus la directivité d’un microphone est étroite (tends vers la bidirectionnalité), plus le microphone paraîtra proche.\nDans le cas de l’utilisation de microphone directif, le placement en proximité et hyperproximité va créer une accentuation du contenu basse-fréquence de la source. Cela devient parfois un élément esthétique, comme sur les voix radiophoniques. Cela aussi peut être un défaut, une exagération qu’il conviendra de corriger en postproduction.\n\n\n15.4.2 Quand choisir une prise de son stéréophonique\nLa prise de son stéréophonique, comme son nom l’indique, regroupe l’ensemble des techniques de prise de son dédié au système de diffusion stéréophonique (deux enceintes séparées de 60° et orientées vers un auditeur placé à équidistance des deux transducteurs).\nL’avantage de tels dispositifs de prises de son est de peupler dès la prise l’espace stéréophonique qui est donné à l’auditeur lors de la diffusion. Ils permettent également de rendre compte de la position de plusieurs évènements sonores ayant lieu dans la même acoustique. Cette dernière est d’ailleurs bien mieux retranscrite par de tels systèmes de prise de son.\nIl s’agit à nouveau d’un choix esthétique. Faire le choix d’une prise de son monophonique permet de renforcer la sensation de frontalité et de densité d’une source. À l’inverse, une prise de son stéréophonique donnera une définition spatiale accrue.\n\n\n15.4.3 Quand choisir la multi-microphonie\nLa multi-microphonie consiste à enregistrer un instrument via l’utilisation de microphones (principalement) directifs, placés à différents endroits jugés pertinents et en hyperproximité.\nCette approche esthétique de la prise de son est devenue indissociable des « musiques actuelles ». Elle offre l’avantage d’une grande flexibilité de traitement lors de la phase de mixage. Voir, elle implique une certaine partie des traitements.\nEn effet, une prise d’hyperproximité va systématiquement relever deux défauts :\n\nun effet de proximité : le grave/bas médium de la source paraît hypertrophié lors de l’emploi de microphones directifs.\nLes dynamiques de jeux sont également hypertrophiées.\n\nL’effet de proximité implique donc bien souvent l’utilisation d’un égaliseur, permettant de corriger cette augmentation artificielle du grave. De même, l’hypertrophie de la dynamique de jeu implique l’usage d’un compresseur afin de corriger ces variations artificielles.\nAfin de recréer une sensation de spatialisation, on utilisera principalement deux outils. En premier lieu, le potentiomètre de panoramique afin de diriger ces sons mono dans l’espace stéréophonique, puis les réverbérations artificielles permettra de reconstituer un champ acoustique et de réintégrer ces sources dans une scène sonore.\nSi l’approche de la prise au couple pouvait être qualifiée de naturaliste, alors la prise de son en multimicrophone sera son pendant spectaculaire. Évidemment, il convient de ne pas aussi franchement opposer ces deux approches et il existe tout un monde de système de prise de son entre ces deux extrêmes."
  },
  {
    "objectID": "methode-pds/considérations_pratiques.html#le-choix-du-préamplificateur",
    "href": "methode-pds/considérations_pratiques.html#le-choix-du-préamplificateur",
    "title": "15  Considérations pratiques",
    "section": "15.5 Le choix du préamplificateur",
    "text": "15.5 Le choix du préamplificateur\nLe rôle du préamplificateur est d’amplifier le signal, le tout en ramenant le minimum de bruit. Un premier élément de choix de préampli va se faire sur le niveau de pression acoustique produit par les sources à enregistrer.\nEnregistrer une batterie impose peut de contrainte sur le préampli quand a sa capacité à amplifier sans rajouter beaucoup de bruit sur le signal. À l’inverse, enregistrer des instruments peux sonores, possiblement avec des microphones peux sensibles, implique l’utilisation de préampli avec une excellente réserve de gain et un excellent rapport signal bruit.\n\n15.5.1 L’influence du préampli sur la « couleur » du son\nIl est assez connu que le préampli peut également devenir un choix esthétique pour influencer la couleur d’une prise de son. Cette question semble assez complexe. Voici quelques éléments de réponse :\n\nLe choix du préampli est d’une influence minime par rapport à tous les autres choix précédemment fait.\nLes préamplis sont souvent catégorisés, en termes de couleur, via les composants utilisés pour réaliser l’amplification. Attention, un composant électronique dépend toujours du contexte dans lequel il est placé (ici, du circuit électronique). Il est donc difficile de précisément qualifier le son d’un préampli à lampe ou à transistor de façon générique.\nLes impédances d’entrée des préamplis ne sont souvent pas évoquées dans ces discussions. Hors, pour la plus parts des microphones (hors statiques), leur impédance de sortie peut être suffisamment élevée pour engendrer une déperdition en aigu et en transitoire. Cette déperdition peut être heureuse, ou malheureuse, mais surtout bien réelle. Une manière de s’en prémunir peut-être d’utiliser des « booster » de microphones (parfois également appelés préamplis), permettant d’augmenter le niveau de sortie des microphones et aussi d’adapter leur impédance."
  },
  {
    "objectID": "methode-pds/déphasage_et_remise_en_phase.html#les-effets-sonores-de-déphasage",
    "href": "methode-pds/déphasage_et_remise_en_phase.html#les-effets-sonores-de-déphasage",
    "title": "16  Déphasage et remise en phase",
    "section": "16.1 Les effets sonores de déphasage",
    "text": "16.1 Les effets sonores de déphasage\nTous les signaux sont caractérisés par une certaine phase. Celle-ci est moins tangible que celles de niveau sonore ou de fréquence. En effet, lorsqu’un signal est écouté seul, celle-ci ne s’entend pas. C’est au moment où plusieurs signaux corrélés (comprendre, enregistrés au même moment, par plusieurs microphones) sont sommés que les différences de phase peuvent s’entendre."
  },
  {
    "objectID": "methode-pds/déphasage_et_remise_en_phase.html#approche-mathématique",
    "href": "methode-pds/déphasage_et_remise_en_phase.html#approche-mathématique",
    "title": "16  Déphasage et remise en phase",
    "section": "16.2 Approche mathématique",
    "text": "16.2 Approche mathématique\nPrenons l’exemple d’un son pur : \\(sin (\\omega t + \\phi)\\) où \\(\\omega = 2\\pi f\\)\nLa phase de ce signal est décrite par \\(\\omega t +\\phi\\)\nLes deux paramètres responsables de déphasages audibles sont :\n\n\\(t\\), le temps\n\\(\\phi\\), la phase à l’origine\n\nAttention, pour un son pur, l’effet de la modification de \\(t\\) ou de \\(\\phi\\) semble très similaire. Ce n’est pas le cas pour des signaux pseudo-périodiques, atténués dans le temps."
  },
  {
    "objectID": "methode-pds/déphasage_et_remise_en_phase.html#les-sources-de-déphasage",
    "href": "methode-pds/déphasage_et_remise_en_phase.html#les-sources-de-déphasage",
    "title": "16  Déphasage et remise en phase",
    "section": "16.3 Les sources de déphasage",
    "text": "16.3 Les sources de déphasage\nLes causes les plus classiques de déphasages sont :\n\nUn câble XLR avec une inversion sur le point chaud et le point froid\nUne prise de son avec une différence de distance entre deux microphones\nUne prise de son utilisant deux microphones positionnés de part et d’autre d’une membrane\nUn retard de certaines fréquences lié aux objets rencontrés par les signaux"
  }
]