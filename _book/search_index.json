[["index.html", "La prise de son musicale 1 Avant-propos 1.1 Mise à jour 1.2 Structures", " La prise de son musicale Jean-Loup Pecquais 2022-12-24 1 Avant-propos Ce livre est né de la nécessité d’un support de cours pour la formation “Technique de Prise de Son”, dispensée par Jean-Loup Pecquais. Il intègre donc l’ensemble des notions abordées, expliquées en détail, ainsi que des exemples sonores. Ce livre est écrit dans la philosophie de l’Open Source. L’intégralité de son contenu est donc disponible gratuitement. Son code source est accessible dans un dépôt GitHub. Ainsi, il est possible à tout à chacun de reporter les éventuelles erreurs ou de proposer des modifications. 1.1 Mise à jour La distribution numérique de ce livre permet une mise à jour régulière de son contenu. Cela implique deux choses : Certaines sections peuvent être incomplètes, et seront complétées plus tard C’est une bonne idée de revenir consulter ce site régulièrement 1.2 Structures Dans un premier temps, ce livre reprend l’ensemble de la chaîne audio, en y explicitant le rôle et le fonctionnement de chacun de ses composants. L’objectif est de fournir une base technique objective au preneur de son. Dans un second temps, le livre détaille un ensemble de techniques de prise de son, insistant particulièrement sur les mécanismes généraux de la prise et sur l’écoute critique. Cependant, un certain nombre “trucs &amp; astuces” de prises de son sont aussi présentées. Ces derniers ne doivent jamais se supplanter à l’écoute critique. "],["quantifier-et-qualifier-le-son.html", "2 Quantifier et qualifier le son 2.1 Phénomène physique 2.2 Perception du son", " 2 Quantifier et qualifier le son Le son peut s’appréhender de plusieurs façons différentes. Particulièrement, sa description physique et psychoacoustique est très précieuse pour tous les praticiens du son. Il convient donc, afin de pouvoir proposer un dispositif cohérent de prise de son, de comprendre la physique élémentaire du son ainsi que d’être capable de le décrire efficacement. 2.1 Phénomène physique 2.1.1 Quelques définitions Le son est une vibration mécanique d’un fluide. Dans le cadre de ce cours, nous ne considérerons que l’air comme médium de propagation. Cette onde cause une variation de la pression dans l’espace. Nous, les êtres humains, le percevons grâce à notre ouïe. Il s’agit donc, par définition, d’un phénomène ondulatoire et peut être caractérisé par un nombre d’oscillations par seconde, aussi appelé fréquence. On estime que notre espèce est sensible aux fréquences allant de 20 Hz (très grave) jusqu’à 20 000 Hz (très aigu). On parlera d’évènement sonore pour parler généralement de phénomènes physiques produisant une onde sonore. Les sons composés d’une seule fréquence se nomment sons purs. Cependant, de tels signaux n’existent pas dans la nature, et sont souvent utilisés afin de réaliser des mesures ou des tests psychoacoustiques. Dans notre environnement, les sons sont donc composés de plusieurs fréquences. La fréquence la plus grave d’un son est sa fréquence fondamentale. Les autres sont alors appelées partiels. Si ces partielles ont pour fréquence un multiple de la fréquence fondamentale, alors on les nomme harmoniques. Plus généralement, on admettra que la composition fréquentielle, ou spectrale, de tout son peut être décomposée par une somme de sinusoïde. L’outil permettant de passer de la représentation temporelle d’un signal à sa représentation fréquentiel s’appelle la transformée de Fourrier. La fréquence fondamentale donne la hauteur du son (sa note en musique par exemple). Les partiels enrichissent cette fréquence fondamentale et créés le timbre d’un son. C’est en partie grâce au timbre que l’on peut reconnaître différents instruments de musiques jouant la même note. Un son se caractérise également par l’évolution de son amplitude au cours du temps. On parle alors de son enveloppe. Un modèle courant d’enveloppe est l’ADSR : Attack, Decay, Sustain, Release, soit Attaque, Décroissance, Maintient et Relâchement. Figure 2.1: Exemple d’enveloppe ADSR Lorsque son temps et très bref, l’ensemble attaque et décroissance forme les transitoires. Cette partie du signal est responsable de la sensation percussive du son. 2.1.2 Relation entre temps, distance et fréquence Il est important de garder à l’esprit que les notions de temps, de fréquence et de distance sont étroitement liées. Nous avons vu ci-dessus que tous les sons peuvent être décrits par une somme de sinusoïde. Leur fréquence la plus grave, dite fondamentale, permet de définir la période. La période est le temps que met un signal à répéter son motif oscillatoire (voir schémas 3.1 et 3.2). Le lien mathématique entre fréquence et période est très simple, car l’un est l’inverse de l’autre : \\[ f = \\frac 1 T \\] Si nous étudions les fréquences extrêmes, audibles par notre ouïe, nous trouvons que pour \\(f_{min} = 20 \\,Hz\\), sa période \\(T_{f_{min}} = 50 \\,ms\\). Pour \\(f_{max} = 20\\,000 \\,Hz\\), \\(T_{f_{max}} = 0.5 \\,ms\\). Une onde sonore est également caractérisée par sa célérité. Celle-ci est constante dans un milieu donné. Dans l’air, à une température de \\(15 \\,°C\\) et au niveau de la mer, sa célérité \\(c\\) est de \\(340\\,m.s^{-1}\\). On admettra cette valeur pour réaliser l’ensemble de nos différents calculs. Comme son unité l’indique, la célérité du son est homogène à une distance divisée par un temps, soit : \\[ c =\\frac d t \\] Suivant cette formule, nous pouvons alors calculer la longueur d’onde correspondant à une fréquence. La longueur d’onde se note \\(\\lambda\\). \\[ \\lambda = cT \\; \\iff \\; \\lambda = \\frac c f\\] Si nous étudions à nouveau les bornes minimale et maximale de notre audition, nous trouvons que \\(\\lambda_{f_{min}} = 17 \\,m\\) et \\(\\lambda_{f_{max}} = 17 \\,mm\\). Nous pouvons également calculer le temps de propagation du son. En pratique, nous serons souvent intéressés par le temps de propagation séparant deux points dans l’espace (par exemple, le temps séparant deux microphones par rapport à un instrument). Figure 2.2: Distance entre deux microphones. \\[ t = \\frac {d_2-d_1}{c}\\] 2.2 Perception du son Nous avons abordé quelques notions de physique permettant de mieux caractériser le phénomène sonore. Comme indiqué au début de ce chapitre, le son peut également être discuté sous l’angle de notre ouïe, et donc, de notre perception. Cette branche de la science se nomme la psychoacoustique et cherche à étudier la façon dont nous percevons le son. Notre corps, et a fortiori notre cerveau, sont des machines extrêmement complexes. Nous sommes équipés d’une multitude de capteurs permettant de sentir le contact d’une matière, des odeurs, d’entendre, de goûter, de voir, de positionner nos membres dans l’espace, de ressentir la douleur, etc. Pris indépendamment, chacun de ces sens est déjà un phénomène complexe à décrire, mais il existe en plus une grande interdépendance entre ceux-ci. Par exemple, l’interdépendance entre la vision et l’audition est à l’origine d’un certain nombre de mécanismes biaisant notre écoute. Nous nous bornerons au fil de ce cours à quelques notions liées à l’ouïe et à son interdépendance à d’autre sens quand cela sera pertinent. 2.2.1 Spectre, timbre et vocabulaire D’un point de vue perceptif, le spectre d’un évènement sonore est facilement remarquable. Il est, par contre, beaucoup plus difficile à qualifier. Il n’est pas rare de rencontrer les adjectifs “chaud”, “brillant”, “rond”, “aéré”, “ouvert”, “sombre”, voir d’autres encore plus ésotérique, pour tenter de communiquer la sensation ressentie à l’écoute de tel ou tel son. Cette difficulté liée à l’absence de vocabulaire commun quant à la qualification le son emmène systématiquement la redéfinition de ce vocable en fonction de son interlocuteur. En effet, le mot “rond” ne signifiera pas forcément la même chose selon à qui on s’adresse. Une stratégie possible consiste à questionner son interlocuteur sur l’utilisation de ses adjectifs tout en cherchant à y associer des exemples sonores. Nous pouvons tout de même nous essayer à cet exercice pour nous permettre d’avoir un vocabulaire commun au fil de ce cours. Vous aurez sans doute compris qu’il n’y aura, dans les termes employés, aucun critère absolu. Figure 2.3: Distance entre deux microphones. Proposition d’association entre bandes de fréquences et sensation. 20 Hz — 80 Hz : Subharmonique, sensation tripale 80 Hz — 160 Hz : Grave, sensation d’assise 160 Hz — 380 Hz : bas-médium, sensation de « chaleur », voir « boueux » 380 Hz — 1400 Hz : Medium, sensation de « boîte » quand trop présent, sonne « creux » quand trop absent 1400 Hz — 3200 Hz : Haut-medium : zone de sensibilité maximale de l’oreille. 3200 Hz — 8000 Hz : Aigu, apporte de la précision voir de l’agressivité 8000 Hz — 20 000 Hz : Air, apporte une sensation d’ouverture voir de finesse Il est intéressant de former son oreille à reconnaître une plage de fréquence, ainsi que d’y associer son propre vocabulaire et une sensation. Les appellations proposées ci-dessus ne sont à prendre que comme guides et n’ont pas valeur de référence. Cela favorise une écoute critique et analytique. Aussi, les fréquences graves ont un effet masquant sur les fréquences plus aiguës. Ce phénomène est dû au fonctionnement de notre oreille, et plus particulièrement de la cochlée. 2.2.2 Pression acoustique &amp; niveau sonore Nous l’avons abordé plus haut, lorsqu’une onde sonore se déplace dans l’air, on constate la variation de la pression atmosphérique en ce point. Dès lors, il est facile de corréler l’amplitude de la variation de la pression avec le niveau sonore entendu (ou mesuré). L’unité du système international de la pression est le pascal (Pa). Or, il est très rare de parler de la pression acoustique en pascal, car la variation de cette pression exprimée en pascal ne correspond pas à ce que nous percevons. En d’autres termes, si la pression acoustique exprimée en pascal double, nous ne percevons pas un son deux fois plus fort. Notre oreille fonctionne de façon logarithmique, et non linéairement, face à une variation de pression acoustique. C’est pour cela que l’on parle généralement de niveau de pression acoustique, où SPL (pour Sound Pressure Level en anglais), qui s’exprimera en décibel. La relation entre la variation de pression et le niveau de pression acoustique se fait grâce à la relation : \\[L_p = 20\\,\\log_{10}\\Big(\\frac{p_{eff}}{p_{ref}}\\Big) \\qquad p_{ref} = 20\\mu Pa\\] Si la pression acoustique double, on observe une augmentation du niveau sonore de 3 dB SPL. Lorsqu’on ressent un doublement du niveau sonore, on observe une augmentation de 10 dB. La question se complexifie lorsque l’on rajoute la dimension fréquentielle à la question de la perception du niveau sonore. En effet, nous percevons des niveaux sonores différents pour différentes fréquences pourtant émises au même niveau de pression acoustique. Pour inclure cette dépendance fréquentielle, nous avons mis en place une unité de mesure : la sonie ou bruyance (loudness en anglais). Il est donc possible ensuite de définir des courbes d’isosonie, c’est-à-dire des courbes indiquant un niveau sonore de perception égale en fonction de la fréquence et du niveau de pression acoustique. Figure 2.4: Courbes d’isosonie, aussi dites de Fletcher-Munson Que conclure de cet abaque ? Notre oreille ne perçoit pas les fréquences de manière égale. Notre zone de sensibilité maximale se situe dans l’aigu (3k-4k Hz). Notre perception d’un matériau sonore en fonction du niveau auquel nous l’écoutons ! 2.2.3 Positionnement dans l’espace Notre système auditif nous permet de situer l’émission d’un son dans l’espace. Cette capacité de localisation repose sur un ensemble de facteurs étroitement liés entre eux. On qualifie notre écoute de binaurale, littéralement, écouter avec deux oreilles. La présence de deux “capteurs de pression” (oserait-on parler de microphones ?) sur les faces latérales de notre crâne et un premier élément expliquant notre capacité de localisation du son. En effet, l’espacement de nos oreilles (en moyenne 15 cm), créer un décalage temporel entre nos deux canaux d’écoutes. Ce léger retard entendu d’un côté ou de l’autre nous permettra de placer un son plutôt à gauche ou plutôt à notre droite. On appelle cet écart de temps différence de temps interaural, ou ITD (interaural time difference en anglais) et se note \\(\\Delta t\\). On pourrait d’ailleurs, grâce aux formules de ce début de chapitre, calculer le retard maximal moyen entre nos deux oreilles. \\[\\Delta t_{max} = \\frac d c = \\frac {0,15}{340} = 0.4 \\&gt; ms\\] Figure 2.5: Illustration de l’ITD Si nos oreilles sont espacées de quelques centimètres, notre tête les séparant représente un obstacle acoustique non négligeable. De plus, les pavillons des oreilles imposent également une certaine directivité à notre écoute. En première approximation, on pourra donc considérer que l’ensemble formé par la tête et les pavillons implique une atténuation linéaire des ondes sonores, elle-même fonction de l’angle d’incidence. On appelle cette différence de niveau différence d’intensité interaural, ou ILD (interaural level difference) et se note \\(\\Delta i\\). On considère que si la différence de niveau de pression acoustique entre les deux oreilles est supérieure à 20 dB, on entendra l’évènement sonore complètement latéralisé. L’ombre acoustique que représentent la tête et le pavillon n’est en réalité pas du tout linéaire en fréquence. La modification du timbre induite par ce système n’est pas perçue par notre cerveau comme une information de couleur, mais bien comme une information de spatialisation. Ainsi, selon l’angle d’incidence de l’évènement sonore, son spectre sera filtré d’une certaine manière qui permettra à notre cerveau de le positionner dans l’espace. La réponse en fréquence d’une tête se nomme HRTF (Head Related Transfer Function). Enfin, nous sommes également capables de déterminer la distance d’un évènement sonore. La plupart des paramètres permettant d’évaluer cette distance sont relatifs. Cela signifie que l’évènement doit être comparé à un autre pour pouvoir le repositionner dans l’espace. On pourra alors comparer : Leurs niveaux sonores : un évènement sonore plus fort paraît plus proche Leurs timbres : l’absorption de l’air aura pour effet de diminuer les fréquences aiguës La sensation de réverbération associée : plus le signal de l’évènement sonore semblera solliciter la réponse acoustique du lieu, plus celui-ci semblera fort. Le temps d’arrivée des premières réflexions : le son direct d’un évènement sonore lointain arrivera quasi simultanément avec ses premières réflexions. Le son direct d’un évènement sonore proche arrivera avant ses premières réflexions. Le chapitre suivant traitera des notions d’acoustique élémentaire ainsi que de la réverbération. "],["acoustique-des-salles.html", "3 Acoustique des salles 3.1 Généralités 3.2 Premières réflexions et filtre en peigne 3.3 Traitement acoustique", " 3 Acoustique des salles Tout environnement, sollicité par un évènement sonore, produit une réponse acoustique. Cette réponse acoustique est appelée réverbération. Elle est caractéristique d’un lieu et peut, dans certains cas, être une alliée précieuse dans notre travail. Dans d’autres, elle est source de problèmes et complexifie grandement notre travail d’écoute analytique. 3.1 Généralités 3.1.1 La réverbération L’acoustique d’une salle est généralement décrite en deux temps : le temps des premières réflexions et le temps du champ diffus. Figure 3.1: Schéma d’une réponse impulsionnelle de réverbération. Les premières réflexions sont les premiers rebonds d’une onde sonore sur les parois d’une salle et sont caractéristiques de la signature acoustique du lieu. Ces rebonds reviennent à l’auditeur avec un certain temps. Ce retard se nomme souvent « pré-délai » dans les moteurs de réverbération artificiels. Ce prédélai est fonction de deux paramètres : la taille de la pièce ; plus la pièce est petite, plus les premières réflexions reviendront à l’auditeur rapidement. les positions de la source sonore et de l’auditeur ; plus l’auditeur est proche de la source, plus les premières réflexions arriveront après le son direct, plus l’auditeur est loin de la source, plus les premières réflexions arriveront en même temps que le son direct. Lorsque les premières réflexions elles-mêmes auront rebondi plusieurs fois sur les parois du lieu, le phénomène d’écho des premières réflexions va se muer en champs diffus, par nature plus dense. La longueur du champ diffus se mesure grâce au RT60. Cette méthode de mesure propose de regarder le temps que met la réverbération à perdre 60 dB. Ce temps permettra ensuite de donner une longueur de réverbération. 3.1.2 Calcul du temps de réverbération L’équation de Sabine permet de calculer le temps de réverbération d’une salle à partir de son volume et du coefficient d’absorption de ses matériaux. \\[RT_{60} = 0.1611 \\times \\frac{V}{\\sum_{i=0}^{k} S_i.\\alpha_i}\\] \\(V\\) s’exprime en \\(m^3\\) et \\(S\\) en \\(m^2\\). \\(\\alpha\\) est le coefficient d’absorption du matériau, en sabins. Ce coefficient est compris entre 0 et 1, plus il est important plus le matériau est absorbant. En guise d’exemple sur l’utilisation de la formule ci-dessus, prenons le cas d’une pièce de \\(25\\,m^2\\) (\\(5\\,m\\) par \\(5\\,m\\)) et de \\(2.40\\,m\\) de hauteur. Nous considérons que le sol est en parquet et les murs en plâtre. Nous avons donc \\(25\\,m^2\\) de parquet et \\(4\\times(5\\times2.4)=48\\,m^2\\). On trouve sur les sites de fabricant de matériaux que le plâtre peint a un coefficient d’absorption de 0.05 sabins et le bois un coefficient de 0.15 sabins. Notre calcul final. \\[RT_{60} = 0.1611 \\times \\frac{25 \\times 2.4}{25\\times0.15+48\\times0.05} \\approx 1.57\\,s\\] On peut dès lors calculer la distance critique, distance à partir de laquelle on entendra autant un évènement sonore que la réponse acoustique de la salle à son stimulus. \\[d_c \\approx 0.057 \\times \\sqrt{\\frac{V}{RT60}}\\] Dans notre exemple \\(d_c \\approx 0.35\\,m\\). Il est souvent considéré que la taille de la pièce joue un rôle déterminant sur la longueur de réverbération. L’équation de Sabine indique bien que le coefficient d’absorption des matériaux y joue un rôle beaucoup plus important. Le modèle de réverbération de l’IRCAM va jusqu’à complètement décorréler la taille de la pièce simulée du temps de réverbération. Au final, la taille de l’espace joue davantage sur la structure temporelle des échos, et donc, principalement sur les premières réflexions. 3.1.3 Limite de l’équation de Sabine Il convient d’observer plusieurs réserves quant à l’utilisation de l’équation de Sabine. Premièrement, elle ne tient pas compte de l’aspect fréquentiel lié à l’absorption des matériaux. En effet, le temps de réverbération des graves est presque toujours plus long que celui des aigus. Afin de contourner ce problème, on pourra chercher des coefficients d’absorption tenant compte de la fréquence et ainsi résoudre l’équation de Sabine pour certaines plages fréquentielles. L’équation de Sabine pose également problème pour de petits espaces (régie d’écoute par exemple) en prédisant un temps de réverbération trop long. Dans ce cas, l’équation d’Eyring est plus adaptée. \\[RT_{60} = -0.1611 \\times \\frac{V}{\\sum_{i=0}^{k} S_i.\\ln(1-\\alpha_i)}\\] L’équation d’Eyring n’améliore pas non plus la problématique fréquentielle. 3.1.4 Le phénomène d’onde stationnaire La plupart des pièces de vie sont des salles rectangulaires. Dans ce cas, les surfaces sont toutes parallèles. Ce type de salle est particulièrement propice à l’apparition d’ondes stationnaires. Une onde stationnaire est un phénomène acoustique provoquant l’augmentation de volume de certaines fréquences (ventre) et la disparition d’autres (nœuds). Nous aborderons ici ce phénomène sous l’angle de l’acoustique des salles, mais il est applicable dans d’autres situations, comme la vibration d’une corde par exemple. Figure 3.2: Les points rouges représente les noeuds, les amplitudes maximales sont les ventres. Infographie par Lucas Vieira Il est possible de calculer les fréquences d’un mode grâce aux formules vues au chapitre précédent : \\[f(n) = \\frac{c}{2L}.n\\] où \\(c=340\\,m.s^{-1}\\), \\(L\\) est la longueur considérée de la pièce. Pour \\(n=1\\) on trouve le mode propre. Pour \\(n&gt;1\\) on trouvera tous les modes harmoniques. Étudions la fréquence du mode propre pour deux cas théoriques : une salle de 16 m² (4x4) et une autre de 49 m² (7x7). On trouvera donc : \\[f(1)_{L=4m} = 42.5 \\,Hz \\&gt;\\&gt;\\&gt;\\&gt; f(1)_{L=7m} = 24 \\,Hz\\] On en déduit donc que, plus la pièce est grande, plus la fréquence des modes propres sera grave. Il convient également de considérer la distance de chaque surface parallèle, car les pièces sont rarement cubiques. Cela implique donc la présence de trois modes propres, plus leurs modes harmoniques, pour une seule et même salle. 3.2 Premières réflexions et filtre en peigne Nous avons vu que la réponse acoustique, ou réverbération, d’une salle se décompose généralement en deux parties, la première étant les premières réflexions. Ces premières réflexions sont donc, comme leur nom l’indique, les premiers rebonds que nous entendons suite à un évènement sonore. Dans de petites pièces, les premières réflexions peuvent être entendues si proche du son direct que cela génère un type de filtrage bien particulier appelé filtre en peigne. ## &lt;string&gt;:1: RuntimeWarning: divide by zero encountered in log10 ## &lt;string&gt;:1: RuntimeWarning: invalid value encountered in multiply Toujours en utilisant les formules définies au premier chapitre, on établit la relation suivante : \\[ fc = \\frac 1{2t} = \\frac c{2d} \\] Où \\(fc\\) correspond à la fréquence d’annulation la plus grave du filtre en peigne. Les autres fréquences se calculent grâce à la relation \\(f(n) = fc*n\\). Le phénomène de filtre en peigne est donc également harmonique. Ainsi, on peut calculer les filtres en peignes présents au point d’écoute d’une régie de mixage ou de prise de son grâce à la mesure du chemin des premières réflexions. Figure 3.3: Ensemble des premières reflexions entendues par une oreille pour une enceinte (hors plafond et plancher/bureau). La réflexion du son sur une paroi est tout à fait comparable à de l’optique géométrique. Une onde sonore arrivant avec un angle d’incidence \\(\\alpha\\) sur une surface sera réfléchie avec le même angle. Ainsi, il est souvent conseillé d’utiliser un miroir lorsque l’on positionne des traitements acoustiques. Lorsque la personne assise au point d’écoute voit une enceinte dans un miroir placé sur un mur, on sait alors qu’il faudra placer le panneau à la place du miroir. On se rend donc compte que l’influence des filtres en peigne générés par les premières réflexions est très importante. Ce phénomène à lui seul explique l’intérêt d’une grande régie d’écoute. En effet, plus une pièce est grande, plus l’écart de temps entre le son direct et les premières réflexions est important. Cela implique deux choses : Notre cerveau favorisera le son direct plus facilement (effet de précédence) À partir d’une certaine taille, l’effet du filtre en peigne se mue en information d’acoustique pour notre cerveau. Au-delà de 40 ms (trajet d’une première réflexion d’environ 14 m), l’écart entre le son direct et les premières réflexions est tel que nous entendons un écho (effet Haas). Afin de réduire au maximum les effets des filtres en peignes, il est recommandé de placer des traitements aux points de réflexion critique par rapport à la position d’écoute (voir schéma ci-dessus). 3.3 Traitement acoustique Grâce aux différents points abordés ci-dessus, nous avons maintenant bien l’idée que l’acoustique d’un lieu est un des facteurs les plus déterminants sur le rendu sonore. Mais c’est aussi celui sur lequel il est plus difficile et technique d’intervenir. On favorisera au maximum une architecture optimisée pour l’acoustique. Dans ce but, il convient de n’avoir aucune surface parallèle, cela permettant de grandement limiter l’apparition d’ondes stationnaires. On choisira également des matériaux avec des propriétés acoustiques intéressantes (plâtre et carrelage sont à proscrire, au profit du bois par exemple). On se posera ensuite la question des endroits de la pièce les plus propices pour y positionner un évènement sonore (enceinte, musicien, etc.). On cherchera donc un point où la contribution des différents modes semble équilibrée. Pour cela, il suffit de se munir d’une enceinte et d’y diffuser une musique ou un signal test qui nous est familier. En déplaçant l’enceinte, on pourra évaluer la contribution acoustique de la pièce en différents points. Une fois ces considérations prises en compte, on pourra alors aborder le traitement de l’acoustique. Il ne faut pas confondre isolation acoustique et traitement acoustique. Dans le premier cas, on chercher a limiter la contribution sonore d’un lieu sur son environnement, dans l’autre on cherche à améliorer la propagation du son dans un espace donné. Une isolation acoustique satisfaisante nécessite de lourds travaux, voire l’aménagement d’une “boîte dans une boîte”. Ces notions d’acoustiques dépassent le cadre de ce cours. 3.3.1 Les types de traitements On trouve, en général, deux types de traitements : Les absorbeurs, qui réduisent l’énergie d’une onde sonore à son impact. Les diffuseurs, qui répartissent l’énergie d’une onde sonore dans l’espace. Dans un lieu où la quantité de réverbération est jugée trop importante, on utilisera des absorbeurs. À l’inverse, dans un lieu où l’on souhaite préserver la quantité de réverbération, mais en évitant les phénomènes de modes ou de filtre en peignes, on utilisera des diffuseurs. Dans de petits lieux, l’usage de diffuseur semble contre-productif, la priorité étant d’absorber au maximum les premières réflexions, celle-ci arrivant très rapidement après l’émission du son direct. 3.3.2 Considération d’acoustique pour le travail de son Il est vivement recommandé d’installer un studio, de prise de son ou de monitoring, dans un lieu plutôt grand. En effet, plus le lieu est grand, plus il sera facile de positionner un point de prise de son ou d’écoute suffisamment éloigné des parois afin de minimiser l’influence des premières réflexions. Aussi, plus le lieu est grand, plus l’espace y sera suffisant pour installer des traitements acoustiques. Certains types de traitements, comme les basstraps, peuvent prendre une place bien trop importante pour être installée dans des pièces de dimension habituelle (chambres, bureau, etc.). On se rappellera aussi de choisir une pièce de travail avec le minimum de surface parallèle, afin de limiter les ondes stationnaires. En ce qui concerne les traitements en eux-mêmes, il est vivement recommandé de traiter en priorité le bas du spectre. L’ajout de basstrap est donc prioritaire sur le reste des traitements. Plus la longueur d’onde à traiter est grande (donc la fréquence grave), plus la taille des matériaux devra être importante. On retrouve donc le point abordé précédemment : traiter une pièce correctement, demande un certain espace. Par ailleurs, il est important que les traitements appliqués à un lieu soient linéaires en fréquence, c’est-à-dire qu’il ne se concentre pas sur une seule zone du spectre. Cela arrive souvent avec les kits de mousses peu onéreux, mais n’ayant une réelle efficacité que dans les médiums et hautes fréquences. Pour une régie d’écoute, on sera tenté de privilégier des traitements d’absorption. En effet, une réverbération trop longue dans une régie de monitoring risque fort de fausser certaines prises de décisions (distance des microphones à la source, quantité de réverbération, etc.). À l’inverse, une pièce avec un temps de réverbération trop court pourra créer un sentiment d’inconfort, voire de malaise. Pour une salle de prise de son, l’idéal est de disposer d’un grand espace avec un traitement acoustique principalement basé sur de la diffusion, pour ensuite disposer de traitements absorbants amovibles permettant de sculpter le rendu acoustique en fonction de la prise de son à réaliser. Pour des petits lieux (- de 25 m²), on cherchera à absorber au maximum afin de limiter les effets de filtre en peigne. "],["description-dune-production-musicale-type.html", "4 Description d’une production musicale type 4.1 les acteurs de la réalisation d’une œuvre enregistrée 4.2 La préproduction 4.3 La production 4.4 La postproduction", " 4 Description d’une production musicale type Afin de comprendre quels vont être les enjeux du preneur de son, il convient de comprendre dans quel contexte il intervient. Certes, il est le premier métier du son à rentrer en scène, mais l’œuvre à enregistrer a déjà très probablement eu une longue vie. Elle a été composée, arrangée, peut-être même déjà interprétée au cours de concerts. À ce stade, le preneur de son aura un regard neuf sur la matière. Il aura donc le potentiel de permettre aux créateurs de prendre du recul sur leur travail. Il convient d’ailleurs de rappeler qu’un preneur de son, aussi talentueux et créatif soit il, est un assistant de création. Cela signifie qu’il met à disposition une compétence technique à un d’artiste pour lui permettre d’avancer sur son projet. Cela implique également que celui ou celle qui a le mot final sur le choix des orientations esthétiques est l’artiste en question. Il convient donc, en tant que preneur de son, d’être force de proposition, tout en sachant respecter le choix (qu’ils soient bons ou mauvais) des artistes. D’un point de vue sonore, le travail de prise de son est absolument critique. Ce sera à ce moment que va se jouer la majorité des choix esthétiques. Il convient donc de réunir les conditions optimales pour : offrir aux musiciens et musiciennes la chance de donner leur meilleure interprétation possible réaliser une prise de son en adéquation avec l’orientation esthétique du projet La plupart des choix faits à la prise de son ne pourront pas être renégociés a posteriori. Il convient donc de mettre d’accord les artistes, le directeur artistique et le preneur de son sur les moyens à mettre en œuvre. Figure 4.1: Entonnoir de la production musicale 4.1 les acteurs de la réalisation d’une œuvre enregistrée Nous allons ici rapidement discuter des différents rôles apparaissant dans la production d’une œuvre musicale enregistrée. Ceux-ci sont volontairement très séparés, bien que dans les cas pratiques, une personne puisse en incarner plusieurs. Le compositeur est la personne qui a composé la mélodie et l’harmonie de l’œuvre. L’arrangeur est chargé de l’orchestration (choix des instruments) et l’écriture des différentes partitions. L’interprète a la responsabilité de retranscrire une partition le plus justement possible, à la fois dans sa dimension technique et sensible. Le directeur artistique (ou DA) supervise l’ensemble de l’enregistrement. Il aura, par exemple, à choisir le preneur de son, le mixeur ou dans quel studio enregistrer. Lors de la session d’enregistrement, il aura à diriger les musiciens (comme un réalisateur dirige ses acteurs au cinéma) afin de leur faire jouer la meilleure interprétation possible pour l’œuvre. Lors du mixage, il sera le principal interlocuteur du mixeur. Pour faire court : il est le garant de l’orientation esthétique du projet. Le producteur finance l’ensemble de projets. C’est donc un investisseur qui attend un retour sur investissement. L’appellation abusive de « producteur » pour parler du directeur artistique vient d’un anglicisme du mot « producer ». Le producteur est donc bien l’équivalent du directeur artistique dans les pays anglo-saxons. Si le DA a besoin d’un certain talent, le producteur a surtout besoin d’argent. Le preneur de son est chargé d’enregistrer les musiciens et musiciennes. Il a donc un rôle premier très technique : il doit inscrire sur un support les ondes sonores produites par ces musiciens. Il a également un rôle esthétique très important, d’un point de vue sonore, car le choix du dispositif de prise de son aura un fort effet sur la suite de la vie de l’œuvre. Le mixeur intervient après la prise de son et doit réaliser une sommation de l’ensemble des points de captations (microphone) vers un format écoutable par le grand public (mono, stéréo, 5.1, Ambisonique, Dolby Atmos, etc.). Son rôle esthétique est fortement contraint par le travail de prise de son. Si celle-ci est réussie, il pourra amplifier et bonifier les choix de production. Dans le cas contraire, il devra lutter pour essayer de faire sortir le meilleur d’une matière imparfaite. Le technicien de mastering est le dernier maillon de la chaîne. Son rôle premier sera de préparer le travail de mixage à aux supports de diffusion. Il se devra également d’offrir une oreille nouvelle sur le travail réalisé au mixage. 4.2 La préproduction La préproduction concerne toutes les étapes d’une œuvre enregistrée qui ont lieu avant ledit enregistrement. On parlera donc en premier lieu de la composition et particulièrement de l’arrangement. La qualité d’un arrangement aura une influence énorme sur la facilité à mixer une œuvre. Si les instruments sont astucieusement répartis sur l’ensemble du spectre sonore, cela sera une difficulté de moins à gérer au mixage par exemple. Il est aussi courant pour des artistes de réaliser des « démos ». Celles-ci sont souvent des enregistrements réalisés en home studio afin de définir un cap esthétique pour la suite de la production sonore. C’est un atout extrêmement précieux pour un preneur de son, cela permet de rapidement identifier quel est le projet esthétique de l’œuvre. 4.3 La production C’est ici que le travail du preneur de son commence. L’étape de production consiste à fixer les interprétations définitives. Le premier objectif est donc de s’assurer du bon enregistrement de tous les canaux prévus. Bien sûr, l’enjeu n’est pas seulement technique, mais aussi esthétique. Et il n’est pas moindre, les choix pris lors de la prise de son seront des carcans impossibles à outrepasser lors de la phase de mixage. Enfin, l’élément le plus déterminant de cette étape est d’obtenir des musiciens leurs meilleures interprétations. La présence d’un directeur artistique est d’une aide précieuse afin de diriger et d’orienter les musiciens. Il permet aussi de faire le lien entre les artistes et l’équipe technique, en exprimant les besoins des uns aux autres. Sur les projets les plus modestes, le poste de directeur artistique est souvent sacrifié. Il en va donc à l’ingénieur du son de, parfois, remplir ce rôle. 4.4 La postproduction Arrivé à ce stade, la majorité du travail est déjà accompli, il ne reste que le mixage et le mastering. Classiquement, chacune de ces tâches incombe à un technicien différent. Le travail du mixeur consistera à réaliser la sommation, généralement en stéréo, de l’ensemble des canaux enregistrés lors de la prise de son. Afin de faire cohabiter tous ces signaux, il est commun d’utiliser des traitements pour les répartir sur l’ensemble du spectre et de gérer leur dynamique. Parfois, ces traitements remplissent un rôle esthétique, en déformant le signal d’origine pour aboutir à une nouvelle matière. Une fois le travail du mixeur terminé, le mastering commence. Le but et d’homogénéiser l’ensemble des titres d’un disque, en volume, en dynamique et en couleur. Ensuite, il convient aussi de définir le niveau de sortie général du disque. La dernière étape consistera à monter l’ordre des morceaux pour le disque, d’y inscrire les métadonnées (nom de l’artiste, des titres, genre musical, etc.) et de générer le fichier final, dédier à l’exploitation. Définir analogue, transducteur, numérique, informatique --> "],["le-chemin-du-signal.html", "5 Le chemin du signal", " 5 Le chemin du signal La première mission d’un preneur de son est d’assurer l’arrivée à bon port des signaux dans l’enregistreur. En effet, toute notion de mise en scène sonore et d’esthétique devient très secondaire si le contenu n’a pas été enregistré. Le diagramme ci-dessous reprend les principaux étages rencontrés par un signal audio dans un contexte de production numérique. Il est essentiel d’être le plus familier possible avec ces différents composants. Figure 5.1: Le chemin du signal. Elle peut-être agrandie en ouvrant l’image dans un nouvel onglet. Nous pourrions catégoriser à partir de ce schéma différents « milieux ». Tout d’abord, nous avons le milieu acoustique, où nous trouverons toutes sortes d’instruments de musique, les différents lieux dans lesquels ils pourront s’y trouver. C’est donc le domaine de l’onde sonore mécanique. On trouve ensuite le milieu analogique, où l’onde sonore est représentée, de façon analogue, par des grandeurs électriques. Celles d’un signal sonore dans un circuit analogique sont fonction, par exemple, de la variation de la pression atmosphérique provoquée en un point par une onde sonore. Les éléments clefs du milieu analogique sont les préamplificateurs et les amplificateurs, mais on trouve aussi certains traitements, comme les égaliseurs et les compresseurs. On définira « analogique » comme une représentation dans laquelle les grandeurs (tension, courant, etc.) qui entrent dans les calculs sont représentées par des grandeurs analogues et qui varient de manière identique (définition du CNRTL). Pour passer du milieu acoustique au milieu analogique, et vice-versa, on utilise des microphones et des haut-parleurs. Tous deux sont des transducteurs, permettant de transformer une énergie en une autre. Le microphone transforme une énergie mécanique en énergie électrique. Le haut-parleur réalise l’opération inverse. On en vient ensuite au milieu numérique. Fondamentalement, le signal est toujours de nature électrique, mais il a subi une opération très importante nommée échantillonnage. On a donc mesuré à intervalle régulier la tension électrique générée par l’onde sonore. Le passage par le numérique permet une myriade de traitements sur le signal, beaucoup plus complexes que ceux permis par l’électronique analogique. L’audio numérique permet aussi un stockage de l’information à moindre coût et l’acheminement d’un grand nombre de voies (canaux) grâce à un faible nombre de modulations (câble). L’appareil permettant de passer du milieu analogique au milieu numérique est le convertisseur analogique/numérique. Il ne s’agit pas d’un transducteur, car les signaux d’entrées et de sorties sont de même nature électrique. Pour opérer l’opération inverse, on utilise un convertisseur numérique/analogique. Le milieu informatique nous permet d’utiliser des applications relatives aux traitements du son par le biais d’ordinateurs. Il s’agit aujourd’hui indubitablement de notre outil de travail principal. Nous y réalisons la grande majorité des traitements audio, ainsi que l’enregistrement et le routage des sources. Le lien entre un ordinateur et un convertisseur A/N/A se fait grâce à un bus de sérialisation associé à un pilote (ou driver). L’ensemble des deux permet de mettre en forme la donnée numérique et de la rendre compréhensible à l’ordinateur. Chaque élément évoqué ci-dessus sera abordé dans des sections dédiées dans la suite de ce livre. "],["notions-élémentaires-délectronique.html", "6 Notions élémentaires d’électronique 6.1 Les composants passifs 6.2 Les semi-conducteurs 6.3 L’impédance et son rôle dans les circuits", " 6 Notions élémentaires d’électronique 6.1 Les composants passifs 6.2 Les semi-conducteurs 6.3 L’impédance et son rôle dans les circuits "],["les-microphones.html", "7 Les microphones 7.1 Petit historique des microphones 7.2 Les types et technologies de microphones 7.3 La directivité", " 7 Les microphones Un microphone est un transducteur permettant de transformer une onde acoustique en signal électrique. Cette opération est réalisée par une membrane. Selon la nature du microphone, cette membrane pourra être constituée d’une feuille métallique d’un condensateur ou encore être rattachée à une bobine. Le microphone est l’outil principal du preneur de son. Le choix du modèle et sa position dans l’espace est déterminants sur le rendu sonore d’une captation. Ces deux paramètres ont par ailleurs une certaine interdépendance : une position souhaitée du microphone pouvant influencer le choix du modèle et vice-versa. 7.1 Petit historique des microphones Sans vouloir rentrer dans un récit exhaustif sur l’invention et l’évolution des microphones, relater les moments clefs de cette technologie permet d’avoir une vision globale du marché d’aujourd’hui. La nécessité de capter un évènement sonore grâce à un microphone provient de trois besoins : le transmettre (télécommunication) l’amplifier (concert, spectacle vivant) l’enregistrer (industrie du disque) En 1876, Alexandre Graham Bell propose un système à base liquide, permettant de transformer une onde sonore en tension électrique. Le système ne fut jamais réellement exploité, car le rendu sonore était jugé trop peu satisfaisant. Le premier type de microphone utilisé industriellement est le microphone à charbon (au UK, par David Edward Hugues, aux US par Emile Berliner et Thomas Edison. Le brevet sera d’ailleurs disputé, avec un gain de cause pour Edison malgré des démonstrations publiques de Hugues antérieur aux publications d’Edison). En raison de sa faible bande passante et de son niveau de bruit élevé, il se révèle de piètres qualités pour l’enregistrement et la transmission de la musique. Il aura, par contre, une place de choix dans les téléphones durant de longues décennies. Viennent ensuite les microphones à condensateur, dont les premiers modèles remontent à 1916, par le chercheur Edward Wente. Ces microphones sont tout d’abord réputés assez capricieux, leurs réponses en fréquences pouvant varier significativement en fonction de l’humidité de l’air et de la température. À cause de ces variations sonores présentes dans les premiers microphones à condensateur, on leur préférera un temps les microphones à ruban. Ils sont inventés en 1923 par Harry Olson. Ils sont par contre d’une grande fragilité mécanique. George Neumann est un des noms à connaître dans cette histoire des microphones. On lui doit, entre autres, la stabilisation des microphones statiques. Il sera aussi le premier à produire un microphone (U87) utilisant un transistor en lieu et place des traditionnels tubes. À partir des années 1970, les microphones dynamiques arrivent sur le marché, notamment porté par la marque Shure. Ces microphones ont la grande qualité d’être très robustes, et remplaceront leurs homologues à ruban dans bien des cas. Depuis, les principales améliorations ont concerné la robustesse d’une part, et la miniaturisation des dispositifs d’autre part, menant ainsi au développement des capsules MEMS. 7.2 Les types et technologies de microphones Avant d’aborder en détail certaines constructions de microphones, il convient de faire attention à certains raccourcis associant des méthodes de fabrications à un niveau présumé de qualité. Par exemple, il est commun d’associer les microphones à électret à une construction « bas de gamme ». Or, c’est oublier que la série 4000 de chez DPA, considérée par beaucoup comme une référence indétrônable de la prise de son, ne contient que des microphones à électret. Les MEMS souffrent du même biais, ceux-ci se retrouvent pourtant de plus en plus souvent sur des microphones ambisoniques, comme le Zyla ou le SPC mic. Nous allons maintenant aborder les types de microphones suivants : Les microphones électrostatiques/à condensateur Les microphones à ruban Les microphones dynamiques 7.2.1 Les microphones électrostatiques/à condensateur Ce sont, historiquement, les premiers microphones à permettre une captation du spectre audible satisfaisante. Ils sont cependant très sensibles aux conditions de température et d’humidité et il fallut attendre les années trente pour que ce problème cesse. Ils nécessitent une alimentation externe, appelée alimentation fantôme, normalisée à +48V. Il existe deux familles de microphones électrostatiques, les condensateurs à hautes fréquences et condensateur polarisés en courant continu. Les microphones à condensateur polarisés en courant continu ont le fonctionnement le plus commun. Un courant continu vient polariser la capsule/condensateur. Lorsqu’une onde sonore rencontre la capsule, une de ses armatures se déforme et génère une variation de tension analogue à la variation de pression. Les microphones à condensateur à haute fréquence proposent une approche différente. Un oscillateur est intégré dans le microphone et la variation de pression enregistrée par le condensateur vient moduler la fréquence de cet oscillateur. Le signal est ensuite démodulé dans la plage audible. Cette méthode de construction offre une impédance de sortie plus faible et une plus grande résistance aux variations de conditions climatiques. Concernant leurs caractéristiques, ces microphones possèdent des réponses en fréquence souvent très linéaire et une excellente réponse en transitoire. Leur niveau de sortie (sensibilité) est élevé. Leur impédance de sortie est basse. Exemples : Neumann U87/AKG C414/Shoeps CMC4/Série 4000 DPA/Série MKH Sennheiser 7.2.2 Les microphones à ruban Les microphones à ruban souvent préférés à leurs homologues statiques dans les débuts de la musique enregistrée. Leur fonctionnement repose sur l’utilisation d’une feuille métallique placée entre deux aimants. Lorsqu’une onde sonore rencontre cette feuille (le ruban), celle-ci vibre et perturbe le champ électromagnétique créé par les aimants et génère une tension analogue à la variation de pression. D’un point de vue sonore, les microphones à ruban ont souvent un bas du spectre assez généreux et une réponse plutôt douce pour les hautes fréquences. Ils sont aussi connus pour avoir une impédance de sortie assez élevée et un niveau de sortie faible. Attention à l’alimentation fantôme (+48V), elle peut endommager le microphone. Exemples : Royer R121/Cohles/Beyerdynamic M160 7.2.3 Les microphones dynamiques Les microphones dynamiques sont conçus pour des conditions d’utilisation rudes, où les niveaux sonores sont élevés et où le risque de chute est important. Ils sont donc monnaie courante en sonorisation. Leur membrane est attachée à une bobine entourant un aimant. Lorsqu’une onde sonore la met en vibration, la bobine se déplace autour de l’aimant, et, par perturbation du champ électromagnétique, génère une tension de sortie analogue à la variation de pression. Leur réponse en fréquence est souvent accidentée, particulièrement dans le haut du spectre. Cela peut être vu comme un inconvénient ou comme un outil de « coloration » du son. Comme leurs homologues à ruban, ils possèdent un niveau de sortie faible et une impédance de sortie élevée. Exemples : Shure SM57/Electrovoice RE20/Sennheiser MD441 7.2.4 La taille des membranes La taille des membranes influe sur la captation du son. Plus la capsule est grande, plus les fréquences aiguës seront diffractées et donc atténuées dans la prise de son. Un microphone à petite membrane est donc techniquement un microphone plus « juste ». Cependant, l’emploi de large membrane permet aussi d’adoucir un surplus d’énergie dans le haut du spectre. 7.2.5 Tubes et transistors Historiquement, les tubes ont été les premiers composants électroniques à permettre l’amplification du signal. Le transistor est apparu à la fin des années 40 et a permis de remplir les mêmes fonctions qu’un tube, par une consommation moindre et avec un encombrement beaucoup plus faible. Certains microphones continuent à être fabriqués avec des tubes, préférant leur comportement vis-à-vis du son. Une écrasante majorité est cependant fabriquée avec des transistors. Le choix entre un microphone à tube et un microphone à transistor semble cependant anecdotique par rapport à son type, à son placement et à sa directivité. 7.3 La directivité La directivité d’un microphone permet de décrire sa capacité à réaliser une « écoute » sélective de son environnement. On rencontre les directivités suivantes : Omnidirectionnel : capte l’ensemble du champ sonore de façon indifférenciée. Hypercardioïde : compromis entre Omnidirectionnel et cardioïde. Cardioïde : capte à l’avant, mais rejette à l’arrière du microphone. Supercardioïde : ressers la zone d’écoute avant au prix de l’apparition d’une résurgence arrière. Hypercardioïde : ressers davantage la zone d’écoute et augmente d’autant plus la résurgence arrière. Bidirectionnel : capte à l’avant et à l’arrière, mais selon un lobe plus resserré qu’en cardioïde. Plus la directivité d’un microphone est large, plus la contribution de l’acoustique est apparente. Le timbre est également aussi linéaire que possible. À l’inverse, plus la directivité tant à être étroite, plus le microphone aura une capacité à échantillonner seulement une zone de l’espace. Le timbre est, par contre, amoindri dans le bas du spectre. Les microphones omnidirectionnels sont donc les plus larges et les plus « neutres », tandis que les microphones bidirectionnels sont les plus focalisés et ont la plus importante perte dans le bas du spectre. Le preneur de son choisit donc une directivité en fonction de la tâche à accomplir. Les microphones directifs ont l’avantage de limiter la contribution d’évènements sonores que l’on ne souhaite pas capter. Les microphones omnidirectionnels ont la faculté d’être un dispositif de prise de son plus transparent, mais seront beaucoup plus sensibles à une acoustique moins optimale, ainsi qu’au bruit environnant. Nous allons par la suite nous intéresser au cœur du microphone : sa capsule. Il existe deux familles de capsules, celles dites « à pression », et celles dites à « gradient de pression ». 7.3.1 Capsules à pression Une capsule sensible à la pression est omnidirectionnelle : elle capte les fluctuations de pressions en un point. Mathématiquement, cette relation s’exprime, en coordonnées polaires, par : \\[ \\theta = 1 \\] L’angle d’incidence de l’onde sonore par rapport au microphone importe donc peu. Pour réaliser une capsule à pression, on enferme une partie de la membrane dans un milieu acoustique à pression constante. 7.3.2 Capsules à gradient de pression Une capsule à gradient de pression est sensible à la variation du champ de pression. Ces capsules ne sont plus omnidirectionnelles, mais bidirectionnelles : elles captent devant et derrière elles. Mathématiquement, une telle directivité s’exprime par la relation (en coordonnées polaires) : \\[ \\theta = cos(\\alpha) \\] Où \\(\\alpha\\) est l’angle d’incidence d’un son par rapport à la capsule. Pour réaliser une capsule à gradient de pression, il suffit de laisser exposer les deux faces de la membrane aux variations de pressions. 7.3.3 Et les autres directivités ? Il est possible, à partir des deux équations ci-dessus, de retrouver toutes les autres directivités. Par exemple, un microphone cardioïde a une équation de directivité polaire tel que : \\[ \\theta(\\alpha) = \\frac{1}{2}(1 + cos[\\alpha]) \\] Elles découlent donc des deux directivités primaires : omnidirectionnelle et bidirectionnelle. Pour obtenir une directivité particulière, il suffit de « doser » l’influence de ces deux directivités. Par exemple, un microphone cardioïde possède une contribution égale de chacune d’elles. Plus on augmente la proportion de la directive bidirectionnelle, plus on tend vers un microphone supercardioïde, voire hypercardioïde. À l’inverse, augmenter la proportion de la directivité omnidirectionnelle fait tendre le microphone vers une directivité hypocardioïde. Il existe deux solutions pour agir sur la contribution des directivités primaires. La première consiste à utiliser un labyrinthe acoustique pour changer le milieu acoustique d’une des faces de la membrane. L’autre consiste à avoir une capsule omnidirectionnelle et une seconde bidirectionnelle et de sommer leur tension de sortie. Les microphones à multidirectivité permettent à l’utilisateur d’influer, soit sur le labyrinthe acoustique, soit sur la sommation des deux capsules. Il n’est pas rare que ces microphones soient moins performants qu’un microphone spécifiquement dédié à une seule directivité. On retiendra donc : \\[ \\theta(\\alpha) = A + B \\cos (\\alpha) \\&gt; où \\&gt; A + B = 1\\] 7.3.4 Directivités réelles Figure 7.1: Directivité réelle d’un microphone cardioïde (Sennheiser) "],["les-modulation-analogiques.html", "8 Les modulation analogiques 8.1 Câble à modulation unique 8.2 Câble à double modulation 8.3 Les connexions asymétriques 8.4 Les connexions symétriques 8.5 Les connexions stéréo asymétriques 8.6 Les connexions d’insert", " 8 Les modulation analogiques Les câbles assurent un rôle de transport vis-à-vis de signaux électriques. Ces signaux peuvent représenter une onde sonore (précédemment captée par un microphone), une valeur de contrôle pour piloter un appareil (pédale d’expression) ou encore une information numérique (câble USB, ethernet, etc.). En audio, il existe principalement deux types de câbles : Les câbles à modulation unique (un signal transporté) Les câbles à double modulation (deux signaux transportés) 8.1 Câble à modulation unique Ils sont constitués de quatre à cinq composants : + d’un cœur composé d’un filament (souvent multibrin) en un métal conducteur, véhiculant le signal électrique. + d’une gaine isolante (plastique) protégeant le cœur + d’une tresse en cuivre connectée à la masse + Parfois, d’une feuille de cuivre, enrobant la tresse, permettant de réaliser une cage de faraday et de protéger le cœur des ondes électromagnétiques. + Enfin, d’une dernière gaine isolante, permettant de protéger l’ensemble du câble. Ces câbles sont communément soudés à des fiches Jack TS, ou RCA. Ils sont communément utilisés pour les instruments des musiques ainsi que pour l’audio grand public. 8.2 Câble à double modulation La structure de ces câbles est identique aux câbles à modulation unique, à la présence près d’un deuxième cœur, permettant de transporter un deuxième signal électrique. Ces câbles sont souvent soudés à des fiches XLR, Jack TRS, Jack Battam, « mini-jack », etc. 8.3 Les connexions asymétriques Les connexions asymétriques permettent de transporter un signal entre une source et un récepteur. Il s’agit de la façon la plus simple de connecter deux appareils devant échanger des signaux. Cependant, sur de longue distance, le câble peut se comporter comme une antenne et induire sur le signal certaines ondes électromagnétiques (comme la radio). Ces connexions utilisent des câbles à modulation unique. 8.4 Les connexions symétriques Le but de ces connexions est de palier au problème des connexions asymétrique. Dans l’appareil émetteur, le signal à transporter est dupliqué, et ce duplicata est inversé en phase. Cette étape s’appelle la symétrisation. C’est deux signaux sont appelés point chaud (signal d’origine) et point froid (signal opposé en phase). Le câble transporte ces deux signaux. Sur son trajet, lorsqu’une perturbation électromagnétique est induite sur le signal, celle-ci s’inscrit en phase sur les deux signaux (point chaud et point froid). À l’arrivée, l’appareil récepteur inverse la phase du point froid et le somme avec le point chaud. Cette étape se nomme la dé-symétrisation. Ainsi le signal d’origine est sommé en phase, tandis que les interférences sont sommées hors phase et s’annulent. Les connexions symétriques nécessitent un câble à double modulation. 8.5 Les connexions stéréo asymétriques Ces connexions permettent de véhiculer deux signaux à travers un seul et même câble. Elles nécessitent un câble à double modulation. 8.6 Les connexions d’insert Utilisées dans les consoles afin de permettre l’insertion de périphérique de traitement externe, ces connexions permettent d’envoyer le signal vers le périphérique et de le récupérer avec le même câble. Elles utilisent des câbles à double modulation. "],["les-préamplificateurs-de-microphones.html", "9 Les préamplificateurs de microphones 9.1 Les qualités d’un préampli 9.2 Les technologies de préampli 9.3 Les contrôles d’un préampli", " 9 Les préamplificateurs de microphones Le rôle d’un préampli est de réaliser une amplification en tension du signal émis par un microphone. Cette opération est indispensable pour permettre à notre signal de traverser tout le reste de la chaîne du traitement audio. 9.1 Les qualités d’un préampli La quantité d’amplification. Plus l’amplification disponible est grande, au plus le préampli sera capable de répondre à des situations exigeantes, tel que l’enregistrement d’une source à faible niveau avec un microphone à faible sensibilité. La réponse en fréquence. Théoriquement, celle-ci doit être la plus neutre possible. Une certaine coloration peut être acceptée (voire souhaitée), mais celle-ci doit rester raisonnable pour répondre à des critères d’utilisations professionnelles. La réponse en transitoire. Certains préamplis auront tendance à adoucir la sensation d’attaque des sources. Cet effet n’est pas souhaitable. Le rapport signal bruit. Nous cherchons toujours à rajouter le moins de bruit possible sur le chemin de notre signal. 9.2 Les technologies de préampli Il existe trois grandes façons de construire un préamplificateur : à tubes (ou lampes). Le tube est historiquement le premier composant électronique permettant l’amplification des signaux. à transistor. L’invention du transistor est sans doute la plus grande révolution technologique du siècle dernier. Celui-ci permet de remplacer les lampes pour une taille bien plus petite et une bien meilleure fiabilité. à circuit intégré. Les circuits intégrés permettent également l’amplification de signaux. Chacune de ces technologies offre de très légère variation de son lorsque les préamplis sont poussés dans leur retranchement. Cette couleur liée à la technologie est toujours à pondérer, car elle dépend grandement de la topologie du circuit. De plus, l’impédance d’entrée et de sortie du préampli a sans doute dans bien des cas un rôle bien plus important. 9.3 Les contrôles d’un préampli Un préampli propose souvent les réglages suivants : Un potentiomètre de gain (qui est souvent remplacé par un sélecteur cranté, plus précis, pour les modèles haut de gamme). Un bouton activant l’alimentation fantôme. En effet, c’est bien le préampli qui génère cette tension d’alimentation pour les microphones statiques. Un bouton d’inversion de phase. Un coupe-bas. "],["la-conversion-analogique-numérique.html", "10 La conversion analogique numérique 10.1 La nécessité de la conversion analogique numérique 10.2 D’un signal continu vers un signal échantillonné 10.3 La fréquence d’échantillonnage 10.4 La résolution de quantification 10.5 La “mauvaise réputation” du son numérique 10.6 La conversion sigma-delta", " 10 La conversion analogique numérique 10.1 La nécessité de la conversion analogique numérique Le support d’enregistrement historique de la musique est la bande magnétique. Le défaut d’un tel support de stockage est principalement son rapport signal/bruit, peu avantageux. A contrario, l’enregistrement numérique offre une dynamique bien supérieure à celle des supports analogiques. Aujourd’hui, la plupart des productions multimédias reposent sur l’utilisation d’ordinateurs en lieu et place des équipements analogiques équivalents. Cela représente pour les structures une économie de coût importante pour peu ou pas de modification sur la qualité du contenu. 10.2 D’un signal continu vers un signal échantillonné La caractéristique principale d’un signal analogique est sa continuité. Un signal continu, en mathématique, se définit par un signal défini en n’importe quel instant. Afin d’être numérisé, un tel signal doit être dénombré. En effet, la notion d’infini imposé par la continuité du signal n’a pas d’existence en numérique. La numérisation du signal est comparable à l’utilisation d’un multimètre pour mesurer une tension. Un convertisseur va prélever la valeur du signal, de façon régulière, au cours du temps. Afin de correctement numériser un signal, il convient de définir deux paramètres : la vitesse de prélèvement, ou fréquence d’échantillonnage la plage de valeur permise pour le signal, ou résolution de quantification 10.3 La fréquence d’échantillonnage Cette fréquence définit le nombre de prélèvements par seconde. Par exemple, un morceau édité sur un CD audio a une fréquence d’échantillonnage de 44 100 Hz (44,1 kHz), cela signifie que le signal est mesuré 44 100 fois par seconde. La fréquence de travail la plus courante est 48 kHz, mais l’on rencontre parfois des valeurs supérieures, multiple de celle-ci : 96 kHz, 192 kHz, etc. Cette augmentation proportionnelle de la fréquence d’échantillonnage s’appelle suréchantillonnage. Certains techniciens espèrent, par le biais de ce suréchantillonnage, améliorer la qualité de l’enregistrement. Le suréchantillonnage vient également à un coût de ressource et d’espace de stockage. Un flux audio échantillonné à 96 kHz demande deux fois plus de ressource et d’espace qu’un flux échantillonné à 48 kHz. 10.3.1 Le théorème de Shannon-Nyquist Cette valeur initiale de 44 100 Hz (ou 48 kHz) n’a pas été choisie au hasard. Pour la comprendre, il faut revenir sur le phénomène que nous cherchons à numériser. Le son est une onde mécanique, et nous l’entendons dans une plage de fréquence comprise entre 20 Hz (très grave) et 20 000 Hz (très aigu). Il faut donc que notre système de numérisation soit capable de reproduire une fréquence maximale allant jusqu’à 20 000 Hz. Pour cela, nous utilisons les résultats des travaux des chercheurs Harry Nyquist et Claude Shannon (tous deux ayant travaillé aux laboratoires Bell). Le théorème de Shannon-Nyquist stipule que, pour être capable d’échantillonner un signal de fréquence \\(f\\), la fréquence d’échantillonnage doit au moins être de \\(2f\\). Un ensemble de points généré par une fréquence ne peut correspondre qu’à cette seule fréquence. Notre plage d’écoute étant limitée à 20 kHz, la fréquence d’échantillonnage minimale dont nous avons besoin est de 40 kHz. Que se passe-t-il si la fréquence du signal dépasse la moitié de la fréquence d’échantillonnage ? Dans ce cas, la vitesse de prélèvement n’est plus suffisante et nous observons l’apparition de nouvelles fréquences ne provenant pas du signal original. Ce phénomène se nomme repliement spectral. 10.4 La résolution de quantification La résolution de quantification permet de définir la plage de valeur dynamique permise dans le système numérique. Celle-ci s’exprime en 8 bit. Par exemple, si nous prenons un convertisseur travaillant en 8 bit. Sa valeur minimale, en binaire, est 0000 0000, ou encore 0 en base 10. Sa valeur maximale, en binaire, est 1111 1111, soit 255. Admettons que ce convertisseur accepte des signaux ayant une tension en entrée variant entre +15V et 0V, celles-ci seront respectivement affectées aux valeurs numériques 255 et 0. Si maintenant, ce convertisseur travaille en 16 bit, ces valeurs seront affectées aux valeurs numériques 65535 et 0. La précision de la mesure de la dynamique du signal n’est donc pas du tout la même. En pratique, augmenter la résolution de quantification permet principalement de définir le niveau de bruit du convertisseur. Plus la résolution est élevée, plus le bruit se retrouvera faible. En 8 bit, l’écart entre le niveau maximal d’un signal et le bruit est de 48 dB, en 16 bit cet écart est de 96 dB, en 24 bit, 144 dB. La résolution de quantification standard en enregistrement est 24 bit. La plage dynamique est telle qu’elle rend le travail d’enregistrement beaucoup plus souple sur les niveaux d’acquisition des différentes sources. 10.5 La “mauvaise réputation” du son numérique Le son numérique a longtemps eu la réputation d’être dur, particulièrement dans le haut du spectre. Cela s’explique assez facilement par la méthode de fabrication des premiers convertisseurs. En effet, toute la difficulté de fabrication d’un convertisseur réside dans la réalisation d’un filtre anti-repliement. Ce filtre enlève les fréquences au-dessus de la moitié de la fréquence d’échantillonnage, afin de prévenir tout repliement spectral. Ce type de filtre est extrêmement délicat à réaliser en analogique. Ce problème est résolu grâce à l’approche sigma-delta. De plus, le repliement spectral parfois généré par certains traitements (saturation, simulation analogique, compresseurs) n’a pas non plus aidé à la réputation du son numérique. En effets, lorsqu’il devient audible, le repliement spectral se caractérise par l’apparition de fréquences non harmoniques souvent qualifiées de dures et désagréables. Il est cependant bon de rappeler que ce phénomène, certes bien réel, se produit aussi dans des conditions de saturation du signal importante et sur des sources sonores riches en aigus. Malgré la vie dur que mène le son numérique dans l’inconscient collectif, il est important de rappeler qu’il a apporté un grand nombre d’avantages sur le son analogique, y compris sur des questions de rendus sonores. Par exemple, la dynamique est bien plus importante, la distorsion involontaire du signal infime et l’ajout de bruit inexistant. 10.6 La conversion sigma-delta Aujourd’hui, les convertisseurs ne travaillent pas directement à 44,1 kHz/16 bit ou 48 kHz/24 bit. Ils utilisent à la place un procédé appelé échantillonnage sigma-delta. Le principe est d’utiliser une fréquence d’échantillonnage très rapide (384 kHz) et de coder la dynamique du signal, en relatif, sur un seul bit (ce bit prend une valeur de 1 si le nouvel échantillon est plus fort que l’ancien, 0 pour le cas inverse). Les formats de travail que nous utilisons sont générés après cette première étape. L’intérêt de cette méthode est double : Le signal est suréchantillonné dès l’enregistrement Les filtres permettant d’éviter le repliement spectral sont donc très simples à réaliser "],["les-modulations-numériques.html", "11 Les modulations numériques", " 11 Les modulations numériques "],["introduction-à-linformatique-musicale.html", "12 Introduction à l’informatique musicale", " 12 Introduction à l’informatique musicale "],["enceintes-et-salle-découte.html", "13 Enceintes et salle d’écoute 13.1 Choisir une paire d’écoutes 13.2 Placer correctement son écoute 13.3 L’écoute au casque 13.4 Casque fermé ou casque ouvert ?", " 13 Enceintes et salle d’écoute De tous les équipements audio nécessaires pour réaliser une prise de son, les enceintes (et les casques) sont certainement les plus importants. En effet, c’est sous leurs influences que nous pourrons écouter et contrôler notre travail. Il est donc crucial d’utiliser des écoutes regroupant un certain nombre de critères minimaux et de les connaître sur le bout des doigts. 13.1 Choisir une paire d’écoutes Choisir une paire d’enceintes peut paraître être un exercice difficile. Il existe énormément de modèles, parcourant des gammes de prix allant de quelques dizaines d’euros à plusieurs dizaines de milliers. Qui plus est, l’enceinte audio est sans doute l’appareil audio que les Hommes savent le moins bien fabriquer. L’enceinte de monitoring parfait, à savoir linéaire en tout point, n’existe pas. Premièrement, nous ne savons pas fabriquer des haut-parleurs capables de reproduire uniformément toutes les fréquences. Ces derniers sont souvent spécialisés dans une certaine plage de fréquence. La plupart des enceintes de monitoring utilisent 3 voies : deux actives (utilisant des haut-parleurs) pour l’aigu et le médium, et une passive (évent avant ou arrière) pour le grave. Deuxièmement, un haut-parleur peut être approché par un modèle « masse-ressort ». Cela signifie qu’il y a une certaine inertie à sa mise en action et une certaine inertie à sa mise en arrêt. L’enceinte idéale devrait posséder une inertie nulle dans les deux cas, ce qui n’est malheureusement pas le cas en pratique. Cette inertie est potentiellement responsable d’un adoucissement des transitoires et d’une sensation de flou. Il y a cependant plusieurs critères assez objectifs pour évaluer la qualité d’une enceinte : La réponse en fréquence : l’enceinte flatte-t-elle particulièrement une zone du spectre ? En délaisse-t-elle une autre ? La réponse en transitoires : les attaques sont-elles respectées ? Retrouve-t-on l’énergie initiale du signal ? La linéarité en fonction du volume : a-t-on une sensation de compression du signal lorsque l’on augmente le niveau envoyé dans l’enceinte ? Le centre fantôme : le centre du système stéréophonique paraît-il stable ? Paraît-il précis ? La couleur sonore de l’enceinte : a-t-on plaisir à écouter du son et de la musique sur ce système ? 13.2 Placer correctement son écoute Afin de satisfaire les critères de la stéréophonie, il convient de respecter les règles suivantes : Les deux enceintes doivent être de même marque, de même modèle et appairée. Si une des membranes a dû être changée sur l’une d’elle, l’autre aurait dû recevoir la même opération. Les deux enceintes doivent être séparées d’un angle de 60° L’auditeur doit être placé à équidistance des deux haut-parleurs, et regarder vers le milieu du segment formé par les deux enceintes. Une fois ces critères respectés, voici quelques conseils sur le placement des enceintes dans une pièce : On préférera des pièces de grandes tailles, afin de repousser au maximum le temps d’arrivée des premières réflexions. Le système stéréophonique devrait être positionné dans un souci de symétrie : l’enceinte de gauche ne devrait pas être plus proche d’un mur que l’enceinte de droite, par exemple. Dans le cas de petit espace, on préféra coller les enceintes contre un mur. Cela permettra de supprimer l’influence d’une des premières réflexions au prix de l’augmentation du niveau de grave. Il est vivement recommandé de procédé au traitement, même minimal, acoustique de la pièce de travail, à commencer par les zones de réflexions premières et par les angles (ou le grave va s’accumuler). Si le traitement acoustique n’est pas envisageable, il convient de privilégier une écoute à faible niveau et une proximité maximale avec les enceintes. 13.3 L’écoute au casque Le casque est un outil permettant d’écouter un signal tout en s’extrayant de son environnement (acoustique et/ou bruit). Cependant, de par son mode de fonctionnement, à savoir deux haut-parleurs placés dans une enceinte en contact direct avec les oreilles, il génère un certain nombre de déformations. Premièrement, la stéréophonie écoutée au casque est hypertrophiée. En effet, dans ces conditions d’écoutes, l’oreille gauche n’entend que le haut-parleur gauche et l’oreille droite n’entend que le haut-parleur droit. Deuxièmement, il est très difficile de trouver des casques avec une réponse en transitoire satisfaisante. Il convient donc d’être excessivement prudent lorsque l’on mix du contenu percussif sur un casque. Troisièmement, les casques sont encore moins linéaires en fréquence que les haut-parleurs, il convient là aussi d’être très prudent lors de la réalisation d’un mixage. Ces défauts peuvent être compensés par l’habitude et la connaissance du système d’écoute, mais la transportabilité d’un mixage (à savoir, sa compatibilité avec d’autres systèmes d’écoute) réalisé au casque est souvent discutable. 13.4 Casque fermé ou casque ouvert ? Le casque fermé, comme son nom l’indique, propose une fabrication enfermant le haut-parleur dans une enceinte close. Cette méthode de fabrication offre l’avantage d’isoler celui qui écoute de l’environnement, mais aussi d’isoler l’environnement de ce qui est diffusé dans le casque. Par contre, ces casques ont souvent une réponse en fréquence très accidentée, et ne sont pas recommandés pour le mixage. Il est par contre vivement recommandé pour les musiciens en session de prise de son. Le casque ouvert, à l’inverse de son homologue fermé, n’offre aucune isolation acoustique, au prix d’une meilleure réponse en fréquence du casque. Ces casques sont tout indiqués pour le mixage, mais beaucoup moins pour des situations de prise de son. "],["mono-et-multimicrophonie.html", "14 Mono et multimicrophonie", " 14 Mono et multimicrophonie "],["la-prise-de-son-au-couple.html", "15 La prise de son au couple 15.1 Généralités sur les mécanismes de la localisation du son par l’oreille humaine 15.2 Principes de la prise de son au couple 15.3 Les topologies classiques de prise de son au couple 15.4 Compléter une prise de son au couple par des appoints", " 15 La prise de son au couple La prise de son au couple stéréophonique regroupe l’ensemble des techniques de prise de son dédié au système d’écoute stéréophonique (deux enceintes séparées de 60° et orientées vers un auditeur placé à équidistance des deux transducteurs). Ces techniques permettent une bien meilleure représentation des espaces des prises de son ainsi qu’une localisation des différents éléments enregistrés dans cet espace. 15.1 Généralités sur les mécanismes de la localisation du son par l’oreille humaine Afin de mieux comprendre comment fonctionne un couple de prise de son, il convient d’étudier rapidement les principes fondamentaux de notre écoute. Notre capacité à localiser les sons dans l’espace repose principalement sur deux mécanismes : + La différence de temps + La différence de niveau 15.1.1 La localisation par différence de temps Nos oreilles sont espacées, d’environ 15 à 25 cm. Cette distance implique qu’un son émis plus proche de l’oreille droite arrivera également plus tôt qu’à l’oreille gauche. Cet écart de temps, de quelques millisecondes, est suffisant pour donner à notre cerveau un indice sur la localisation du son. Afin de sentir l’ordre de grandeur en jeu, calculons la différence de temps (\\(\\Delta t\\)) maximale pour un individu possédant un écart d’oreille de 20 cm. On sait que la célérité du son dans l’air vaut \\(c = 340 m.s^{-1}\\), et est invariant en fonction de la fréquence. On sait également que \\(c = \\frac{d}{t}\\). Dès lors, si on pose \\(d = 20 cm\\) soit \\(d = 0.2 m\\), on peut en déduire que : \\(t = \\frac{d}{c} \\iff t= \\frac{0.2}{340} \\approx 0.0006 s \\approx 0.6 ms\\) Afin de mettre en relief ce résultat, il est communément admis que l’oreille humaine commence à faire la différence entre deux répétitions d’un même son à partir de \\(20 ms\\). 15.1.2 La localisation par différence d’intensité A priori, l’espace entre nos deux oreilles n’est pas creux. La densité de notre crâne et de son contenu va réfléchir et absorber une partie des fréquences rencontrées. Également, la partie externe de nos oreilles, appelées pavillon, permet, grâce à sa forme, de donner une directivité à notre écoute. En d’autres termes, notre tête et le pavillon de nos oreilles se comportent comme un filtre, variant en fonction de l’angle d’incidence de la source. Cette altération du timbre n’est pas perçue comme une coloration, mais bien comme une information de localisation. La modélisation mathématique de ces filtres se retrouve dans la littérature scientifique sous le nom HRTF. Cette atténuation séquentiellement dépendante est décisive dans notre capacité à localiser les sons. On la retrouve communément sous le nom \\(\\Delta i\\). 15.1.3 Prévalence fréquentielle de ces deux phénomènes Il est communément admis que le \\(\\Delta t\\) aura une efficacité maximale dans les basses fréquences, et le \\(\\Delta i\\) dans les hautes fréquences. 15.2 Principes de la prise de son au couple Pour créer son effet stéréophonique, les couples de prise de son utilisent les mêmes mécanismes que notre écoute naturelle : + La différence de temps + La différence d’intensité Il va de soi que, pour fonctionner de façon optimale, les microphones utilisés pour réaliser une prise de son stéréophonique doivent être de même marque, de même modèle et appairée. Afin de manipuler ces mécanismes, le preneur de son peut jouer sur les paramètres suivant : + La directivité des microphones + L’angle entre les capsules + La distance entre les capsules Modifier chacun de ses paramètres influe sur l’angle de prise de son. Plus l’ange de prise de son est faible, plus l’impression de stéréophonie sera grande. Plus l’angle de prise de son est grand, plus l’impression de stéréophonie sera faible, jusqu’à tendre vers la monophonie. Attention de ne pas confondre l’angle de prise de son avec l’angle entre les capsules. 15.2.1 Comment choisir un angle de prise de son. L’angle de prise de son est étroitement lié à la distance du couple par rapport à l’évènement sonore à enregistrer. En règle générale, plus le couple est loin des objets sonores à enregistrer, plus son angle de prise de son sera faible. À l’inverse, plus le couple sera proche, plus son angle de prise de son sera grand. Ensuite, lors de la réalisation d’un couple de prise de son, il est commun d’enregistrer un ensemble d’éléments : plusieurs instruments (batterie), voire plusieurs musiciens (quatuor à corde, orchestre). L’objectif est bien souvent de retrouver une sensation de disposition des éléments dans l’espace proche de la situation réelle. On cherche donc un angle de prise de son suffisamment petit pour que les sources occupent l’intégralité de l’espace stéréophonique, mais également suffisamment grand pour ne pas créer une sensation de trou au centre. 15.2.2 Comment réaliser un angle de prise de son. Plusieurs outils existent pour aider le preneur de son à configurer son angle de prise de son correctement. Il est important de commencer par évoquer les abaques de Michael Williams, ayant cherché à étudier l’angle de prise de son et ses qualités en fonction des paramètres vues précédemment. Les résultats de ses travaux se trouvent sur le site mmad.info. On trouve également beaucoup d’application mobile, comme celle du constructeur du microphone Neumann, s’appuyant sur les travaux de Michael Williams pour aider leurs utilisateurs à correctement positionner leurs microphones. Évidemment, et heureusement, rien n’est spécifique à un fabricant de microphones en particulier, l’application d’un constructeur A peut servir pour placer des microphones d’un constructeur B. Plus récemment, des chercheurs britanniques ont développé une application dénommée MARRS, permettant de positionner son couple de prise de son par rapport aux sources via une interface graphique très simple à utiliser. Cette application est disponible sur mobile et sur navigateur internet. 15.2.3 Privilégier le \\(\\Delta i\\) ou le \\(\\Delta t\\) ? La différence de perception du champ stéréophonique est très différente entre celui produit par le \\(\\Delta i\\) ou par le \\(\\Delta t\\). Un couple reposant sur le \\(\\Delta i\\) aura une sensation de localisation des sources précise. De plus si un tel couple enregistre une source ce déplaçant a vitesse constante, la sensation de déplacement retranscrite par le couple sera, elle aussi, linéaire. Il est également possible de sommer les deux microphones ensemble afin d’obtenir un signal monophonique. Un tel couple est appelé compatible mono. Un couple reposant sur le \\(\\Delta t\\) aura une sensation de localisation plus floue, mais apportera un sens de l’espace plus grand et une dimension spacieuse. À l’inverse d’un couple \\(\\Delta i\\), la sensation d’un déplacement linéaire d’une source n’est pas linéaire. Il n’est pas possible de sommer les deux capsules pour en obtenir une réduction mono sans générer des altérations de timbre sévères. Chaque couple possède ses avantages et ses inconvénients. Heureusement, nous ne sommes pas limités à l’un où l’autre et nous pouvons à loisir réaliser une combinaison des deux mécanismes. 15.3 Les topologies classiques de prise de son au couple Le premier ingénieur à se poser la question du son stéréophonique est l’anglais Alan Blumlein en 1929. Il imagine l’entièreté de la chaîne d’enregistrement et de diffusion nécessaire à la stéréophonie. Cependant, la BBC lui impose comme contrainte que toutes ses propositions soient compatibles avec des systèmes monophoniques. Il inventera donc le couple XY et MS. Plus tard, la plupart des radios européennes développeront des couples de prises de son mêlant \\(\\Delta i\\) et \\(\\Delta t\\), tel que l’ORTF. 15.3.1 Le couple Blumlein / XY Les deux microphones sont ici directifs, placés au même point de l’espace et ongulé d’une certaine valeur entre eux. De par les contraintes technologiques de son époque, Blumlein a décrit ce couple pour une utilisation de deux microphones bidirectionnels. Il est aujourd’hui plus commun de le rencontrer avec deux cardioïdes. Dans sa version originale, le couple Blumlein comprend donc deux microphones bidirectionnels avec un angle de 90°. La formulation du couple XY comprend deux microphones cardioïdes avec un angle compris entre 90° et 135°. 15.3.2 Le couple MS Le couple MS, également inventé par Alan Blumlein, permet de doser la quantité de stéréophonie après l’enregistrement. Pour se faire, ce couple utilise deux microphones : + Un omnidirectionnel, historiquement, mais aujourd’hui fréquemment remplacé par un microphone cardioïde. + Un bidirectionnel Le microphone omnidirectionnel, ou cardioïde, va rendre compte du centre de la stéréophonie, tandis que le microphone bidirectionnel rendra compte de la latéralité. Une fois enregistrés, ces deux canaux ont besoin d’être convertis, plus exactement dématricés, vers une paire de canaux stéréophonique. L’opération est très simple : \\(L = M+S\\) \\(R = M-S\\) Cette opération peut être réalisée sur une console de mixage, telle que décrite ci-dessous. Figure 15.1: Dématriçage MS 15.3.3 Le couple ORTF Le couple ORTF, inventé par la radio française du même nom, combine l’effet du \\(\\Delta i\\) et du \\(\\Delta t\\) afin de s’approcher de l’écoute humaine. Sa topologie est précisément définie. Elle propose l’utilisation d’une paire de microphones cardioïde, ongulé du 110° et avec un écart de 17 cm. 15.3.4 Les couples AB Les couples AB peuvent avoir une définition ambiguë. Une partie de la littérature scientifique considère comme couple AB tout couple non coïncident. À cet égard l’ORTF est considéré comme un couple AB. Pour d’autre, les couples AB ne concernent que des couples constitués de microphones omnidirectionnels. Ces derniers ont la particularité de n’utiliser que le \\(\\Delta t\\) afin de placer les sources dans l’espace. Le rendu est donc souvent spacieux, au prix d’une certaine instabilité et d’un certain manque de précision de l’image stéréophonique. 15.4 Compléter une prise de son au couple par des appoints Il est commun, lors d’une prise de son au couple, de chercher à obtenir une entière satisfaction sonore à la seule aide du couple. Cependant, cela n’est parfois pas possible, souvent pour des contraintes physiques et acoustiques (un instrument de l’ensemble jouant moins fort que les autres). Dans ces cas, l’utilisation d’appoint, donc de microphone supplémentaire, placé en proximité de la source, va permettre de venir récupérer une précision supplémentaire de l’instrument. Lors de l’étape de mixage, le couple servira de base principale et l’on viendra ajouter la quantité nécessaire d’appoints pour préciser le propos. Il sera parfois nécessaire de remettre en phase l’appoint et le couple pour améliorer la sommation de l’ensemble. "],["considérations-pratiques.html", "16 Considérations pratiques 16.1 Le confort du musicien 16.2 Le choix de l’instrument 16.3 Le choix de l’acoustique 16.4 Placer et choisir son microphone 16.5 Le choix du préamplificateur", " 16 Considérations pratiques Le bon déroulement de l’enregistrement d’instruments acoustiques dépend de multiple facteur. Trié par ordre d’importance décroissante, nous trouvons : Le confort du musicien La qualité de l’instrument enregistré La qualité de l’acoustique de la pièce où a lieu l’enregistrement Le placement du microphone Le choix du microphone (technologie et directivité) Le choix du préampli 16.1 Le confort du musicien Même si elle peut sembler triviale, cette « étape » de la chaîne de prise de son est de loin la plus importante. La qualité de l’interprétation donnée par le musicien dépendra grandement de son état moral et psychologique : Est-il stressé Est-il confiant Se sent-il accueilli etc. Nous pourrions considérer qu’un ou une musicienne arrivant dans un studio d’enregistrement se présente avec un taux de confiance maximal envers l’équipe technique. Dès lors l’objectif des différents techniciens est de conserver cette jauge au maximum. Les premières minutes sont particulièrement importantes et va poser un ressenti fort sur la journée de travail. Il y a donc un équilibre à trouver entre un accueil chaleureux et décontracté et un rapport productiviste et sérieux. Le système permettant aux musiciens de communiquer entre eux et avec les techniciens est primordial. En pratique, il n’est pas rare de dédier certains microphones du plateau à cette tâche. Du côté régi, le « talkback » est l’outil de communication premier des techniciens présents sur la session. Il convient de l’utiliser avec soin et prudence. Un musicien peut rapidement se sentir isolé, s’il enregistre seul. Il convient de maintenir un contact régulier et précis afin de ne pas l’abandonner dans le seul dans sa cabine. Qui plus est, un quiproquo peut être vite arrivé avec les systèmes de talkback. Prudence quant à l’état d’ouverture ou de fermeture du microphone. 16.2 Le choix de l’instrument La plupart des musiciens se présenteront avec leurs instruments. La marge de manœuvre est donc ici quasi nulle. Cependant il n’est pas rare que le studio possède du « backline », souvent composé de batteries, d’amplificateur guitare et basse, voire de guitares et de basses. Si l’instrument utilisé par le musicien pose problème pour la prise de son, proposer une alternative peut s’avérer être un bon pari. Il convient évidemment de sonder l’ouverture du musicien par rapport à cette proposition, afin de ne pas le braquer. Il peut également être intéressant de « préparer » les instruments. Cette technique est très courante sur les pianos et les batteries, afin de changer les propriétés acoustiques de l’instrument grâce a l’utilisation de draps, coussins, couvertures disposées dans ou sur l’instrument. 16.3 Le choix de l’acoustique L’acoustique de la salle d’enregistrement est-elle aussi plus souvent une contrainte qu’une variable d’ajustement. On préférera souvent de grandes salles afin de limiter l’apparition prématurée de premières réflexions. Plus la salle sera petite, plus celle-ci apportera une forte coloration sur le contenu enregistré. Il convient donc d’être attentif aux petites cabines de studio, celles-ci sont souvent très mates, mais leur apport sur le timbre des instruments qui y sont enregistrés est souvent très important. Lorsque l’on a la possibilité d’enregistrer dans de grandes salles, il est souvent intéressant de disposer des quelques panneaux acoustiques mobiles, afin de modeler la pièce à sa convenance. Si l’acoustique imposée est défavorable, on préférera dans ce cas des prises d’hyper proximité, afin de minimiser son effet au maximum. 16.4 Placer et choisir son microphone En pratique, il est bien difficile de dissocier le choix du microphone de son placement, les deux étant très interdépendants. Cependant, il convient de garder à l’esprit que le positionnement du microphone est, parmi les deux, sans doute le plus déterminant. La première étape, avant même de choisir un microphone, consiste à écouter l’instrument dans l’acoustique d’enregistrement. Il s’agit ici d’une écoute active. On se déplace autour de l’instrument, on s’en approche, on s’en éloigne, afin de sentir l’interaction entre la source et l’acoustique du lieu. Aussi, il est important de trouver deux zones d’émission particulière de l’instrument : la zone de projection maximale et la zone au timbre le plus favorable. La première peut nous servir à positionner l’instrumentiste par rapport aux autres instruments afin de minimiser les reprises entre microphones. La deuxième zone nous indique l’axe de prise de son. Cette zone au timbre le plus favorable est relative. Elle dépend de l’instrument, bien sûr, mais aussi du modèle. Elle dépend également du mode de jeu, de l’articulation du joueur et évidemment, de l’esthétique de la musique. 16.4.1 Le rapport a la distance du microphone La distance de positionnement du microphone est un élément excessivement important sur le rendu esthétique de la prise de son. En règle générale, plus on prend de distance, plus on approche une prise de son naturaliste, cherchant à reproduire un évènement sonore dans son environnement, tel qu’il aurait été entendu dans la pièce. Plus on se rapproche, plus on fragmente l’événement sonore et plus on l’arrache aussi a son contexte de diffusion. Afin de déterminer efficacement le placement d’un microphone, il convient d’abord d’en connaître sa distance critique. Celle-ci correspond au point, dans une pièce, où le son provenant directement d’une source est perçu au même niveau sonore que la réponse acoustique à cette source. Cela signifie que si nous plaçons notre microphone au-delà de ce point, nous obtiendrons plus d’acoustique que de son direct de l’instrument. Il est important aussi de considérer que la directivité du microphone influe sur la distance critique. En effet, plus la directivité du microphone est large (tends vers l’omnidirectionnalité), plus le microphone paraîtra éloigné de la source. À l’inverse, plus la directivité d’un microphone est étroite (tends vers la bidirectionnalité), plus le microphone paraîtra proche. Dans le cas de l’utilisation de microphone directif, le placement en proximité et hyperproximité va créer une accentuation du contenu basse-fréquence de la source. Cela devient parfois un élément esthétique, comme sur les voix radiophoniques. Cela aussi peut être un défaut, une exagération qu’il conviendra de corriger en postproduction. 16.4.2 Quand choisir une prise de son stéréophonique La prise de son stéréophonique, comme son nom l’indique, regroupe l’ensemble des techniques de prise de son dédié au système de diffusion stéréophonique (deux enceintes séparées de 60° et orientées vers un auditeur placé à équidistance des deux transducteurs). L’avantage de tels dispositifs de prises de son est de peupler dès la prise l’espace stéréophonique qui est donné à l’auditeur lors de la diffusion. Ils permettent également de rendre compte de la position de plusieurs évènements sonores ayant lieu dans la même acoustique. Cette dernière est d’ailleurs bien mieux retranscrite par de tels systèmes de prise de son. Il s’agit à nouveau d’un choix esthétique. Faire le choix d’une prise de son monophonique permet de renforcer la sensation de frontalité et de densité d’une source. À l’inverse, une prise de son stéréophonique donnera une définition spatiale accrue. 16.4.3 Quand choisir le multi-microphonie La multi-microphonie consiste à enregistrer un instrument via l’utilisation de microphones (principalement) directifs, placés à différents endroits jugés pertinents et en hyperproximité. Cette approche esthétique de la prise de son est devenue indissociable des « musiques actuelles ». Elle offre l’avantage d’une grande flexibilité de traitement lors de la phase de mixage. Voir, elle implique une certaine partie des traitements. En effet, une prise d’hyperproximité va systématiquement relever deux défauts : + un effet de proximité : le grave/bas médium de la source paraît hypertrophié lors de l’emploi de microphones directifs. + Les dynamiques de jeux sont également hypertrophiées. L’effet de proximité implique donc bien souvent l’utilisation d’un égaliseur, permettant de corriger cette augmentation artificielle du grave. De même, l’hypertrophie de la dynamique de jeu implique l’usage d’un compresseur afin de corriger ces variations artificielles. Afin de recréer une sensation de spatialisation, on utilisera principalement deux outils. En premier lieu, le potentiomètre de panoramique afin de diriger ces sons mono dans l’espace stéréophonique, puis les réverbérations artificielles permettra de reconstituer un champ acoustique et de réintégrer ces sources dans une scène sonore. Si l’approche de la prise au couple pouvait être qualifiée de naturaliste, alors la prise de son en multimicrophone sera son pendant spectaculaire. Évidemment, il convient de ne pas aussi franchement opposer ces deux approches et il existe tout un monde de système de prise de son entre ces deux extrêmes. 16.5 Le choix du préamplificateur Le rôle du préamplificateur est d’amplifier le signal, le tout en ramenant le minimum de bruit. Un premier élément de choix de préampli va se faire sur le niveau de pression acoustique produit par les sources à enregistrer. Enregistrer une batterie impose peut de contrainte sur le préampli quand a sa capacité à amplifier sans rajouter beaucoup de bruit sur le signal. À l’inverse, enregistrer des instruments peux sonores, possiblement avec des microphones peux sensibles, implique l’utilisation de préampli avec une excellente réserve de gain et un excellent rapport signal bruit. 16.5.1 L’influence du préampli sur la « couleur » du son Il est assez connu que le préampli peut également devenir un choix esthétique pour influencer la couleur d’une prise de son. Cette question semble assez complexe. Voici quelques éléments de réponse : Le choix du préampli est d’une influence minime par rapport à tous les autres choix précédemment fait. Les préamplis sont souvent catégorisés, en termes de couleur, via les composants utilisés pour réaliser l’amplification. Attention, un composant électronique dépend toujours du contexte dans lequel il est placé (ici, du circuit électronique). Il est donc difficile de précisément qualifier le son d’un préampli à lampe ou à transistor de façon générique. Les impédances d’entrée des préamplis ne sont souvent pas évoquées dans ces discussions. Hors, pour la plus parts des microphones (hors statiques), leur impédance de sortie peut être suffisamment élevée pour engendrer une déperdition en aigu et en transitoire. Cette déperdition peut être heureuse, ou malheureuse, mais surtout bien réelle. Une manière de s’en prémunir peut-être d’utiliser des « booster » de microphones (parfois également appelés préamplis), permettant d’augmenter le niveau de sortie des microphones et aussi d’adapter leur impédance. "],["déphasage-et-remise-en-phase.html", "17 Déphasage et remise en phase 17.1 Les effets sonores de déphasage 17.2 Approche mathématique 17.3 Les sources de déphasage", " 17 Déphasage et remise en phase 17.1 Les effets sonores de déphasage Tous les signaux sont caractérisés par une certaine phase. Celle-ci est moins tangible que celles de niveau sonore ou de fréquence. En effet, lorsqu’un signal est écouté seul, celle-ci ne s’entend pas. C’est au moment où plusieurs signaux corrélés (comprendre, enregistrés au même moment, par plusieurs microphones) sont sommés que les différences de phase peuvent s’entendre. 17.2 Approche mathématique Prenons l’exemple d’un son pur : \\(sin (\\omega t + \\phi)\\) où \\(\\omega = 2\\pi f\\) La phase de ce signal est décrite par \\(\\omega t +\\phi\\) Les deux paramètres responsables de déphasages audibles sont : + \\(t\\), le temps + \\(\\phi\\), la phase à l’origine Attention, pour un son pur, l’effet de la modification de \\(t\\) ou de \\(\\phi\\) semble très similaire. Ce n’est pas le cas pour des signaux pseudo-périodiques, atténués dans le temps. 17.3 Les sources de déphasage Les causes les plus classiques de déphasages sont : Un câble XLR avec une inversion sur le point chaud et le point froid Une prise de son avec une différence de distance entre deux microphones Une prise de son utilisant deux microphones positionnés de part et d’autre d’une membrane Un retard de certaines fréquences lié aux objets rencontrés par les signaux "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
